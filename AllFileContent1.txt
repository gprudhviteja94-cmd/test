2026-01-08 Team Onboarding and Project Coordination

Creation Time: 2026/1/8


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-08 11:33 (Duration: ~34 minutes)
  â€¢ Location: No relevant content mentioned
  â€¢ Attendee: Sachin (Speaker 1), Prudhvi/Prithvi (Speaker 2), Kundan, Nirmal, Sameer, Siddhi, Aipa, Sindhuja, Athar (front-end), ANSI/Anci, additional team members referenced (USCDO/US consumer team)


ðŸ“’Meeting Outline


Team onboarding and working model

  â€¢ Introductions and backgrounds
    
    â€¢ Two new team members onboarded: Nirmal (ex-Goldman Sachs; 6 years; Java/finance) and Prithvi (7 years; Java, Spring Boot, microservices, CI/CD, GCP/AWS, full-stack; prior experience in banking/insurance; joined via Synechron). Additional intro from another member with 2.5â€“3 years on GCP and SQL, currently senior associate via Cognizant.

  â€¢ Team structure and domains
    
    â€¢ Team: USCDO (US Consumer Data Office) building automation tools for US consumer data.
    â€¢ Senior engineer: Sachin. Module leads include Kundan, Ayyappa, Sameer, and Siddhi.

  â€¢ Agile process and ceremonies
    
    â€¢ 10-day sprints using Rally. Tasks assigned at sprint start; demo to Product Owner for acceptance.
    â€¢ Daily stand-up plus an evening stand-up to update product team; attendance emphasized.

  â€¢ Work location and coordination
    
    â€¢ Three Bangalore-based members (including Kundan) to align in-office days together (same three days/week) for coordination; flexible otherwise. Sindhuja is Bangalore contact for logistics/leaves.


Access, training, and setup

  â€¢ Training requirements and resources
    
    â€¢ Mandatory trainings to be completed first via Noomi Learning.
    â€¢ Core learning modules: BigQuery/GCP data stack; videos available with corresponding documentation for those who prefer reading.
    â€¢ Note to verify latest training links; older Dev group links may be outdated.

  â€¢ Local setup and documentation
    
    â€¢ New members asked for local setup docs; guidance to use provided learning materials and coordinate with Sindhuja for access/support.


Data pipelines, scripts, and production status

  â€¢ Release status and freezes
    
    â€¢ Planned go-live was expected today; blocked due to freeze period not updated. Pipeline approval on hold to prevent breaking E3 data. Owner is engaging with the approving team and will provide updates.

  â€¢ Q-Track and IC* scripts
    
    â€¢ Q-Track â€œskipsâ€ migration deployed to production; data from yesterday updated. Validation in progress by ANSI.
    
    â€¢ ICMP/ICMV/ICMA/ICMB scripts discussed:
      
      â€¢ ICMP modifications completed.
      â€¢ ICMA: frame functionality code fixes deployed to production; validation yet to start for some models.
      â€¢ ICMB: pending; expected to be quick given experience from ICMA. Prioritize ICM(D/B) validations first.

  â€¢ Validation findings and fixes
    
    â€¢ Two scripts (MGM and rewards integrity): issues found.
      
      â€¢ MGM and reverse integrity producing null results in actual valuesâ€”rechecking logic.
      â€¢ Rewards integrity had syntax errorsâ€”being fixed; will re-push and revalidate.
    
    â€¢ Dynamic parameters in rewards integrity were not updatedâ€”now noted and being corrected.

  â€¢ Execution schedule and prioritization
    
    â€¢ Next execution cycle scheduled for coming Monday; team to use execution list to prioritize validations and fixes accordingly.
    â€¢ MGM script is newly onboarded; finalize if near completion, otherwise prioritize ICMB/ICMD parity validation first.

  â€¢ Decommissioning and housekeeping
    
    â€¢ â€œThree modality surveyâ€ decommissioned in production; Confluence page created. Share link in group and add relevant members.

  â€¢ Front-end coordination
    
    â€¢ Athar (FE) to coordinate on â€œformsâ€ and â€œself-serveâ€ design with Sameer post-lunch (around 1 PM) if available; aim to close design decisions.
    â€¢ BPM change: sub-status removed since not set currently; still visibleâ€”needs follow-up.


ðŸ“‹Overview

  â€¢ New team members introduced; backgrounds align with Java, microservices, GCP/AWS, finance domains.
  â€¢ Working model confirmed: Agile with 10-day sprints on Rally; mandatory stand-ups and coordinated in-office days for Bangalore team.
  â€¢ Mandatory trainings and BigQuery/GCP learning path established; ensure latest links and complete before deeper project tasks.
  â€¢ Todayâ€™s go-live blocked by freeze period; pipeline approval pending to protect E3 data.
  â€¢ Q-Track migration live; validations ongoing. IC* scripts: ICMP modified, ICMA fixes deployed, ICMB/ICMD validations prioritized.
  â€¢ Validation issues identified (null actuals, syntax errors, dynamic parameters); fixes underway with revalidation planned before Monday execution.
  â€¢ MGM script is new; complete if close, else defer behind ICMB/ICMD parity validations.
  â€¢ â€œThree modality surveyâ€ decommissioned; documentation created and to be shared on Confluence.
  â€¢ Front-end â€œformsâ€ and â€œself-serveâ€ design sync planned post-lunch; BPM sub-status removal needs cleanup.


ðŸŽ¯Todo List

  â€¢ Sachin:
    
    â€¢ Resolve pipeline approval by coordinating freeze period update; provide status â€œheads upâ€ to team, ASAP
    â€¢ Share project overview and module allocations with Nirmal and Prithvi, 1â€“2 days
    â€¢ Add new members to relevant groups and share Confluence links (decommission doc, training resources), Today

  â€¢ Nirmal:
    
    â€¢ Complete mandatory trainings on Noomi Learning, This week
    â€¢ Review BigQuery/GCP learning modules (videos or docs), This week
    â€¢ Coordinate in-office days with Kundan/Sindhuja for next week, Before week start

  â€¢ Prithvi:
    
    â€¢ Complete mandatory trainings on Noomi Learning, This week
    â€¢ Review BigQuery/GCP learning modules (videos or docs), This week
    â€¢ Confirm local setup needs and access with Sindhuja, This week
    â€¢ Align in-office days with Kundan/Sindhuja for next week, Before week start

  â€¢ ANSI/Validation owner:
    
    â€¢ Validate Q-Track production data for yesterday and document results, Today
    â€¢ Fix rewards integrity syntax errors and dynamic parameter updates; re-push and revalidate, Before Monday execution
    â€¢ Investigate null actuals in MGM/reverse integrity; correct logic and revalidate, Before Monday execution

  â€¢ IC* scripts owners (Anci/Team: ICMA/ICMB/ICMD):
    
    â€¢ Prioritize ICMB/ICMD parity validations; leverage prior frame functionality experience, Before Monday execution
    â€¢ If MGM is near completion, finish; otherwise defer until ICMB/ICMD close, This sprint
    â€¢ Confirm which E3 tables each script points to and ensure test coverage in E3, This sprint

  â€¢ Kundan:
    
    â€¢ Coordinate Bangalore office attendance (three common days) with Prithvi and Nirmal; inform Sindhuja, This week
    â€¢ Share any changes expected from downstream/partner teams once freeze/pipeline unblocks, As available

  â€¢ Sindhuja:
    
    â€¢ Support access, leaves registration process, and onboarding logistics for new members, Ongoing
    â€¢ Verify latest training links vs older Dev group links; share updated list, Today

  â€¢ Athar:
    
    â€¢ Check Sameerâ€™s availability post-lunch (~1 PM) and coordinate â€œformsâ€ and â€œself-serveâ€ design session; aim to finalize approach, Today
    â€¢ Investigate BPM sub-status still appearing after removal; fix or raise with backend, This week

  â€¢ Owner (Confluence doc):
    
    â€¢ Share â€œThree modality survey decommissionâ€ Confluence link in group and add new members, Today

  â€¢ Team (general):
    
    â€¢ Do not miss daily and evening stand-ups; post updates and planned leaves per process, Ongoing2026-01-09 Artifact Onboarding and Repository Logic Discussion

Creation Time: 2026/1/9


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-09 11:31 (Duration: 4214 seconds)
  â€¢ Location: [insert location]
  â€¢ Attendee: Anand, Sachin, Samir, Nirmal, Prithvi (mentioned), Kundan, Deepak, Rachana, Poonam, Raj / Rajendra (mentioned), others (not all explicitly named)


ðŸ“’Meeting Outline


Artifact onboarding & repository logic

  â€¢ Artifacts table & onboarding method The onboarding method reads records from the Artifacts table (artifacts and repository metadata). It inspects artifact rows, determines primary method and focus metric, and inserts the appropriate repository/threshold records. Demonstration requested to query and show the couple of created artifact records so the onboarding flow can be walked through starting from those records.
  â€¢ Throughput / repository / primary method decision The repository contains fields that decide the primary method (e.g., primary method, focus metric). The process will pick the primary method and populate repository/threshold rows. Throughput data and thresholds are part of this flow; status flags (e.g., a boolean flag) will be updated to true after processing.
  â€¢ Comparison & Q-Track insertion Once data is aggregated per the focus metric (count/mean/median) and compared against threshold, the result (true/false or required action) is inserted into Q-Track similar to existing Q-Track behavior. For one-time metrics, thresholds already exist and the pipeline selects focus metric and performs aggregation over the source table (e.g., last N days or months) to calculate mean/median/count.


Daily & monthly checks, query logic, and code changes

  â€¢ Daily Rank check / private methods A daily-rank check was implemented; some input structure was changed and the author wants confirmation whether the changes are correct. There are two private methods used for the daily check; the team will review the JSON/input format in code and update queries if needed.
  â€¢ Aggregation frequency and profiling For time-windowed aggregations, the system should use frequency (daily/monthly) to pick the window. For monthly calculations, avoid hard-coding day counts â€” use dynamic date arithmetic (e.g., SQL functions to get previous month's day count or last month's date range). The recommendation: use SQL date functions rather than passing static day counts.
  â€¢ Action items around code demonstration & sharing The presenter will show the JSON and related code parts and share the conference page; an update / changes will be sent by ~2â€“3 PM. The team noted a delay due to PI planning; target is to send to the reviewer by early afternoon.


Script validations (ICMB / MGM1 / others) and DAG runs

  â€¢ Validation status DAGs have been run and data segregated. Progress on validations is ongoing; team expects to complete validation tasks in about 1â€“1.5 hours for the current chunk. From the team: 30 scripts sent to Rachana for review (sheâ€™s busy and expected to respond today). Overall there were ~40 scripts; these were split into three groups and sent for review. Also sent for MGM1 review.
  â€¢ Next steps on validations Wait for Rachanaâ€™s acknowledgement to enable unstable environments as planned. High-priority items (e.g., FGM) were flagged and Rachana and ANSI updates awaited. The team agreed to finish remaining validations; if no major issues, start building new scripts on Monday, otherwise fix issues.
  â€¢ Work assignment Focus on ICMB1 first; park non-critical items in the ANSI bucket. If anyone has worked on other scripts, they can pick them up; otherwise concentrate on ICMB validations.


Training, onboarding (Lumi Academy, Q-Track, Composer, entitlement access)

  â€¢ Lumi trainings Team members must complete Lumi Academy trainings; expected readiness by Wednesday (max). Manager emphasized Lumi training as project-related and prioritized completion. Daily status updates on trainings are required in the designated channel.
  â€¢ Order of trainings Complete Lumi training first. Then do Q-Track training (track overview, composer, demo, code, data transformers). After Q-Track, do Lazily/Resi/other tool trainings as needed. Quattro training material (PDF) will be shared later.
  â€¢ Entitlement/access & Composer Raise entitlement/access requests as required (process guidance will be provided). A Composer setup doc exists and needs modificationâ€”owner will notify which document to update.
  â€¢ Time logging When claiming hours in Clarity, productive hours should reflect project work and these training activities. Guidance: if spending 2 hours on company-mandated training, ensure remaining hours go to Lumi/project work.


Control job migration & knowledge transfer

  â€¢ Control tool & monthly vs daily jobs Action: complete the daily job first (twist code/query for daily check), then set up a monthly job. Samir to set up a call with Prithvi to explain the control tool so Prithvi can help create the monthly job; Samir will continue to own daily job. Plan to migrate existing Flips after control job setup.
  â€¢ Monthly window logic Monthly job must compute last month's date range dynamically (not hard-coded). Use SQL date functions to determine the correct number of days per month.


Office presence, coordination & communications

  â€¢ Office days Suggested office days: coincide with Deepak (Monday, Wednesday, Thursday) â€” at least come two of these days, ideally three, for better coordination and visibility (esp. because Raj is not in India). Team members to confirm in-office dates.
  â€¢ Stakeholder escalations ServiceNow integration and vendor communication delays noted. If external responses (e.g., Sara) are not received today, escalate to Rajendra. Team was advised to tag Rajendra if no replies to avoid delay.
  â€¢ Language & inclusivity Manager requested use of a common language so all participants can follow discussions; avoid side conversations in other languages.


Miscellaneous operational points

  â€¢ Screenshots & documentation Document for R&D needs screenshot integration and grammar edits. Deadline: content to be shared by ~2 PM (penalty time mentioned). Manager asked where the doc stands; author to integrate screenshots and text and share.
  â€¢ API readiness REST API details to be finalized by 2â€“3 PM so manager can review with Raj in the evening if changes needed. Once APIs are ready, notify manager and a call will be arranged if required.
  â€¢ Upload/download & Excel folder Upload and download datasets (upload into Excel folder) â€” affirmed during the call.


ðŸ“‹Overview

  â€¢ Artifacts onboarding reads artifacts, decides primary method and focus metric, inserts repository and threshold records, and updates status flags.
  â€¢ Daily Rank check implemented; code/input changes need validation and possible query updates.
  â€¢ Aggregations use focus metric (count/mean/median) across the appropriate time window; use SQL date functions to handle month lengths dynamically.
  â€¢ Validation progress: DAG runs done, ~30 scripts sent to Rachana for review; waiting for acknowledgement to enable unstable environment. MGM1 items also sent.
  â€¢ Prioritize ICMB1 validations; park lower-priority items to ANSI bucket if needed.
  â€¢ Trainings: complete Lumi Academy first, then Q-Track; daily training status updates required in channel. Target readiness by Wednesday.
  â€¢ Control tool: Samir to coordinate with Prithvi to take on monthly job; Samir to finish daily job first.
  â€¢ Office coordination: align in-office days with Deepak (Mon/Wed/Thu preferred); confirm calendar.
  â€¢ Escalate vendor/ServiceNow non-responses to Rajendra if no reply today.
  â€¢ Documentation to R&D: integrate screenshots, grammar edits; share by ~2 PM.


ðŸŽ¯Todo List

  â€¢ Anand:
    
    â€¢ Query and show the couple of Artifacts table records used for onboarding (ASAP, today by 2 PM)
    â€¢ Share the conference page and code JSON snippet for the daily Rank check (by 2â€“3 PM)

  â€¢ Sachin:
    
    â€¢ Complete remaining ICMB validations and share validation status (within ~1â€“1.5 hours from meeting update; aim today)
    â€¢ Follow up with Rachana for acknowledgement on the 30 scripts (today)

  â€¢ Samir:
    
    â€¢ Focus on ICMB1 validation and close outstanding items (continue immediately; finalize by Monday if issues)
    â€¢ Set up a call with Prithvi to explain the control tool and create monthly job handover (schedule this week)
    â€¢ Complete Lumi Academy trainings and provide daily training status in the channel (progress update daily; target complete by Wednesday)

  â€¢ Nirmal:
    
    â€¢ Continue Lumi trainings (currently halfway) and post daily status updates (complete by Wednesday)
    â€¢ Assist with Q-Track training after Lumi completion

  â€¢ Training / Access owner (Kundan / Manager):
    
    â€¢ Provide guidance and tag training links in the channel; help with entitlement/access raising steps (ASAP)
    â€¢ Share Quattro PDF and details for Q-Track training sequence (this week)

  â€¢ Docs owner (author of R&D doc):
    
    â€¢ Integrate screenshots, correct grammar, and share document to R&D (by ~2 PM today)
    â€¢ Update Composer setup doc as requested (owner will be informed which doc to modify; timeline: this week)

  â€¢ API owner (whoâ€™s implementing REST APIs):
    
    â€¢ Finalize REST API details and inform manager for call/validation (ready by 2â€“3 PM)

  â€¢ Escalation owner (team lead / manager):
    
    â€¢ If no response from external vendor / ServiceNow thread today, tag Rajendra and escalate (today)

  â€¢ Operations (all):
    
    â€¢ Confirm in-office days (align with Deepak: Mon/Wed/Thu preferred) and share calendar (this week)

Please update the shared channel with daily training completion and progress on validation tasks. If anyone hits a blocker, notify the manager immediately so it can be unblocked.2026-01-08 QTrack/DQIM Achievements and Upcoming Objectives

Creation Time: 2026/1/8


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-08 18:11 (Duration: ~44 minutes 40 seconds)
  â€¢ Location: WebEx
  â€¢ Attendee: Amit, Anand, Sunita, Sachin, Aman, Matt, Jason, Raj (mentioned), Amir, Athar, Syed, Uma, Anindya, Nirmal, Prithvi, Sumit, Suman, team members from Alpha/GCS and governance teams (referenced)


ðŸ“’Meeting Outline


QTrack/DQIM achievements and recent releases

  â€¢ Insurance analyst/back-engine tables status
    
    â€¢ 9 of 10 Insurance analyst tables live in E3; the remaining one is pending source connectivity and planned to go live post 15 Jan. All 5 back-engine tablesâ€™ MD changes live in E3; one migration held due to source data migration dependency. Targets for Insurance in 2025 were substantially met; misses attributed to source-side issues.

  â€¢ QTrack scripts migration and alerting
    
    â€¢ Phase 1 (118 scripts) and Phase 2 (80 scripts) migrated; majority running with ~90% alerts healthy. A few scripts/alerts need rectification and are being handled. Major initiative successfully delivered last year.

  â€¢ QTrack UI to DQIM portal transition
    
    â€¢ Action UI for script alerts went to production last year. Team agreed to refer to it as DQIM (Data Quality Issue Management) portal going forward (rename from QTrack UI). DQIM centralizes alerts and downstream user inputs (true/false positives), replacing email-based processes and aligning with Project Guardian.


Upcoming PI 26.1 objectives and scope

  â€¢ DQIM enhancements and integrations
    
    â€¢ ServiceNow integration: Auto-incident creation for true positives (in E2, first priority).
    â€¢ UX improvements: Add â€œmonitor rule,â€ justification field (to surface in Kibana), inline pop-ups, and bulk updates.

  â€¢ ML-driven threshold checks
    
    â€¢ Scripts integrated; need end-to-end pipeline for new threshold rules: control process, repository/results tables, and scheduled execution. Migrate existing checks. Algorithm selection (â€œpickerâ€) and methodologies to be refined during scale-up; research expected during migration.

  â€¢ Self-serve rule creation portal
    
    â€¢ Front-end to capture parameterized requirements; back-end automated rule creation and approval workflow integrating with existing framework. Currently in design; expected to be time-intensive.

  â€¢ New and existing rule work
    
    â€¢ New use cases: Insurance ongoing; Goodwill and AFC requirements expected; other teams may submit requests. Existing rule tuning/threshold adjustments to continue. One story added for production support.

  â€¢ RSI/Resi handover and Insurance support
    
    â€¢ Knowledge transfer of Resi to Jason and Matt to conclude next week. Team will still provide dev support for Insurance and Resi; placeholder features to be added for ongoing support.


Dependencies, governance, and cross-team collaboration

  â€¢ External dependencies
    
    â€¢ DQIM depends on Alpha/GCS (Saurabhâ€™s team; Athar and Syed involved) for specific data.
    â€¢ Insurance pending item depends on source teamâ€™s connectivity/migration.
    â€¢ Governance alignment: Project Guardian introducing tiered controls (Tier 1/2/3) with mandated checks; QTrack/DQIM expected to implement new categories as policies finalize (Aman and Uma leading governance analysis).

  â€¢ Naming and clarity
    
    â€¢ Standardize naming to DQIM (Data Quality Issue Management) portal to avoid confusion with QTrack (anomaly detection tool).


Resi planning and collaboration

  â€¢ Resi priorities and capacity
    
    â€¢ Team has reserved capacity for 2â€“3 Resi features per PI alongside QTrack work. Early next week, epic links to be shared; Jason/Matt to prioritize features. Top candidates: complete data sharing built last year and â€œtop dataâ€ entity integration from an MS acquisition; more details to come.

  â€¢ PI and sprint cadence
    
    â€¢ PI spans 2.5â€“3 months (5 sprints of 2 weeks). Features will be split into sprints based on product priority and team availability. Jason/Matt to attend sprint planning; features to be created and ordered by priority.


Operational/demo notes and minor items

  â€¢ Demos
    
    â€¢ Sprint-end demos cover progress; no extra demo needed now.

  â€¢ UI/Reporting clarifications
    
    â€¢ Brief discussion on front-end labels (e.g., total rebates vs. premium production) and data/format alignment; resolved during call.

  â€¢ Custom run configuration
    
    â€¢ Request for a single â€œrun/truckâ€ document to manage add/delete actions and reruns; supports both upload-time and post-run adjustments.

  â€¢ Confidence vote and gratitude
    
    â€¢ Conducted on PI page during call; general appreciation for teamâ€™s work.


Team updates

  â€¢ New joiners
    
    â€¢ Nirmal (6 years, Java backend) and Prithvi (7 years, Java backend, Spring microservices, CI/CD, JNI) joined; will support US Consumer initiatives as backend engineers.

  â€¢ US collaboration
    
    â€¢ Suman based in US; additional US resource expected (POC joining soon). Likely Insurance POC: Jane (to be confirmed by Aman).


ðŸ“‹Overview

  â€¢ Insurance 2025 targets largely met; final analyst table to go live after 15 Jan pending source connectivity.
  â€¢ QTrack migrations successful; ~90% alert health; DQIM portal live and becomes the central issue management platform.
  â€¢ For PI 26.1, priorities: DQIM ServiceNow integration and UX enhancements; ML threshold end-to-end pipeline and migration; Self-serve portal design/build; ongoing new use cases and rule tuning; production support.
  â€¢ Dependencies include Alpha/GCS data for DQIM and governance-driven control expansions via Project Guardian.
  â€¢ Resi: capacity reserved; Jason/Matt to define and prioritize features (data sharing completion and new entity integration are likely).
  â€¢ Processes: PI ~3 months with 2-week sprints; product owners to join planning; features to be created with clear priorities.
  â€¢ Team expanded with two backend engineers; cross-geo collaboration reinforced.


ðŸŽ¯Todo List

  â€¢ Sachin:
    
    â€¢ Plan detailed features/aspects for DQIM, ML thresholds, and Self-serve next week; align with Anand.
    â€¢ Add placeholder features for continued dev support to Insurance and Resi; confirm Insurance POC with Aman (likely Jane).
    â€¢ Coordinate with Raj on database details for self-serve/product implementation; share updates with Anand next week.

  â€¢ Anand:
    
    â€¢ Share epic links for PI 26.1 early next week; circulate to Jason/Matt for Resi feature creation and prioritization.
    â€¢ Invite Jason/Matt to sprint planning and share ceremony links.

  â€¢ Jason/Matt:
    
    â€¢ Prepare and prioritize Resi feature list (e.g., data sharing completion, new entity â€œtop dataâ€ integration) and create features under shared epics; before PI start.
    â€¢ Attend sprint planning to align scope with team capacity.

  â€¢ Amir:
    
    â€¢ Complete end-to-end testing for ML threshold pipeline (new rule flow, repository/results updates, scheduling) and plan migration of existing checks.

  â€¢ DQIM team (UI/Backend):
    
    â€¢ Implement ServiceNow integration for true positives (E2 priority).
    â€¢ Deliver UX enhancements: add â€œmonitor rule,â€ justification capture (Kibana visibility), inline modals, bulk updates.

  â€¢ Self-serve team:
    
    â€¢ Finalize design for front-end intake and back-end automated rule creation with approval workflow; estimate build phases.

  â€¢ Alpha/GCS (Saurabhâ€™s team: Athar, Syed):
    
    â€¢ Provide required datasets/interfaces to unblock DQIM dependencies.

  â€¢ Insurance team liaison (Aman):
    
    â€¢ Confirm Insurance POC (likely Jane) and coordinate ongoing support needs; provide go-live readiness for the pending analyst table post 15 Jan.

  â€¢ Governance team (Aman, Uma):
    
    â€¢ Finalize tiered control definitions and required checks; provide implementation guidance for QTrack/DQIM alignment under Project Guardian.

  â€¢ Reporting/UI owners:
    
    â€¢ Standardize front-end labels (e.g., premium production/total rebates) to match business usage and data feeds; update documentation.

  â€¢ Platform/Run-config owner:
    
    â€¢ Create/maintain a single â€œrun/truckâ€ document to support add/delete and rerun workflows for custom scenarios.

  â€¢ Meeting follow-up:
    
    â€¢ Cancel tomorrowâ€™s call (objectives covered); schedule 10:00 meeting tomorrow morning between Anand and requester as agreed.2026-01-12 Access and Validation Workflow Updates

Creation Time: 2026/1/12


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-12 18:05 (Duration: 1526 seconds)
  â€¢ Location: Remote (video/voice meeting)
  â€¢ Attendee: Aman, Anand, Rachna, Raj (mentioned), Sachin, Om, Jean (mentioned), Kundan, Samir, Nirbal, Prithvi, Nirmal, Suman, Sindhuja, Ayapa, Kundal, (others referenced: VPS/VP, platform team, Lumi team)


ðŸ“’Meeting Outline


Access & Validation Workflows

  â€¢ Insurance table access The team lacks access to the insurance table needed for end-to-end validation. Aman raised the access request today and asked Raj to approve. Because access is pending, testers could not complete validations; meanwhile developers were asked to continue building remaining script logic without waiting for the access.
  â€¢ Hand-in-hand result checks / sharing results Suggested closer collaborative checks where rule interpretation and code are validated together (hand-in-hand result check). If teams cannot do live pairing, at minimum they should share intermediate results for review to avoid repeated rework.
  â€¢ Existing script results sharing The team is already sharing results for existing scripts and many have been signed off from the reviewer side. Remaining fixes are being tracked. The team agreed to keep sharing results and continue incremental sign-offs.


Script Development Status and Assignments

  â€¢ Script progress & prioritization Current status noted: 3 tasks highlighted â€” one done, one in progress, one to start. From ANSI side a couple of scripts are done; one C8 (?) to start as it needs basic validation. Team to continue building scripts even while some resources are blocked.
  â€¢ Test calls scheduling There are multiple testing calls for different people. Discussion about attendance: organizer will attend initially; other reviewers may join based on availability. Calls were kept separate intentionally so each personâ€™s usage and perspective of the platform/rules can be observed independently.
  â€¢ Script distribution and resource utilization Team to allocate script validators across team members: recommendation to assign ~3â€“4 scripts per person per day to distribute load. Samir was underutilized last week; upcoming assistance expected from Nirbal and Prithvi. Backup arrangements: Prithvi to be backup for RISI MX Cust history table.


Quantro / Dashboard / Platform Blockers

  â€¢ Quantro / Dashboard blocked Dashboard work (Quantro) is blocked pending platform teamâ€™s action since 29 Dec. Aman escalated to Raj and tagged relevant platform contacts. Some code changes are required to complete A2 build and validation with variables discussed â€” screen share planned to show details and fields needed.
  â€¢ Incidents assignment and routing Currently incidents get assigned to USCDU group; planned routing is for incidents to land in the correct stakeholderâ€™s bucket. A discussion about incident assignment routing will occur once details are available.
  â€¢ Human task on updates & SFTP/Lumi flow For valid update requests, the process would create a human task. There is a pending discussion with Raj about why a human task is required again. The bundle of information from ACE will be sent via SFTP to Lumi portal/warehouse. Rejected entries should not appear on main screens; team suggested building a separate date-range filtered screen to view rejected requests if needed. Design will be reviewed with Raj and REST APIs / BPM components will be finalized afterward.


Rules Versioning / Audit Behavior & UI Expectations

  â€¢ Versioning model explained Example discussed: a rule inserted on 8 Jan and approved -> active/displayed in UI. Same rule updated on 9 Feb and rejected -> becomes disabled; audit table will contain multiple entries (insert/update/reject) and UI should show the active/latest approved version. If an update is in-progress (neither approved nor rejected), it should be viewable in UI for approver action. The SFTP file generation will take only the active entry as of the date for Lumi ingestion.
  â€¢ Rejected items / audit screens Rejected entries won't show on the main UI. The team can add a separate screen for rejected requests filtered by date range; created timestamps and flags exist in data for filtering. Final design to be approved by Raj.


Insurance Deployment / Schema Freeze & Replan

  â€¢ Deployment timing & RFC A deployment (insurance) was blocked due to schema-freeze/phase period up to 12 Jan (no schema change allowed). Replan set for stable redeployment on 15 Jan; RFC created. Aman will update the group when deployment progresses.
  â€¢ Handover to Jean Jean will take over insurance work, but no firm handover date yet â€” not started. Jean assigned to take it eventually; schedule TBD.


Pack Engine & Packaging Tables

  â€¢ Data availability & slowness Pack engine data mostly available; one packaging table lagging due to slowness that needs platform fix. Four other packaging tables are available and have timestamp variables ready. Team awaiting platform fix for the remaining table.


Upstream Data Quality & Audit Expectations

  â€¢ Data quality emphasis Leadership stressed not to load junk/corrupt data into upstream systems. Upstream failures impact downstream processes and high-level reporting; these are monitored at VP level. If upstream issues are found, stop processes and fix immediately.
  â€¢ Ownership & fixes If individuals lack bandwidth, assign fixes to others (ANSI, Kundan, Samir, Nirbal, Prithvi). Daily jobs must run reliably; prioritize upstream data correctness over speed.


Technical Issue: Ambiguous Column causing query failure

  â€¢ Symptom & preliminary analysis A previously-working query now errors due to column ambiguity (same column name in two joined tables). The code ran earlier without issue; suspicion that a schema change added the column or modified tables. Team to check last modification timestamps and change notifications (mod on 9th Jan referenced).
  â€¢ Next steps for resolution First run internal review with Ayapa or Kundan to confirm whether table schema changed and whether the product team should be engaged. If internal review is inconclusive, then contact Rachna. Team to add table prefix or qualification for the column or reverse/roll back unintended schema changes if feasible. Sindhuja can make hosts on the call if needed for ad-hoc troubleshooting.


Coordination & Immediate Steps

  â€¢ Short meetings & follow-ups Nirmal, Prithvi and Suman to connect in next five minutes for immediate handover/coordination. Organizer reminded attendees they can continue work/discuss offline; some attendees left to rejoin shortly.
  â€¢ Availability window Aman offered availability at 08:30 for follow-up if additional details are needed before escalating to Rachna.


ðŸ“‹Overview

  â€¢ Insurance table access requested today; Raj action pending â€” testers should build scripts while access is awaited.
  â€¢ Hand-in-hand validation or results sharing recommended to reduce misinterpretation between rules and code.
  â€¢ Existing script results are being shared; most have been signed off; remaining fixes in progress.
  â€¢ Quantro/dashboard work blocked by platform team since 29 Dec; escalated to Raj; code changes needed for A2 build variables.
  â€¢ Incident routing needs adjustment so incidents flow to correct stakeholder buckets; details pending.
  â€¢ Versioning and audit behavior clarified: audit table keeps all versions; UI displays latest active approved entry; rejected items excluded from main UI (option to create a rejected-screen).
  â€¢ Insurance redeployment replan set for 15 Jan due to schema freeze; RFC in progress; Jean will take over insurance later (handover date TBD).
  â€¢ Pack engine: most data available; one packaging table still lagging due to platform slowness.
  â€¢ Emphasis on upstream data quality â€” stop and fix on any corrupt data; VP-level visibility demands strict auditability.
  â€¢ Query failures due to ambiguous column likely caused by schema change; internal review to precede escalation to product team.


ðŸŽ¯Todo List

  â€¢ Aman:
    
    â€¢ Follow up with Raj to approve insurance table access â€” immediate (ASAP).
    â€¢ Continue escalation to platform team for dashboard/Quantro blocker (ongoing) and share status updates.
    â€¢ Share screen & required field details with stakeholders for A2 build validation (this week).

  â€¢ Raj:
    
    â€¢ Review and approve insurance table access request (ASAP).
    â€¢ Review design for rejected-request screen and REST APIs/BPM and provide approval (TBD).

  â€¢ Dev/Test team (owners: Anand / Sachin / Om):
    
    â€¢ Continue building remaining scripts without waiting for insurance table access (ongoing).
    â€¢ Share script results for review and iterate fixes (daily).
    â€¢ Distribute scripts across validators (assign ~3â€“4 scripts/person/day) â€” implement immediately.

  â€¢ Jean:
    
    â€¢ Prepare to take over insurance work; coordinate handover plan with Aman (date TBD).

  â€¢ Platform team:
    
    â€¢ Resolve platform blockage preventing dashboard deployment (issue open since 29 Dec) â€” priority (ASAP).
    â€¢ Fix slowness for the remaining packaging table affecting pack engine (ASAP).

  â€¢ Nirmal / Prithvi / Suman:
    
    â€¢ Connect in 5 minutes for handover/coordination on RISI MX Cust history backup and related tasks (immediate).
    â€¢ Prithvi to be backup owner for RISI MX Cust history table â€” coordinate knowledge transfer (this week).

  â€¢ Data/Product owners (Ayapa / Kundan / Rachna):
    
    â€¢ Review the ambiguous-column query failure internally; confirm if schema change occurred and whether to engage product team (ASAP).
    â€¢ If unresolved, Rachna to be involved after internal review (before further escalation).

  â€¢ Aman / Team:
    
    â€¢ Create a date-range filtered screen or design for rejected requests if team wants rejected visibility (design review with Raj needed) (TBD).
    â€¢ Finalize REST APIs and BPM flow after Rajâ€™s design sign-off (TBD).

  â€¢ All:
    
    â€¢ Ensure upstream jobs do not publish corrupted data; if issues found, stop downstream processes and fix immediately (ongoing).
    â€¢ Monitor modification notifications for schema changes and track freeze-window constraints (ongoing).


Transcription

00:00:02 - 00:01:03 Unknown Speaker:

So sometimes what happens is like the rule is being understood and the code is being written wrongly. So I would suggest like a hand in hand result check. Or at least if they can share, that would be better, otherwise, what will happen? They will work the same way to you for a review. Yeah, yeah, sure, that works. Now the problem is we don't have that insurance table access. I just raised it today. I already asked Raj to approve that one. And that is the reason they were not able to, you know, complete the end-to-end validation. So I asked them. Till the time we are, you know, getting the access, let's not, you know, wait and watch for that. You can go and start building the rest of the script. Sure, sure, that works.

00:01:03 - 00:02:38 Unknown Speaker:

Such a thank you. So three are with so one, one done, one in progress and one. He would start, uh, three with ANSI, so couple of them from ANSI side are done. One C eight to start because he is a bit initial level of validation. Okay, yeah, okay. And on the existing scripts, uh, we are already sharing you the results. And I can see, like, you know, most of them have been, uh, signed up from your site also. And uh, whatever other fixes are there, we, with respect to that, you know, progress on the Quantro one, uh, give me a second on this one, uh, the dashboard one, uh, it is kind of blocked, uh. Now I have already escalated to Raj. Also, I tagged them and this is something being, you know, waiting on the platform team since 29th of December.

00:02:38 - 00:04:41 Unknown Speaker:

I'll share my screen, uh, and I.And. And there is some code changes that is being required from just to complete our A2. Build on with respect to the validation with those variables that we talked about. Okay. We still have a week. I still need few field details because right now those incidents are being assigned to our group, that USCDU group. But ideally, that would go to your bucket and the, uh, you know, the respective stakeholder for that, with respect to the, you know, whenever you are free, do let me know. I'll just say the screen and then once we get the details, we can wrap up that one. Also, these testing calls, right? I think you have already seen them. Yeah, I already did it. But one question, am I required on all those calls or can't we club?

00:04:41 - 00:06:57 Unknown Speaker:

I've just intruded you and that's not to be there if you want to just pitch in anything, but not necessarily. You can see based on your availability here. Maybe first of all, I'll attend and. And then can't we keep those calls at the same time for all those three different people? No, I wanted to specifically keep them separate just to see their own, what do you say? Okay, I got input and go. Yeah, I got it. Because the way they are using the platform or just say the rules, right, that itself is different how they're thinking about it. So I just, that's why I wanted to keep it separate. Anyway, I think it was for the last person, right? US. Thank you for watching. Not create a request if the request is valid.

00:06:57 - 00:09:11 Unknown Speaker:

My bad, I I opened a update one, give me a second, okay? So in case it's a valid request, it would go and create a human task again. Why a human task again? This is a point of discussion I need to take up with Raj. This would be basically this bundle piece of information would transfer to the SFTP file, and then that would go to the Lumi portal. Those all are different. Right now. I am just talking about the ace part. Okay, yeah, in case of updates. And again, the valid request, if it is a watcher list, not the case ID, rule ID. Just for a hypothetical scenario, if it is a rule ID 1, which has been inserted to your database on 8th of Jan, if it is approved, it would remain active and that would be displayed to your UI.

00:09:12 - 00:11:20 Unknown Speaker:

The same rule ID, when updated on 9th of Feb, it has been rejected, so it would remain disabled, So you are still seeing... the same rule, this would become active, this would become inactive, so you would be viewing this 10th March rule. Okay, now again, somebody has come on 10th of April, updated it, uh, and it's still in, you know, in progress status. Like it's not approved, not reject. It would be viewable to you over the UI screen, where you can approve or reject. But actual rule, this is the planning that we had right now. So these all four entry would be there in this audit table. But at the end of the day, whichever rule is with the active status, that would be displayed to Okay. Now somebody has updated this role and this went for a approval.

00:11:20 - 00:12:36 Unknown Speaker:

Yeah, okay, but but no, no. Just take an example, uh, let's not come to this step, okay so, or just take an example of this one. So this remains in progress, but this is not visible. Request or rejected. Oh yeah, and you are still viewing this one, yeah, but if you have approved it, in case you have proved it. Previous person inactive was I go okay. And now in this, in this part, you will be viewing the latest version of this 10th module. Whatever I did, my question is, where are you saving these versions? This all person would be stored on the create a SFTP file that would be taking the latest date. Active one entryway for that day, and then it would be sending it to the Lumi Warehouse. Uh, such, in one question, uh.

00:12:36 - 00:13:34 Unknown Speaker:

The rejected ones won't be showing up in any of the screens. Now, if you want, I can show you the reject one also to a different screen, those which has been rejected. With our date range, we can create a date range within that data, which whichever request is being rejected, we can add another screen, uh, uh, like this one where you can see the rejected request over there. Okay, okay, if you want, if that is something that we can discuss first, yeah, yeah. So if you want, we can create another screen for the rejected request. And we'll give our date range. Because I won't be displaying all the rejected lists. Makes sense. The date range, we already have this created timestamp and all these flags are already included. I just need to review this design with Raj once he approves.

00:13:34 - 00:14:39 Unknown Speaker:

This one, the BPM in that I have shown and some of the REST API that we are going to create it for this one. So once I review, I'll update you guys on this one, on this Qt Excel. Yeah. Okay, Sachin. Yeah. Okay. Okay, that's it from my side. Anand and Rachna, any questions? Otherwise, I'll go to insurance one. All good. All good. Thank you. Thanks, Anand and Rachna. Aman and Om, I already dropped an email regarding that deployment. I think this was due to that phase period that is still intact, 12th of Jan. And due to that reason, They are not allowing any schema change that would break the existing schema. And that's the reason I had to replan for the insurance. Do stable redeployment on 15th, so RFC is already being created and once that deployed, I'll keep you guys posted on and on.

00:14:39 - 00:16:12 Unknown Speaker:

With respect to Pack engine, I think now all the data's are already available. That's the only thing we're doing right now. Oh, that's the only thing. Uh, because on the uh, all those four packaging table, because one is still not lying, you know, right there. That is some. Due to the slowness they need to fix, rest of the four tables are already with respect to those timestamp variable. Okay, thank you. Okay. Yeah. Aman, just one quick question. This would be taken by Jean on this insurance one and if yes, by when we are planning to have our handover, a complete handover to Jean? No, I won't be able to give you the date. She will take it, but when she will take it, we have not started.

00:17:11 - 00:19:15 Unknown Speaker:

If it has been made on 4th, if somebody has observed, why did we waited a week to fix that one? Thank you. Not be dumping junk data over there. Please be, uh, you know, these all are auditable records, please make sure the report are going to form, the alerts are going to get formed. Uh, there are lots of, uh, dependency. This is the main upstream process, based on which the there are two more downstream processes are there, right? That would get impacted if your data is corrupt in the upstream leadership. Okay, so that report are being viewed by the VPS label people, it's not that, you know, even Purna is viewing it. It has been monitored at the VP level. So please make sure the upstream runs without any issues, and if you find any issue, stop everything and fix that immediately.

00:19:15 - 00:20:11 Unknown Speaker:

Yeah, and then I'll leave it to you, you have to read it how you want to, and I'll leave it to you. Jobs on a daily basis. Okay. Even if you don't have a time, if you want, let ANSI to fix it or Kundan to fix it or whoever pitch in. If you have the bandwidth, go ahead and fix it. Otherwise, whoever is free, ask them. Right now, last week, Samir was completely free. We did not utilize him properly apart from those script validators. And now we would have Nirbal and Prithvi also pitching in from upcoming week. So please make sure that the script is being assigned to respective folks. Maybe three or four script per day, that would not be a lot.

00:20:11 - 00:21:20 Unknown Speaker:

Even if you are running 10 scripts, even if you distribute a couple of scripts, that would not be lots of work for each of the folks we are working in the team. We have a backup for RISI MX Cust history table. Basically, I would love you with Prithvi. So, make Prithvi as your backup so that in case you are not working, Prithvi can take care of that. Sure, sir. Okay. Okay, then. Nirmal, Prithvi and Suman, let's connect in next five minutes. Anyone have anything? Sure. Not sure, like, why it is wrong, it is getting an issue. But previously it was wrong due to some ambiguity of a column, so we just need to add that specific table before that column name. So that one. I just want to check with her and I have to push the chain, you know, no?

00:21:20 - 00:22:06 Unknown Speaker:

But first of all, did you discuss with Power Kunden on that? Because it should not be, I am aware with the issue right now, it should. Yeah, I have a question. No, no, it should not be. That is directly going to the product team. So I have an internal discussion. First of all, review it, either with Ayapa or Kundal. And then, uh, if if required, uh, you can reach out to Rachna. First of all, have an internal view. Yeah, sure, okay, yeah, yeah. In case in case required, you need any details from me, I'll be free at 8 30. Do let me know, uh, before reaching out to Rachna. Is that issue like, I know what it is? But I just want to double? Just have a review, man, just add a second opinion.

00:22:06 - 00:23:14 Unknown Speaker:

If we can fix it by our own. No need to reach out to Rachna, but, uh, if you guys are still confused, do let me know before reaching out to Rachna. Yeah, rest of the folks feel free to wrap up, so maybe you can continue such a if you are able to see, it's coming up. Yeah, so there is a problem. So this code is actually ran fine previously, but like now, it is showing an ambiguity. Because this same column is present in both these two, where it is these two tables, so so why? Why it was not coming earlier? Did they modify this table. But this column should be added to one of the table or like... These are the SOR tables, right? So, I see the leisure event and this fulfillment one.

00:23:14 - 00:24:17 Unknown Speaker:

So, did they added this new column or to any of the table due to which it is coming and camping? Yes. First of all, analyze that one. Is there a modification to this table? Because look, this was a freeze date. Here. So we are not expecting any schema change. So how it was running earlier in that case, uh, that part, like, I'm, I'm really surprised, actually. No man. Look, this is the problem right? When we when we go to the product team, right? Um, so how it was running earlier in that case, because. I think you continue. Make me host or submit host. I will check that. Let me make you the host. No, man. I don't have the rights. I think Sindhuja was having the rights to make you the host.

00:24:17 - 00:25:22 Unknown Speaker:

Or you guys can connect over your own hands. No, no. Whoever is on the call will become the host. So you don't have to worry about it. Okay. Yeah, you can continue if you need. I will not end the call. I will just leave. Okay? Yeah. Okay. Anything else for me can I drop off? Yeah, yeah, yeah, no problem. Cindy. Thank you. Thank you. I'll be. I'll be. I'll be joining in a couple of minutes. Uh, just have a, you know, a glass of water. You guys can join over that place. I'll join you. Okay, yeah, check this one. If they have added new new column, right, there would be a notification sent on the your email box. I can see that last mod was on 9th, I don't know what was the daily stand up. January 12th, Amex.2026-01-12 Data Transformation Flow Demonstration

Creation Time: 2026/1/12


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-12 10:00 (Duration: 807 seconds)
  â€¢ Location: No location mentioned
  â€¢ Attendee: No attendee names mentioned


ðŸ“’Meeting Outline


Data transformation flow canvas and steps

  â€¢ Creating a transform column The presenter demonstrated creating a Transform Column named "Transform Call Demo". They opened the transformation editor, added two source columns (one from a join step referencing CM11 and another named trans data), created a concatenation expression combining the two columns, validated the expression successfully, clicked Done, and saved. The transform step now contains the two input columns and the resulting transform column with the concatenation logic.

  â€¢ Transform step capabilities Within the transform step the presenter showed additional available options: filter condition, group by, order by, unnest, and a step-level SQL preview. The preview displays the SQL for that transformation step confirming the concatenation of the two columns. The schema view reflects the two input columns and the new transform column.

  â€¢ Creating an output node The presenter created an output node named "output test demo flow". They explained output options: table type (staging vs final), table name, and write mode (append or override). After selecting staging and saving, the output step was created. This completes the primary node sequence for the example flow.


Flow utilities and management features

  â€¢ Auto-arrange and parameters Auto-arrange functionality was shown to neatly lay out nodes in the canvas for visibility. The Parameters panel lists flow-level and platform-level parameters and offers the ability to create new parameters from the flow UI.

  â€¢ Compare (flow JSON and sequence) Flow JSON compare allows selecting two versions of a flow to identify differences. In the demo only version 0.1 existed, so comparing the same version showed no differences. Sequence compare similarly showed no differences for identical versions.

  â€¢ Flow actions: clone vs branch The presenter explained cloning a flow creates a completely new, separate flow. Creating a branch creates a new version within the same flow (versioning). Both options were briefly demonstrated in the actions menu.

  â€¢ Re-import functionality Re-import allows importing a flow (or a specific version) back into the environment. In the demo the flow was created from scratch, so re-import was not used, but the dialog for selecting a version to import was shown.

  â€¢ Preview flow SQL The preview feature renders SQL for the complete flow (end-to-end) so users can inspect generated SQL before execution.

  â€¢ Save, Submit, Publish / Promote Save allows continuing edits later. Submit finalizes the flow for publishing and prevents further edits. After submit, Publish/Promote options appear to move the flow through environments: E1 -> E2 -> E3. The presenter showed the publish dialog and explained the promotion journey but did not execute promotion in the demo.


Next steps and execution

  â€¢ Execution & next demo The current demo focused on building the canvas and flow structure. Execution of the flow will be covered in a separate demo (data transformation accelerator and composer demo).


Modify Input Source functionality (brief mention)

  â€¢ Modify Input Source The presenter opened a flow under project "Data Trans" (version 0.1) and introduced the Modify Input Source capability that allows changing input sources for a flow. Details were introduced but not fully demonstrated in this recording.


Extraneous conversation (irrelevant content)

  â€¢ Several lines of unrelated verbal exchange (personal/domestic conversation) were present in the recording and are not relevant to the demo content.


ðŸ“‹Overview

  â€¢ Transform column creation: created "Transform Call Demo" combining two columns (from join CM11 and trans data) via concatenation; expression validated and saved.
  â€¢ Transform step capabilities: filter, group by, order by, unnest, step-level SQL preview; schema updated with transform column.
  â€¢ Output node creation: created "output test demo flow", set table type to staging and save; discussed append vs override modes.
  â€¢ Canvas utilities: auto-arrange for layout; parameters view and creation.
  â€¢ Comparison tools: Flow JSON compare and sequence compare (no differences shown for single version).
  â€¢ Flow actions: clone (new flow) vs branch (new version); re-import option; preview full flow SQL.
  â€¢ Lifecycle: Save (editable) vs Submit (locks for publish); Publish/Promote workflow E1 -> E2 -> E3 (demonstrated but not executed).
  â€¢ Execution: Actual execution of flow deferred to Data Transformation Accelerator & Composer demo.
  â€¢ Modify Input Source: capability introduced; not fully demoed.
  â€¢ Irrelevant personal conversation captured in recording and excluded from technical actions.


ðŸŽ¯Todo List

  â€¢ Owner: Flow author (unspecified)
    
    â€¢ Finalize and document transform expressions and test cases, by [date not provided]
    â€¢ Configure output table details (confirm table name, staging vs final, and write mode), by [date not provided]

  â€¢ Owner: Platform/DevOps or Release owner (unspecified)
    
    â€¢ Define promotion schedule and gating criteria for E1 -> E2 -> E3 promotion, by [date not provided]
    â€¢ Prepare environment access/permissions for publishing flows, by [date not provided]

  â€¢ Owner: Demo presenter / Product team
    
    â€¢ Deliver follow-up demo covering flow execution in Data Transformation Accelerator & Composer, target date TBD
    â€¢ Provide a clean recording or transcript excluding unrelated conversation, and clarify attendee list and location for official records, by [date not provided]

(Notes: Meeting recording included personal unrelated chatter; confirm and redact if storing official artifacts.)


Transcription

00:00:04 - 00:01:15 Unknown Speaker:

Columns These are coming up here. So this is the trans state and the CM11. The next step is I click on this Transform column and name this as Transform Call Demo, and then click on this Edit icon. So this provides me the window to create this transformation step. Let's say so I want to get it say I conquered CM11, it says in the join step, and another column was trans data. So, it's trans data in this join step. Okay, and then I click on validate. So, now this expression is also successfully validated. Okay, and then I click on done. And then save it. So here in, uh, I have. In this transformation step, I have added these two columns, and then I have created a simple, complete operation on these two columns.

00:01:15 - 00:02:17 Unknown Speaker:

And here is my transformations uh transform column. Okay, now, if you see here, I have the transformations done also. When I go into this step, I see there are options to do other transformations also. I can have filter condition, I can have group by conditions, I can do order by unnest on these, and then I also have the option to preview the SQL for this particular transformation step. As you see, this shows me that these two columns have been concatenated in this transformation column. So this is basically a step level preview. Okay, now? And then in the schema you see the same thing, that two columns which I added, and then the transform column, so this was about the transformation step. Then the next step for me would be to create the output node.

00:02:17 - 00:03:15 Unknown Speaker:

Let me just zoom it out a bit for the increased visibility now as I click on output. First step is to provide the output node name. Let me call this as output test demo flow. Now in the output options, what you see is first is the table type, whether it is staging table or the final output table. So let's say I keep it at staging. Table name I have already given. Type mode can be append or override. Depending on whatever the whatever is the user's requirement, then I click on Save and next so this creates the output step for me. So these were the different steps which can be leveraged in order to create a particular flow. Now moving on, the I'm like, I'll explain what all we have is functionality for this particular flow.

00:03:15 - 00:04:13 Unknown Speaker:

First is the auto arrange functionality, which auto arranges this particular flow in. So that this is easy for viewers to view the different nodes and steps in the flow. Next is about parameters, which gives me a list of all the parameters which are associated with this particular flow, as well as platform level parameters. Then I also have the capability of creating new parameters from here. Next is the compare thing, wherein the users can compare flow as well as compare the sequence. So, I'll just move on to these one by one. So, first is the flow JSON compare wherein the user can select two different versions of a flow. So, since right now I just have a particular version in this flow that is just 0.1. So, then there won't be any differences since we are comparing the same thing.

00:04:13 - 00:05:12 Unknown Speaker:

So, this shows that there are no differences in the two selected flows. And similarly, I can also go for comparing the sequence. So here and again, if I click on compare, this will also not show any differences. Because these are the same versions of a particular flow. Next, I go to the flow action steps, wherein I get the option of cloning a flow. So clone flow basically allows the users to create another flow. From this particular flow, so this is about creating a completely new flow out of this flow. Whereas the branching option, which you see here, create a branch. This allows users to create a new version in the same flow. So, clone flow is creating a completely new flow, whereas branching a flow is about creating a new version in this particular flow itself.

00:05:13 - 00:06:48 Unknown Speaker:

Then we have the re-import functionality, which allows the users to import a flow again. Okay, so if I click here, you see this asks me this, but which version I wish to, uh, re-import or import again? But since we I am on the base version and I don't, I haven't imported this from somewhere, but I have just worked this, worked on this from scratch, so, In case the user wants to import the flow again, this is where the user connection upon. The next thing we have in line is preview, which allows me to view the SQL for complete flow. So that is the flow SQL. Okay, So, put that. The user also needs to save and submit a flow, so once the users create must have created this particular flow, the next step would be to save and submit.

00:06:48 - 00:08:05 Unknown Speaker:

The difference between these is save allows the users to edit a flow once they are back, whereas submitting a particular flow does not allow further edits on this flow. Okay, once you submit a flow, the Publish Option option comes up, which allows the users to publish this flow or promote this flow. From E1 to first E2 and then further to E3. So the first step is to submit a flow, then the user can publish to E2 and then publish further to E3. Now, once I click on Produce Equal, as you see, this gives me. And then once the user clicks on publish, this allows whether we want to promote this particular flow to E2. I won't proceed with this in this particular demo. This was just a test flow, But this will be the journey which the user would need to follow.

00:08:05 - 00:11:53 Unknown Speaker:

That this flow, which you see here, is created in E1, you need to promote it to E2 and then further to E3. So this was about the different functionalities we have on offer in this data transformation utility. This demo was particularly concerned with canvas of around creating a particular flow. Now, the next step to this would be the execution of the flow which will be covered in the data transformation accelerator and composer demo. Thank you so much for the attention. Have a nice day. We are going to have a look at the Modify Input Source Functionality Data Transformation Tool. So, as you see, I have opened a flow named this under the project Data Trans. I am currently on first version of this flow. And under this functionality, what we allow is that a user can change.

00:12:06 - 00:13:18 Unknown Speaker:

Tell me. Uncle, your bus is not coming. Tell me, how many people are there? They are coming. They are preparing food. They are leaving at 10.30. Okay. I will reach there on time. You will reach there? Sir, are you coming here? There is no vehicle. They will take you to Reddy. Okay, can you come here now? Yes, I will come. Uncle, there is a call at 11 o'clock. It is from East West. Oh, 11 o'clock call. Okay. Then ask them to come here. Ready? Okay. I will come. Wait here. Take the laptop. I will tell you later. Why did you call me like this? Okay, I'll be there. If you come, I don't even know how to come.2026-01-12 Project Migration and Development Status Update

Creation Time: 2026/1/12


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-12 10:48 (Duration: 1042 seconds)
  â€¢ Location: [Not specified]
  â€¢ Attendee: Praveen, Anu, Panavit, Samir (mentioned), Rajan (mentioned), Ayapa (mentioned), NC/App (mentioned), Rajendra Bhatt (mentioned), plus unnamed team members


ðŸ“’Meeting Outline


Project timeline & scope (Migration to Lumi / Cornerstone â†’ Cloud)

  â€¢ High-level status The team completed high-level HMD planning and is transitioning into development, which may have started immediately. Migration from Cornerstone to the new platform (Lumi â†’ BigQuery/cloud) is underway. The project is large and long-term â€” estimated minimum two years of sustained work with additional tasks likely beyond that.

  â€¢ Pipeline & incoming work Multiple projects are in the pipeline; several new deliverables (six to seven items) must be delivered in upcoming sprints. Expectations from stakeholders vary, and similar migration efforts across the organization will continue to generate new work for the team.

  â€¢ Work allocation / expectations Team members should engage with data-related tasks when they have bandwidth. Not everyone is expected to be dedicated full-time to data activities â€” work on it as time permits.


Data migration & ingestion details

  â€¢ Data responsibilities Cornerstone team will handle pulling data from on-premise to cloud (data ingestion). Our team will need to migrate and adapt scripts and flows related to that ingestion, including bulk ingestion, real-time ingestion, and daily flows.

  â€¢ Lumi framework usage Lumi provides prebuilt flows and documentation (bulk, real-time, daily). The team must leverage Lumiâ€™s flows and modify where necessary, integrating our changes back into the framework.

  â€¢ Training / required learning Relevant Lumi training modules identified: Lumi Overview, Lumi Usage Policy, Lumi Data Transformation (completed by the speaker); next required items: BigQuery Overview, Data Ingestion Lumi Part 1 and Part 2. Team members should watch the Lumi portal videos and review docs â€” videos explain steps and flows clearly. Some pipelines may not be visible until names are added to access lists.


Access, permissions & onboarding

  â€¢ GCP & project access Speaker has GCP access but needs project-specific access. Action: raise access requests (IEQs) for required project resources â€” check with Ayapa for the list of required IEQs or consult the Confluence page. Avoid raising excessive IEQs to prevent manual justification to management (e.g., Rajendra Bhatt).

  â€¢ Table-level access Table-level access requests were included by mirroring Samirâ€™s request; these may be accepted or rejected. Current recommendation: request only necessary project access for now.

  â€¢ Who to contact If access issues arise, check with Ayapa, NC, or App (team members who maintain Confluence and access lists).


Development status â€” CL/Agents/LLM solution

  â€¢ Codebase & POC The developer has implemented code for agents (orchestrator, execution, generation) and completed the POC â€” demo-ready. They can connect to Gmail, extract CLs, and get recommendations. The implementation follows the architecture document; three sub-agents were created as planned.

  â€¢ Data loading & master data Phase zero master data (5 years of historical data from Excel) still needs to be created/loaded. This is necessary for the Retrieval-Augmented Generation (RAG) component.

  â€¢ Current functionality Without RAG, the system can:
    
    â€¢ Retrieve CL-to-CL mappings and identify related CLs (e.g., 5â€“10 related CLs).
    â€¢ Compute code differences, organizer comments, and CL-level details.
    â€¢ Provide that CL-level context to the LLM with a system prompt to determine whether an integration test is required. The system produces reasonable results currently, but accuracy will improve once RAG (historical data retrieval and QA correlation) is implemented.

  â€¢ RAG requirement Implementing RAG is identified as the missing piece to correlate CLs with five years of historical QA/sales data and to improve answer accuracy.


Meetings & logistics

  â€¢ Attendance / scheduling Some teammates will be out on holiday (14th noted as holiday). The speaker will not be present tomorrow but plans to be back on Wednesday (dates discussed). Agreement to catch up on the first working day after holidays.

  â€¢ Communication Request to ping/call to coordinate meeting attendance and follow-ups.


ðŸ“‹Overview

  â€¢ Development has moved from planning to active coding/POC; HMD completed.
  â€¢ Migration from Cornerstone to Lumi/BigQuery is ongoing and will be long-term (>= 2 years).
  â€¢ Cornerstone team will perform raw data pulls; our team must migrate and adapt ingestion scripts and flows.
  â€¢ Lumi training modules remain to be completed: BigQuery Overview and Data Ingestion (Part 1 & 2).
  â€¢ Access requests: project-specific GCP access required; consult Ayapa/Confluence; avoid excessive IEQs.
  â€¢ Codebase POC for agents (orchestrator, execution, generation) is implemented and demo-ready.
  â€¢ Master data loading (5 years) and RAG integration are outstanding; RAG is needed for higher accuracy and historical QA correlation.
  â€¢ Meeting coordination planned around upcoming holiday; team will sync on the first working day back.


ðŸŽ¯Todo List

  â€¢ Ayapa / Confluence maintainers:
    
    â€¢ Provide the list of required IEQs and project-specific access steps to new team members â€” AS SOON AS POSSIBLE (no hard deadline given; coordinate immediately).

  â€¢ Developer / Speaker (owns Lumi trainings & initial setup):
    
    â€¢ Complete Lumi trainings: BigQuery Overview, Data Ingestion Lumi Part 1, Data Ingestion Lumi Part 2 â€” by next sync (recommended within 1 week).
    â€¢ Verify Lumi portal access and watch the ingestion pipeline videos; note pipelines not visible due to access and report missing visibility to Ayapa â€” within 3 days.

  â€¢ Access requester (new team members):
    
    â€¢ Raise IEQs only for necessary project access (consult Ayapa/Confluence); avoid unnecessary table-level requests â€” raise immediately; adjust based on responses.

  â€¢ Data engineering / Cornerstone team:
    
    â€¢ Continue on-premise â†’ cloud data pulls (bulk, real-time, daily) and share ingestion outputs and expectations for script migration â€” ongoing.

  â€¢ Development team (owners of agents & RAG):
    
    â€¢ Prepare and load master data (5 years historical Excel) for Phase 0 to enable RAG â€” target: as soon as access/data available (coordinate with data team).
    â€¢ Implement RAG retrieval and integrate with LLM prompts to improve accuracy for QA correlation and historical context â€” after master data is loaded; target: next sprint planning.
    â€¢ Demo current POC to stakeholders (show CL extraction, recommendations, code diffs, and LLM decisions) â€” schedule demo in coming week (coordinate exact date).

  â€¢ Project manager / Lead:
    
    â€¢ Coordinate sprint priorities for the six-seven new deliverables in pipeline; clarify stakeholder expectations and timelines â€” before next sprint planning.

Notes: Where explicit deadlines were not given in the meeting, action items are prioritized with suggested timing and require owners to confirm exact dates. If any attendee was misidentified or additional attendees should be listed, please reply with corrections.


Transcription

00:00:13 - 00:01:29 Unknown Speaker:

We have done some high level HMD and all these things. Uh, yeah, so development will start, maybe, I think, from today, or somehow. Okay, and then we have this, uh, data in this and where we are, you know, migrating the data from Cornerstone. Okay, so everyone should be working in all these everything like data. Yeah, that's okay. It's not like, you know, you are specifically dedicated if you have the time, you have to work. How long this will go? A lot of work is there for us, at least for next two years. What? How long this will go? At least two years work is there for us. Yes, it will go, you know, more than that one. It's a long time project because we this migration activity right now. They started when we were there.

00:01:29 - 00:02:10 Unknown Speaker:

So there are a lot of projects in the pipeline, people are working on it and we are also getting it. Because you deliver a lot of things and some new things. This year, we got a lot of new things in the pipeline that they are never studying, right? Six, seven things are there that we have to deliver by this, the sprint and not the system that you can say, okay, okay. So they have the different expectations. So going forward also, you know, we will be getting this because this is, uh, uh, you know, everyone is migrating to this normal setting. Okay, okay, got it. Why I am asking is that Lumi trainings most on data side, that do I really need to go through them properly?

00:02:10 - 00:03:09 Unknown Speaker:

Or that's why I was asking data we, we are not handling the data, but the team is there. Okay, sorry, team is there. Okay, yes, so we are not going to query and do all these things. So from cornerstone side, they will pull the data. Okay, that is. You are talking about data ingestion part from on-premise to cloud. Yes. That they would be doing it. But those scripts, okay, once they migrate the data, we need to migrate the scripts related to that. Oh, there is, you know, there are different steps in there for this migration one. So, there are different approaches there, like, if you see, that's the bulk ingestion is there, and somewhere they have this real-time ingestion time. Daily data is there. So, those all flows are there that are already provided by the loop table.

00:03:10 - 00:04:11 Unknown Speaker:

We have to, you know, leverage that one and then, you know, changes will be required. So, in between, we will, you know, perform our activity and pass it to the framework methods again. Okay. Okay. Basically, it's a kind of migration only. So... so for me, the list it is showing is Lumi Overview, Lumi Usage Policy, Lumi Data Transformation. I think till here I completed, I mean, this is anywhere required. Next is Big Query Overview Data Ingestional Lumi. Part one, data will be two. Uh, two parts will be there. Yeah, part one and part two. Yeah, yes, correct, so those two I need to go through that. Yeah, just go through those documents pretty straightforward. Okay, just go through that one and you have the access to Lumi Portal, right? Yeah, yeah, yeah, you can see it there.

00:04:11 - 00:05:03 Unknown Speaker:

There are a few of the pipelines, I think you will be not able to see those pipelines because your name is not there, right? We have not added your name. And that is not needed. Also because you will get the access and all, so they will not tell you. So, you know, just go through that in the video. It's, you know, they explained it quite nicely, you know, what are the steps, what they are doing, how the things are and all. So, you will understand it. Okay, okay. Anything else? Yeah, that's it for now. I'll go through these trainings. On code base, code base access, I have reached access to GCP only. Uh, no, once you have to raise the access for the project specific, just check with Ayapa. Once.

00:05:03 - 00:05:54 Unknown Speaker:

Okay, he has the list of all these IQs, he will share with you. Okay, okay, or in our conference page. I think it is there so you can get it from there. I don't find any complaints related to that one you share with respect to GCP. If you didn't find this, check with NC or App. Okay, these guys are maintaining this conference. So, they will let you know, you know, which all IEQ will be required basically for project related access, okay. Do not raise too many IEQs, like that we will go to, you know, the Rajendra Bhatt and, you know, we have to provide the explanation. Okay. Just know me access, project access, I think right now this much is enough for you. Okay. And for GCP, whatever Shamir has, I raised the same request.

00:05:56 - 00:06:54 Unknown Speaker:

Okay, even that is fine. Table level access you have to reject. But not sure you need it or not because whatever the access Samir has, Rajan means that you also needed that many things. I simply added all that list. Let me see what happens. That's fine. They will reject or accept it. That's fine. Okay. You are coming tomorrow? Tomorrow, I will not be there. I will come on Wednesday. Wednesday, what is the date? Wednesday is holiday day. We have on 13th. 14th. 14th is holiday for us. I must go there. I don't need it. Okay, okay. Got it. Tomorrow, it's not. It's too hard. I'm coming daily. Yeah, I know. I try it, yeah. Fine, let's see. That's fine, that's fine. We'll meet, you know, on the first day.

00:06:55 - 00:09:09 Unknown Speaker:

Because my stage holiday, if you are coming, most probably I'll be coming. Yeah, yeah. So let's catch up on the first day, huh? Sure, sure, yeah. All right. Okay, thank you. Can you just ping me? Sure. Sure, come then, yeah. Yeah, thank you so much, come then. Okay. Thank you. Bye. Okay, thank you. It's the same error. Is everyone coming? Yes, they are coming. Yes, they are coming. Yes, they are coming. Yes, they are coming. Yes, they are coming. That's what I'm excited about. Do you think it will take time for you to stop? No, I don't think so. Tell me, what is this? You are the only one who is watching. We don't have anything to stop. I don't have anything to stop. I don't have anything to stop. I don't have anything to stop.

00:09:09 - 00:11:12 Unknown Speaker:

I don't have anything to stop. I don't have anything to stop. I don't have anything to stop. I don't have anything to stop. I was afraid that my mother would come to my house. I was afraid that my mother would come to my house. I was afraid that my mother would come to my house. I was afraid that my mother would come to my house. I was afraid that my mother would come to my house. Exceptionally large stages are occurring. Yes, not, obviously it could have been done, but not done now. Sprint and go to back and forth separate operator and using the is running here in the back and freedom for you, you can find and separate the back and separate. Hey, hi, man. Okay, hi, everyone. Hi, Panavit. Good morning. Hey, Praveen.

00:11:12 - 00:12:27 Unknown Speaker:

Hi, Anu. Good to see you. Let's see the link. Okay. Since you are taking care of the solution alone, right, we will go. Because what we agreed that there is with him, he must have joined the CL verification whatever you already have, right? Yeah. You use that and above that add this. The existing CL verification function. Yeah, yeah, yeah. We will add them. You were supposed to get access and everything. Is the accessing solved quickly? Yeah, so right now the situation, I mean, I'm in position to run the code. So you can start coding then. I hope you can start it. Yes, sir. What I want to understand is how far are we through and what we're not through. Okay. So with respect to development, I have created the test cases. Sorry, not test cases, the code part.

00:12:28 - 00:14:34 Unknown Speaker:

Agents have created, I mean, I can even share my screen. Okay. So, yeah, I created in Critic, I created all the code base with respect to all the agents, starting with orchestrator and then execution, generation. So, POC, everything is done. It's ready for even demo also. Because when you say, that means I'm able to connect to Gmail, I'm able to understand the CL, I'm able to extract the CL out, I'm able to get the recommendations, because the CL had all those, remember the three sub-agents we created? Yes. According to the architecture document, have you done everything coding? Yes, yes, yeah. Except data loading in phase zero, we have to create master data, right? From the excel data five years data that is available so that will be used. Okay. Okay. Okay. So probably one thing is, uh.

00:14:34 - 00:15:24 Unknown Speaker:

Without a rag implementation, where we would be looking out for five years. Historical data and matching this CL with any of those and correlate the response, I mean, the QA data. For that and for this, what would be the answer? So this is the rag implementation, right? This thing is not done other than that. So currently, what it does is it gets from one CL number to another CL number, what, uh, CLs are existing. Okay, it will get like 5, 10. What hour is there what the QA team runs to get all the sales between two day? Two CLs, right? It gets that if you provide both of them and two and then each CL, it will go through and it will get the code difference. Uh, organizer comments, Okay, and then, uh, CL details.

00:15:25 - 00:17:08 Unknown Speaker:

All the complete information it fetches at a CL level, CL number level and try to give it to the LLM for taking it digital. So we have put a prompt or system context for the LLM that we are looking to check if it requires an integration testing or not. So if you ask me, it is doing a nice job right now. But if I do with the RAG, it will be more accurate. Yeah, yeah. So could we let us do it okay but demo it doing it as well. Okay, okay. I'm coming up with a . Sorry, yeah, go ahead.2026-01-12 Q-Track Overview and Implementation Planning

Creation Time: 2026/1/12


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-12 18:31 (Duration: 3239 seconds)
  â€¢ Location: [Virtual meeting â€” recorded session]
  â€¢ Attendee: Sachin, Suman Reddy Gupta, Prithvi, Nirmal, Monica (mentioned), Anish (mentioned), Raj (mentioned), other team members (unspecified)


ðŸ“’Meeting Outline


Q-Track (Data Quality Monitoring Tool - overview & alerts)

  â€¢ What Q-Track is Q-Track is a data quality monitoring tool (dashboard + automation) that runs predefined rules on BigQuery tables (example: USCDO/USCFO tables). Rules are configured per table/column and run on a schedule; results are persisted and automated alerts are generated when rules fail.

  â€¢ Rule examples and behavior Examples explained:
    
    â€¢ Primary key uniqueness: if a RESI_user_id duplicate appears, an alert is generated.
    â€¢ Non-null constraint: if bonus_amount is null when it should not be, an alert is generated.
    â€¢ Threshold/percentage checks: e.g., count of future dates should not exceed 5% of total; expected value 0.05 vs actual triggers failure.
    â€¢ Valid-value checks, missing-value checks (100% field-rate), stagnant checks, etc. The presenter explained how business statements (intake form) translate to SQL checks and how expected vs actual values determine pass/fail.

  â€¢ Data sources & scope Q-Track rules run on the table after ingestion; it does not track which upstream source supplied the data. Rules are table-level or column-level; a single table can have multiple rules.

  â€¢ Notifications When a rule fails, an automated email alert is sent to the business distribution list (table owners / business users). Emails are system-generated by the automation; team does not manually compose these. Pass results are stored for monitoring but do not trigger business notifications (except internal monitoring).

  â€¢ DQ repository & results storage
    
    â€¢ Rule definitions (DQ rule ID / model ID) are stored in a DQ repository table (master).
    â€¢ Execution results (pass/fail, actual/expected values) are written to a DQ results table.
    â€¢ The implementation scripts read rules from the repository, execute queries, store results, and trigger alerts based on results.


Q-Track UI (Issue Management / Human Tasking)

  â€¢ Problem addressed A high volume of daily email alerts makes manual triage difficult. Q-Track UI (also called DQMI / issue management) centralizes failed results so users can take action without sifting through many emails.

  â€¢ Daily processing & human task creation
    
    â€¢ At end of day, the automation aggregates the DQ results produced that day (e.g., 100 results).
    â€¢ The downstream process ingests all results; for passed items no action is required.
    â€¢ For failed items (e.g., 10 fails), the system creates â€œhuman tasksâ€ in the Q-Track UI â€” one task per failed rule â€” so users can triage them.

  â€¢ UI workflows / possible actions
    
    â€¢ Users can view failed records, open details, reserve/assign tasks (to avoid duplicates across a distribution list), add comments, request rule deletes/modifications, or mark whether the failure is a true data issue.
    â€¢ If the failure is a true data incident, the UI can create a ServiceNow incident (or similar ticket) to route to support/engineering for remediation.
    â€¢ If the rule itself needs revision or deletion, the user can request changes (which then go through product/PO approval).

  â€¢ Users of the UI
    
    â€¢ Intended users are business/table owners and teams responsible for data quality. They will triage and mark failures as expected or requiring action.
    â€¢ The UI is not only for internal infra team; business users will also use it to review failures rather than rely solely on email.


Q-Track Reporting (Monthly / Quarterly Reporting & Trends)

  â€¢ Purpose Reporting visualizes rule run statistics over time for leadership and stakeholders: unique rules executed, pass/fail counts, trends, pass/fail ratios, and lists of frequently failing rules.

  â€¢ Data & visuals Reports will be based on historical DQ results (e.g., last 12 months), showing trend analytics and rule-level breakdowns. This helps leadership see overall data quality health.


Q-Track Self-Serve / Onboarding UI (in discovery)

  â€¢ Goal Provide a UI where business users can submit new DQ rules via a guided intake: select use case, table name, check type, variables, filter conditions, expected values, frequency, etc.

  â€¢ Flow
    
    â€¢ User fills intake fields (the intake form is currently an Excel template in use).
    â€¢ Submitted rule creates a rule definition in the DQ repository (DQ rule ID auto-generated).
    â€¢ Rule goes through approval (product/PO) before being deployed.
    â€¢ Planned capability: allow users to modify existing rules or create new ones from the UI. The design/discovery is in progress.

  â€¢ Advanced possibility (planned) Exploration of generating SQL queries automatically from the user-provided rule description (agentic tool). This is in discovery; implementation not started.


Controls & Rule Efficiency (future initiative)

  â€¢ Control analysis A control initiative will evaluate how effective existing rules are against changing data distributions and may modify or adjust checks automatically (in pipeline / implementation stage). This aims to keep rules relevant as data evolves.


Implementation / Operational Notes

  â€¢ Existing scripts / DAGs The scheduling, orchestration (DAGs), and scripts that execute rules and push results to downstream processes are already in place; implementation details and hands-on KT will be provided in follow-up sessions.

  â€¢ Environments & metadata Historically rules ran on Cornerstone; now migrated to Lumi warehouse (BigQuery). Metadata and result tables feed the UI and reporting indices (e.g., Elastic index referenced pulls from results table).

  â€¢ Open items discussed
    
    â€¢ Confirmation about which platforms (iOS/Android) or CLIDs were unclear in the meeting and need clarification outside this session.
    â€¢ Access/permission issues while running a specific query in production were raised (example: length = 6 rule). Follow-ups with Anish, Raj, or relevant owners (ANSI/IMB) were proposed.


ðŸ“‹Overview

  â€¢ Q-Track is a BigQuery-based automated data quality monitoring system that:
    â€¢ Stores rule definitions in a DQ repository table.
    â€¢ Executes SQL checks per rule on schedule and writes results to a DQ results table.
    â€¢ Sends automated emails to business users for failures.
  â€¢ Q-Track UI centralizes failed-rule triage by creating human tasks for failures and enabling assignment, comments, and ticket creation (ServiceNow) where needed.
  â€¢ Reporting module will visualize historical trends and rule effectiveness for leadership.
  â€¢ Self-serve onboarding UI is in discovery; intended to let business users create and modify rules through a UI and save them into the repository.
  â€¢ Controls initiative is planned to assess and auto-tune rule effectiveness over time.
  â€¢ Implementation scripts, DAGs, and automation exist; deeper KT and hands-on sessions are required to onboard engineers (Prithvi, Nirmal).


ðŸŽ¯Todo List

  â€¢ Sachin / Suman:
    
    â€¢ Provide the recording and the shared documents (intake Excel and any slides) to attendees, by 2026-01-13 morning.
    â€¢ Schedule a KT session focused on implementation (scripts/DAGs) for Prithvi and Nirmal â€” target: 2026-01-13 (date/time to be confirmed).

  â€¢ Prithvi:
    
    â€¢ Review the recording and provided docs before KT; set up local environment/access for hands-on â€” complete before the KT (by 2026-01-13 morning).
    â€¢ Prepare questions on DQ repository structure and result table schema for the KT.

  â€¢ Nirmal:
    
    â€¢ Review the recording and provided docs before KT; set up local environment/access for hands-on â€” complete before the KT (by 2026-01-13 morning).
    â€¢ Validate permissions/access required to run production queries and list any missing accesses.

  â€¢ Sachin:
    
    â€¢ Clarify ownership/contacts for unknown items (CLIDs, iOS vs Android responsibilities, Monicaâ€™s scope) and circulate contact list â€” due 2026-01-13.
    â€¢ Follow up on the production query permission issue (length = 6) with Raj / ANSI / IMB and update the reporter â€” follow-up by 2026-01-12 20:30 (as discussed).

  â€¢ Product / PO:
    
    â€¢ Define approval workflow for self-serve rule submissions (who approves, SLA) and provide to engineering â€” target: 2026-01-20.

  â€¢ Infra / DevOps:
    
    â€¢ Ensure Elastic index and other reporting indices are connected to the DQ results table and are refreshing correctly â€” validate and report status by 2026-01-15.

  â€¢ All attendees:
    
    â€¢ Watch the provided recording and prepare for the implementation KT session (expected hands-on); confirm availability for the KT â€” reply by 2026-01-13 morning.

If any attendee wants clarifications or corrections to this summary (missing items, attendees, or deadlines), please reply and I will update the notes and Todo list accordingly.


Transcription

00:00:01 - 00:04:38 Unknown Speaker:

Q track training by Sachin and Suman Reddy Gupta on January 12th. No CLC on this. Do you know the story of this? He is asking for a new feature. But what is this commitment? Now we will test out whether it is coming or not. Now we will test out whether it is coming or not. Am I audible? Can somebody confirm? Yes. Sharing my stream, let me know once it is up in critical contents, in recommendation, which need to be tested, which not to be tested. For iOS, we don't know. For Monica is for Android. I don't know that CLIDs. This is the reason. Or else Anish will be the winner. Anish will be the winner. I'll give the girl's number. I'll give the girl's number. Once you guys are ready, I'll share my screen.

00:04:44 - 00:06:00 Unknown Speaker:

Wait, I will get... The girl actually shared the document. Yeah, she shared that document. This meeting is being recorded. Okay, uh, so starting with this one, I'll start the Q track over here. Okay, uh, so we, we already talked about it, right? What is this? Uh, Q track is? This is, you know, data quality monitoring tool. Uh, so this would be basically for any kind of animal detection. Uh, with respect to the, you know, tables that we are using for this USCDO one, okay, uh, so we have built a dashboard. Uh, it should be having, you know, few rules configured for those particular tables, and you know, when we run those rules. It would identify whether the data that has been persisted to that table are suffice to those rules or not.

00:06:00 - 00:07:01 Unknown Speaker:

If yes, we are not doing anything. But in case that doesn't suffice to those stored rules, then it would generate an alerts and intimate the business team that the data that has been persisted, those tables are not with respect to that. You know, data quality check that we are abide to, you know, made over there, okay, what kind of data and all. I'll talk about it. Uh, so, just for an hypothetical example, right? If I give you a pre plain example, pretty plain example, right? Suppose this is one of the roles that we are running over this table, right? Suppose this table is having this RESI user ID, which is a primary key over that table. So this should be always unique, right? So, if there is a duplicate user ID which is being inserted to this table, then our rule, would, you know?

00:07:01 - 00:08:11 Unknown Speaker:

It would generate our alert and intimate the users that there is a duplicate user ID that has been inserted to this Razium Express table. Or just for another example, for this one, bonus amount variables would not be null. If there is a column called bonus amount, and if somebody has inserted a null value to that, so this would generate an alert, and it would intimate the user. So these are the different type of rules we are running over there in that. Quality check that the dashboard that we have designed, sorry, this, I mean, report, runs on a big query database, yes, so that is what database, exactly exactly those runs. And I'll try to show you one of the alerts if there is a look in the data, if this is one of the roles which ran right.

00:08:11 - 00:09:12 Unknown Speaker:

There were no failure alert, so this would not intimate the user. Let me check if there was a failure that has occurred today. So users in the sense who would be notified that there is an issue. It's U.S. Business user, business business user who owns this table. And what is upstream systems for this? Upstream system, there are different upstream systems. So this table might be getting data from different source system. So we are not bothered about from which source the data has been pulled up to this table. It is just running the rules over this one. We are not here to rectify the rules. Sorry, rectify the data or something. The data can come from different source. Okay, it not confined to a particular source, so the data can come to this table from different source.

00:09:12 - 00:10:14 Unknown Speaker:

Once the data has been ingested to this table. Now, our rule would run on those latest data based on the schedule date that that we have made for these rules. Okay, so this is one kind of rule that has failed today. Okay, it is a daily check that means this job is running on our daily basis, right? And it was, uh, required to check like, count of records were created, having a future date. Okay, so this has failed. Uh, due to that, you know, count was not matched with a future date. Let me give you a better example from this one, maybe. So, in case of us, we are not making anything, but if there is a failure, we would be implementing the user. So, this is one of the rule.

00:10:15 - 00:11:04 Unknown Speaker:

And again, there is a, then single table can have multiple rules. Okay. You can see the table name again same, but we are making a run on their different variables. Variables is nothing but their, you know, columns, details. Okay. So, this is one of the table. Where the check type is that for this variable, count of future years, right, should not exceed 5% of total dates. So expected value was 0.05, but we received this one. So this rule failed, so we intimated this user. Okay, this one for the doc type 05, count should lie within the expected rate. The expected rate was between 1 to 4,284, but we received a count of 8583. So that's the reason this rule failed. So now, These are the alerts that has raised to this business box.

00:11:04 - 00:11:59 Unknown Speaker:

And then they would be taking any actions over this table. So they are also added in the mail. Yeah, yeah, they are added in this mail, right? Okay, these are the persons, so we notify them through mail only, we will not get in touch with them. No, no, we, we are not doing anything, right. Okay, so, correct. Not going to modify any data because those are products and data. Notifying them. It will, it will happen through mail only, we will not notify through some charter. Yes, again, forwarding them. Okay, no, no, no, these are all automation system. Okay, even this mail has been triggered through our automation process. We haven't created or composed this email. Okay, okay, you can see this. This is a automated email. When this rule was running over the portal.

00:12:00 - 00:12:56 Unknown Speaker:

When it identify these rules fade it has automatically created this email and then sent it to the customer It's not that we are sending this email in you know manually is all our automation. Okay, and There are validations at table level and sorry column level exactly data rule level You can see these many these many alerts ran today, right? If I see this one, look these many alerts running today from different different time, from 4 30 a.m. Till 6 30, right? So these many rules ran, so we did not create it. These many emails, these all have been system generated emails, right? Okay, now, let me know if you have any other question, otherwise I'll go ahead and go to the next part. Yeah, I can continue. Later, if there is anything, I'll ask.

00:12:58 - 00:13:40 Unknown Speaker:

Okay, so, on table, as we were talking about the alerts, there can be a different kind of alerts like threshold check, right? Uh, so if I filter this one, so these are some rules which has been configured in the table. These are the rules basically. Okay, so different types of alert can be there, like threshold check, right? This is one type of check. Like. Percentage difference in count of the conversation. Months would vary, not vary, should not be, you know, more than 30 percent. Okay. And how it is being made? This is the filter criteria that we are applying through the query. So you have to apply this means this is a when you want to make a layman understand what this check is all about.

00:13:40 - 00:14:31 Unknown Speaker:

But when you are transferring this, transforming this rules, you have to write some query like this one, which would be applicable to your SQL query. That is the implementation part we can talk about later. Okay. So this is one kind of check that we talked. A threshold check. Similarly, there can be a valid value check like count of record where RF awarded date should have a future date. Missing value check like there should be a 100% field rate. 100% field rate on what? On that particular CM13. So that CM13 column on that table should not have any null value. If it is having a null value, it is not sufficing to 100% field rate so it would intimidate the customers. Okay? So it's a business rule given by the business team and we implemented using the queries.

00:14:31 - 00:15:41 Unknown Speaker:

Exactly. So you are going to get these descriptions from the user and user they would be giving you this kind of statement. Like for this model, ID average should not deviate more than 10 percent of the previous run date. Okay, okay, now you have to transfer these business into a, you know, a statement to a SQL query for that table particular table. Okay, okay. And we are storing these all details to our DQ repository table, which I was showing, uh, or in during the DS you call a bit later. No, no, no. DQ repositories Women DQ result is where you are storing the values. Okay, okay. These are stored in the DQ repository table that will go to the DQ repository table only. Like incidents when I, Uh, no, no, no, you're getting confused.

00:15:41 - 00:17:04 Unknown Speaker:

I'll let you know. These rules are being stored in the DQ repository table. Okay, because. Suppose I was showing you one of the failure alerts recently, right? Let me see if I can get that one. By mistake I closed that one. Give me a second. Yeah, just take an example. So this table is having these many rules, right? You can count it, maybe 10 or 12 rules are there, right? Yes, so all these rules are stored in your DQ repository table. Because when you are running those roles, you need these roles from some, some master repository, right? How do you identify that I have these many roles? This risk first account table, right? So these rules are getting stored in your DQ repository table, please make sure you understand it once you are running those rules.

00:17:04 - 00:18:08 Unknown Speaker:

Okay, this is the expected value that you have put it over the query, but you receive this actual value. So when it is a fail or pass, right, these results you are storing in the result table. Yeah, gotcha. So that implementation part, we will talk it later because that would confuse Prithvi and Nirmal. So let me go back to that agenda that we were talking. So there are different kinds of rules. It can be a threshold rule, valid value, missing value, stagnant check. So different kinds of checks that we are applying over this table. Any questions Prithvi and Nirmal and Suman if you have? Model ID Can I get more info? What is model ID? There? Model ID is again. Like, you know, they wants to run different rules. Check over one table, right?

00:18:08 - 00:19:05 Unknown Speaker:

So for each rule they used to create a model ID. Or you can consider these as a DQ ID, or the rule ID, you can consider a unique rule ID for that table. These would be input parameter. When we talk about model ID, this is nothing but unique DQ role ID. Okay. Okay. Okay. So, one question I have, Sachin. So, the business users are there, right? So, are they, they are actually giving us the requirements. So, what information they will have? So, that's, we will not be knowing. This is what I was, this is what I was. There will be an intake form. Okay, I intake form like this. This is an intake form Okay, this excel is nothing but an intake form Okay, so they will be sending us these details Over this intake form.

00:19:05 - 00:20:14 Unknown Speaker:

Okay, They might be filling these details, You might not get every details right like this filter criteria, and all right. They might be giving you description that check. Type the table type, table frequency, which variable they wants? Pure the details, right? And then we have to translate those to the SQL query. So as of now, what was happening? As we are not direct point of contact for the business? User Anil Dandrachana used to take this intake request from the business. They used to transfer these uh, you know, uh statement to uh this filter criteria, SQL criteria. And then they used to provide us, okay, and then we used to insert this to the DQ repository table. So this is the ready mail compost.

00:20:19 - 00:21:27 Unknown Speaker:

Once we insert those rules to the DQ repository, now we need to incorporate those rules to our alerts type on the rules that we are running. We need to modify those existing scripts, accommodate those rules to the table, and then that table would run those rules to give you that output which we are seeing, right? Has been, you know, filled up? Then we would be calculating those one and then that would go to that. If that case, only the email alert is getting notified to the end user for pass, it would not for pass. You can see if this is a past result, this is not going. This is just to us, the reason being we want to monitor these jobs, so we are getting notified that these rules have been passed.

00:21:27 - 00:22:37 Unknown Speaker:

But if if there is a failure, then that would be notified, and in that case, uh, the support team was take support, so that has been notified to that customer. Now, tell me if you have any questions on this one. Would you mind repeating this one? Onboarding? Let me let me make you guys another thing in a simple manner. Okay, suppose on a limit around. Suppose if I have a employee table? Okay, suppose table underscore employee. Okay now, suppose tomorrow somebody come to me and ask me. As a team, I need to create a rule where I need to identify, give me the result. Those employee where email ID is null right? This is the statement that they have given, right? What SQL query I have to write? Select Start from this table Underscore employee right.

00:22:37 - 00:23:54 Unknown Speaker:

Where? Suppose that column was email underscore ID? Like all equals to equals to, however, you want to transform it, right, equals to equals to null, right? So this would give me a result, right? If the result count is zero. Suppose if you do a count star of this one, right? In case this gives you a count star, this comes with a result of zero, right? There is no employee, so this would be marked as pass no alerts to with even one, right, even one employees. Then this would be marked as fail and then there would be a email alert to the business team. I hope this makes sense. Now. This one gets stored in your DQ repository table, and this result, these two results. Basically, these are getting saved into DQ Results table.

00:23:54 - 00:24:58 Unknown Speaker:

And based on the result, either it would be notified to the customer or it would not be notified. This is your implementation part which is running in your script and which is responsible for storing this result and these email alerts. Now let me know if you guys have any questions. So, uh, this is, uh, okay, this is good, uh, threshold calculation, you said there in the document. Okay, this example, I understood. Uh, can you explain this slide again? If you don't mind? Sorry, which one? This slide? Onboarding process? Okay, yeah, onboarding process. This requests are coming from the business user, right? Okay, so business users have sent this one. I need a result of those employee where the employee I employee email ID is null. Okay, right? So that comes as your intake form right now.

00:24:58 - 00:26:10 Unknown Speaker:

This has been, uh, you know, refined by the product team. Okay, they have refined it, they stated. Give me those results where email ID filter criteria equals to, not where this one this is your filter criteria, right? This is your filter criteria email ID equals to equals to null. Now they asked me, save this filter criteria with the rule ID DQ Rule ID suppose 10 and save into the DQ repository table, so they will provide us the ID number. Also. Isn't it? Yeah, I mean, cities are auto generated, okay, so this is the filter criteria, so it is not only one rule, right? There might be multiple rules for that table, so whenever that rules is running for this one. Now, they would say that this I need maybe a in a monthly report, right?

00:26:10 - 00:27:04 Unknown Speaker:

If they ask to run it on a monthly report. I am not running this, uh, rule on this table, this employee table. On a daily basis, I will be running on maybe first day of the month or last day of the month every month, right? So whenever my role ran, it would analyze all the data for that table, right? And if it find any email ID with null value, it would alert the business if there is even even one employee without email ID. So they would find out the count, right? Once they find out the account, they would fix the, you know, either data or whatever access that is required, right? The table owner would take the access on that. Uh, okay. And what is CS metadata? Sorry, CS metadata is the DQID, is it cornerstone metadata, where those tables persist?

00:27:04 - 00:28:09 Unknown Speaker:

But now we have migrated from Cornerstone to Lumi, so that would be Lumi Warehouse. Okay, earlier, these roles were running on cornerstone, that's why you are saying this cornerstone, okay, so our this is the job which executes these rules monthly, hourly, so is good enough to take the information from the metadata table and execute it right now. That scripting, everything is there already, correct, correct, okay. And those all you know, Dag. And all those implementations we can talk about in the next session because that would be lots of information for you guys right now. Okay, so this part is clear, yeah, for me. So this is one of the initiative that we were running. So that is your Q Track data monitoring tool. Okay, that's it.

00:28:09 - 00:29:04 Unknown Speaker:

Generate the alert, its job is over, okay, so this, uh, rules would be changing, is it continuously? So we won't be changing? Based on their requirement, they will modify the rule they will be possessed with this rule, right? Okay, suppose they want to twist the rule, they would come up with the request and all right. So those all things can happen. How frequently they create new rules. Not much as of now, but we are providing them a UI interface. Again, that's something I was discussing in the previous call, which is in pipeline right now. I'll talk about that later. Okay. Because that is in enriching stage right now. Okay. Okay. So, this is one of the initiatives. This is one of the use case that we are running. Now, these alerts have been generated. Okay.

00:29:04 - 00:30:12 Unknown Speaker:

Now, what next? This has been notified to the customer. What next? So our next next initiative is Q Track UI. Now, please don't get confused with this one. So first one was Q Track Anomaly Tool, data monitoring tool, whatever you want to go on. Okay, okay, so that has been done till this alert getting created. Okay, this is done till the alert is done. Okay, now next come is Q Track UI or the DQMI. You can consider data quality management. I am, I think, issue management, data quality, issue management. Okay, that is again a UI for these alerts. So what we are doing in this one, in this, in this project, this is another, this is first project. Or the use case, whatever the you would like to say. And this is our second initiative, what we are doing now.

00:30:12 - 00:31:14 Unknown Speaker:

You have these results, right? So there will be lots of, uh, you know, fast fail, pass fail, type of, uh, like I was showing, right? Uh. Those many alerts have been created for a day, right? So it would be very difficult for the end user to track. Going through these emails. Right? There would be lots of emails, correct? So it would be very have to go manually, one by one, right to take an action on this one, correct. Yes, right. So what we have done now? Understand this one very minutely, we are going to create a file out of these these reports, right? Suppose? Irrespective of pass or fail, irrespective of pass or fail, suppose now as. Suman was mentioning, right, that results are going into DQ results table at the end of the day when the rule ran is over, right?

00:31:14 - 00:32:16 Unknown Speaker:

So this DQ result might be having 100 entry today. Okay, so what we are going to do? We are pulling these all 100 rules out of that DQ result table on a daily basis. So whatever run today would be having, we would be pulling all those results tomorrow, not today. We are waiting for these results to get over. Okay, so once the day gets over, like post 12 a.m. Right, the next day starts. We would be pulling all those results and then pushing to a downstream process which is going to process all these 100 alerts. You can consider, okay, whatever it is there in the tick result. Now, out of those 100 alerts, suppose 10 are failed, okay, and 90 are passed. But this tool is going to process this all 100.

00:32:16 - 00:33:17 Unknown Speaker:

Uh, records what it would do out of those 100 alerts if 90 are passed. We are not taking any action on those 91. For these 10, we are creating a human task. Okay, for these 10, we are going to create a human task. Why? Uh, give me a second, I'll log in and then I'll share my screen. There would be password, uh, so I don't wanted to share the password, let me just log in and then I'll please share my screen. Till this point, you are clear, right? Suppose there would be 100 entry? It is going to process those all 100. Okay, out of those 100 records, if 10 are failed, it is going to create your those 100 means 10 as the human task. Okay, what is human task and all don't worry, I'll make you understand.

00:33:30 - 00:34:51 Unknown Speaker:

You don't need to do anything with respect to the sequence. Let me reset my screen again. I am just, you know, letting you know. The business logic right now. Implementation we will discuss in the next call. Is my screen visible? Visible? Now? No, it's loading, I think in the black screen, and I can see it is visible. I hope it is visible now. Yeah, now, now it will. Okay, so now you have this hundred uh insertion right out of these hundred ones. 90 volts might have been passed. Okay, we are not taking any action on this 91. But there might be 10 failed one, right? So for all those failed one, there would be a human task which is being created for this Qtrak UI dashboard. Okay, so what user can do?

00:34:51 - 00:35:59 Unknown Speaker:

Instead, of, you know, manually checking all those alerts? They can land to this page and then they can check what are the failed results for today's date. That is TQL RAND data. Got it? Do let me know, guys. Yeah, it's just a table data, right? Exactly, the result data. The result that we have stored, right? These are the results, right? So can you show... Yeah, sorry. All the data you are just displaying it in the front end. Yeah, only for the failed one, only the failed one would appear to this screen. Passed one are not going to get appeared on this screen. Because passed one means those abide to your rule processing that you have designed, right? So in that case, those would not be visible over this screen, but the failed one would be visible on this screen so that end user can take actions on this.

00:35:59 - 00:36:56 Unknown Speaker:

Can you show us? List of actions available on the drop down there is, so all those failed results would appear over here and then they can come here. On this action. They can click on View details. Now again, as we are sending this to the that particular team, right? I think this is us open, open, open. As I have opened through this user, right, not able to reserve any tasks, give me a second. Is there anything that has been opened recently? So basically, look, this one, right? So now, when we are sending this to the end user, right, we are sending it over the distribution list. That distribution list might be having 10 users working for that team, right? Suppose Sachin, Prithvi, Nirmal and Suman are working for that team.

00:36:56 - 00:37:46 Unknown Speaker:

And on those 10 alerts, we need to take action, right? So Sachin can't go and work alone on those 10 alerts. So when we are sending this one, I can reserve this. So Sachin would be working on this alert. Suman won't be working on this one. So we have distributed this one so that once I reserve it, this would be assigned to me. This task is going to get assigned to me. And Suman can work on the rest of the 9 alerts. And then I can take action on this based on it. Whether this alert looks true or if it looks false. If this alert is false, then I can ask to delete this rule and then it would go to the PO. They would take action to delete this rule from the DQ repository by submitting a request.

00:37:47 - 00:38:50 Unknown Speaker:

If they want to revise this rule, if they want to modify, they would comment it and then modify. But if the rule is true, if the rule is true, it really failed. With the business justification that they are looking for, then this is going to create a service now. Incident, okay, and somebody need to work on that service now. Incident and incident look like it would be difficult for you, but the incident would look like something like this. I don't have the actual sample, but I'll just, you know, walk you through gets created. Suppose this is one of the incident, okay, just just an example, right? So incident would be created, it would be assigned to their respective team, and then they would walk on this incident whatever they want to do. Okay?

00:38:50 - 00:39:58 Unknown Speaker:

So this is another tool that is built on that dashboard, clear, just to take action on those when that has been created. On Q Track, the next is third. One is for reporting that is monthly or quarterly reporting. Now who is doing what? What kind of you know actions we took, what kind of you know activity that we did on those alerts and all right. At the end of the day, maybe a monthly report or something that you would like to view in the leadership level. So on that case, we do have another initiative called this Q Track monthly reporting. Okay, so this is going to visualize the data that you are playing with. Okay, give me a second, let me increase the date because last one year.

00:39:58 - 00:41:00 Unknown Speaker:

So this is going to give you how many unique rules you ran for last one year, how many have been passed? What is the failed right, what is the overall past success ratio? This would create a graph, a trend analytics. Right now, data is not there. That's the reason you can see only this. This uh minute, uh column. Okay, and then these are all the failed rules that have been failed for the last one year. Okay, these are the particular rule, and these are the data on which clear this is. This is quite simple, right? This is just a recording. I don't think I need to explain this or let me know if you need any clarification on this. Okay, so these are the three initiatives which are these two are already running in the production.

00:41:00 - 00:42:07 Unknown Speaker:

Uh, this is. This would go by this month or maybe initial week of FAb, and now I'll come that Prithvi was having, that is the fourth initiative, taking a rule description from the user. So that is Q-Track self-serve, which is in discovery or the reasoning phase, which I'm taking care. This is exactly what you are looking for. So we are going to design this UI for the end user. Where this is not being built right now, the designing is in progress. So here we are going to ask the user to select their use case, provide their table name, provide their check type, whatever variable, filter, condition and all. And this should going to create a rule, right? Once this creates a rule, this would go for the user's approval, the productive approval.

00:42:08 - 00:43:01 Unknown Speaker:

Either they can accept it, either they can reject it. Okay. So this is going to be an onboarding page for the Q-DRAC where we are going to provide the leverage to the end user for onboarding their roles either on a table or with respect to the use case. Again, these are lots of discussion. I'll let you know later. And whatever existing roles they have, as Prithvi, you were mentioning, right? We are going to give them a... flexibility, even to modify those rules, they can come back over here, modify the rules and all that. Okay, so that is still, uh, in the discovery phase. So first of all, you guys can focus on these three things, and then later on I'll give you more details on this one. Any questions at this point?

00:43:01 - 00:43:55 Unknown Speaker:

So I'm getting it out here. So basically, there is a website that would be you used to verify whether the rule that that failed is a valid rule failure. You showed me one website, right? Q Track UI. So who are those users? They are not business users. Is it who is going to say whether it is a valid rule that failed the business users? No, no, no. This would be used by the business users if business. I told you, right? They won't be able to view these many alerts on a daily basis, right? So for that one, the failed one is going to get displayed over this UI. Okay, so they themselves assign few of the rules that got failed and they rectify themselves. Okay, they'll say whether this is fine, this is an expected rule, or this needs a correction, or not.

00:43:55 - 00:45:00 Unknown Speaker:

Exactly. Okay, and this final UI, okay, that is to create rule set. But, uh, when they submit it, what is the plan? Do we get it to a database table? Or, yeah, yeah, yeah, that would come from our database only. You need to save those details, otherwise how would you do that, right? So we are going to save that information in a table and we will implement it in the background. Correct initiative that we are doing. Okay, okay, I think, okay, fine, too early to talk. I maybe we can implement some little general kind of thing here to create query. Also in the back end, these are all automation, you haven't seen the implementation. That's the reason it's seems like, you know, uh, it's all like, uh, kind of manual stuff, but these all are no, no, this is good.

00:45:00 - 00:45:57 Unknown Speaker:

First, three things, obviously this that is required. Okay, last thing I am talking about, which is under implementation. When they submit a rule, a new rule, using the information they provide, we can even create a query using some agentic tool or something. I thought, Okay, yeah, that is, that is in, uh, implement, that is in discovery, right? We are already doing the discovery right now for that how we are going to take it up. Okay, okay, okay, good. That is not being implemented or that hasn't been started yet, right? That is still in pipeline and we started doing that. I'll stop the recording. Do let me know if you guys have any other question. Sachin, the elastic cells we have shown, that index is getting the data from the result table, just pursuing the- Yes.

00:46:23 - 00:47:31 Unknown Speaker:

I stopped the recording, I'll say the you know recording to you, go through it once. Tomorrow we can have a KT session. First of all, for the implementation of this one, the Q Track Data monitoring tool, and then this one. And then maybe similarly, we have insurance insurance again. I will give you the overview and then maybe they see, and then 80 part, we will start. And again this this includes controls, which is again a queue tag initiative. Okay, okay, so that comes with control, so that is also again. This is an implementation stage right now, so this is to analyze the current data again. Maybe I missed this, maybe in the next one, I'll explain it to you. What is this control is all about? Okay? So this control is going to verify the rules that we have configured.

00:47:31 - 00:48:27 Unknown Speaker:

How much efficient these rules on the current data, like your data gets fluctuated, right? Data always going to get changed, right? So how efficient these rules are? Otherwise, this control is going to modify those results by itself. That is the initiative that we are doing. Okay, okay, okay, once the recording is available, you guys and so when? I hope you should have now more clarity compared to Prithvi and Nirmal. In case you have any query, do let me know. I'll set up another call tomorrow and then maybe a ATO with respect to the implementation, and I'll expect Prithvi and Nirmal by the time you know you're going through these video. Please set up your system so that once you give a hands-on on one of the script, it would be easier for you to identify.

00:48:28 - 00:50:11 Unknown Speaker:

All these are running and all. Okay? Sure, sir. Thank you. Okay. Okay, then. Thank you, guys. Thank you, sir. Thanks so much. Thank you. Yes, go ahead. I have one now. Can I share my screen? Yeah, go ahead. Yeah, it's visible. Actually. This is the one query I have written for, the one this table USCC RBB insurance length equals to 6, so I have given already. It is like, a OK, you are calculating the length equals to string equals to 6, no length equals to 6. So I have given once. Like, expected value is 0, here, expected value is 0, and whether it will be fail, actual value is 0 or else it will be fail. Because where? And the command is here, the the query should be, uh, passes here, it will be running the total.

00:50:11 - 00:51:21 Unknown Speaker:

But what is? The doubt is here for the axis, or I don't know which one, where I am missing for the query I am missing, I don't know. It should be like a pass now. Okay, now it is fine, it's showing the result. Suppose if I take this one out, not equals to 6, it is equal to 6. Then now here it will be like a actual value will be shown, right? Again I will be running. So did you check the SBC option and only ID? What is the different length of that? First of all check that one. That you have put it right, this is yes. But while I am running in the production, like the P, just I'm checking. I'm not, uh, I'm not like inserting anything here, I'm just I'm selecting here from the select.

00:51:21 - 00:52:28 Unknown Speaker:

While it will be like, actual value. Is this one here? While doing in the Pro Department of Development, it's like a different something weird, I think. First of all condition, I am not understanding why it is the hardcore condition they are putting. What is the business rule that they are trying to achieve from this one? They asked me, length equals to six. Uh, check who asked you? Who asked you the business role? The description? No man, that is not correct. Length equals to six. Yes Well, So can you do it with equals to equals to six In the market as it goes to equal to six and try it out? So this is one of the input take with V, which is trying to implement, right? So they have given the requirement now.

00:52:28 - 00:53:24 Unknown Speaker:

He has to create a query out of it. Okay, okay, can you mark it as equals to equal to six? Uh, yes, and I have a call with Raj in next five minutes. Okay, uh, okay. Anyway, I will check with this one with the answer, check with Ayapa or NC if you want, check with our N. C. Ones. Because I need to go through all the details before giving you the conclusion, right? Yes, because I already connected with Franks and IMB. So, I returned because the quarry is running successfully. Whether it will be like a, in the production, like a, for my access, somewhere I am missing. Yes, with ANSI. If it's not able to help you out, let me know. I will connect with you at 8.30. Okay? Yes, Sachin.

00:53:25 - 00:53:50 Unknown Speaker:

I will take it to anyone, the ANSI or IMB. Yeah. should thank you for checking thank you any uh other doubt or you guys are good yeah for now thank you guys thank you2026-01-12 Validation and Workflow Updates

Creation Time: 2026/1/12


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-12 11:31 (Duration: 2004 seconds)
  â€¢ Location: [insert location]
  â€¢ Attendee: Sameer, Sachin, Regina (mentioned), Rachna, Hansi, Kondal, Sindhuja (absent), Ansi, Kundan, Prithvi, Nirmal, Nirman, Rajendra (tagged), Gaurav, Saurav, Bhagwan, Athar, Kanchan, Kondal, others (names referenced in discussion)


ðŸ“’Meeting Outline


Validation & ICMB / Quanto Workstream

  â€¢ ICMB completion & email Sameer confirmed ICMB data validation is complete and will send the email with results in a couple of minutes. Team asked Sameer to send that ICMB email immediately and then start focusing on the Quanto task.

  â€¢ Quanto work Sachin and Sameer to focus on Quanto next; Sameer to report blockers if any.

  â€¢ Validation counts & pending items
    
    â€¢ Sameer: Sent 15 scripts; recipient returned 16 (15 + 1). Waiting to know how many will be signed off.
    â€¢ Rewards integrity: 1 script pending.
    â€¢ DQID server: ~12 pending items.
    â€¢ Two scripts raised in Jira due to missing Maestro notifications; awaiting resolution.
    â€¢ Many validations from Sameerâ€™s side completed; pending responses from Rachna on approvals.

  â€¢ Follow-ups Team to follow up with Rachna on outstanding emails/results. Sachin emphasized prompt acknowledgement emails when an issue is seen.


Feed Key (Julian date) Issue â€” Data Mismatch

  â€¢ Problem description A feed key which had been treated as 7 digits now appears as 5 digits because the Julian date portion changed for 2026 (counting days after the year yields 5 digits). This causes substring-based logic (expecting 7 digits) to fail, producing mismatches in some scripts.

  â€¢ Investigation
    
    â€¢ The erroneous feed is coming from BC Non-premium table (as noted).
    â€¢ Removing the filter condition that relied on fixed-length substring returned correct results in queries.
    â€¢ Team validated corrected results and Sameer will share the queries/results with Regina asking if the filter condition can be removed.
    â€¢ No response yet from Regina on these clarifications (multiple emails sent â€” Friday and today).

  â€¢ Next steps discussed
    
    â€¢ Review and decide if filter condition should be removed or logic updated to handle variable-length feed key as Julian date length changes.
    â€¢ Discuss Julian date handling further on a follow-up call.


RTF Events / Sub-status & E2 Transition

  â€¢ Issue Not receiving the expected sub-status (RTF events / sub-status updates) in the workspace. The event triggered from RTF side is not appearing in the workspace or E2; logs/process not showing expected entries.

  â€¢ Expectations vs actions
    
    â€¢ Leadership expects the team to prioritize sub-status issues (primary focus), not pivot prematurely to RTF event investigation.
    â€¢ Confusion: team previously moved to E2; the focus should remain on ensuring sub-status flow and why sub-status is missing.
    â€¢ It was noted that a discussion with the external contact (Raaz? Urad? RTF owner) was performed but the team still needs to re-connect to address the sub-status specifically.

  â€¢ Responsibility & escalation
    
    â€¢ Team members should prioritize getting sub-status working to avoid blocking dependent user stories (notably Kundan and Kundanâ€™s user stories).
    â€¢ If contacts (e.g., Rajendra, Gaurav) do not respond, escalate appropriately.
    â€¢ Athar was asked to prioritize closing sub-status items today as he will be on leave starting tomorrow.


ServiceNow / Dashboard / Story Acceptance

  â€¢ Integration & dependents
    â€¢ Dashboards and ServiceNow integration are high priority; incomplete sub-status will block story acceptance.
    â€¢ Ensure stories and validations are ready for acceptance and coordinate with Anand (or relevant approvers) to get stories accepted next week.
    â€¢ Gaurav did not yet reply on a particular item; team to follow up.


Access, Training & Knowledge Transfer (KT)

  â€¢ Access issues
    
    â€¢ Some members lacked access to training materials and environments (BigQuery, Lumi, Q-Track recordings).
    â€¢ One member reported GCP account errors but later confirmed BigQuery access was granted after raising a ticket.

  â€¢ Recordings lost
    
    â€¢ Previous WebEx recordings/Q-Track training content appears deleted (retention policy suspected). Team acknowledged the recordings were removed.
    â€¢ Plan: re-run KT sessions this week, record them again, and update the setup document. Sachin to set up KT plan and sessions, record, and share updated materials.

  â€¢ Onsite attendance
    
    â€¢ Team expected in-office on Monday, Wednesday, Thursday unless urgent reasons otherwise. Coordinate with Kundan and others if on-site tasks require presence.

  â€¢ Communication channel guidance
    
    â€¢ Do not use the current meeting group for day-to-day issue reporting; use the specified Slack/Teams channel (the â€œstep channelâ€) for queries so others can respond promptly if primary contact is not available.


Miscellaneous Technical Notes & Actions

  â€¢ Feed key code The code currently expects a 7-digit feed key; produces 5-digit feed keys in 2026 due to Julian date formatâ€”code needs adjustment to support variable length or to normalize feed key format.

  â€¢ Data profiling Team reviewed feed key in the environment and confirmed mismatch between generated feed key and expected pattern.


ðŸ“‹Overview

  â€¢ ICMB validation completed; Sameer to send email with results now.
  â€¢ Several validation scripts still pending (rewards integrity, ~12 DQID items, two Jira-blocked scripts).
  â€¢ Feed key mismatch due to Julian date change (7-digit â†’ 5-digit); removing substring-dependent filter returned correct results â€” need decision to remove/update filter.
  â€¢ Waiting on Regina and Rachna responses for clarifications and approvals.
  â€¢ Sub-status not being received in workspace/E2; this is the priority to unblock dependent stories.
  â€¢ Team must acknowledge emails promptly when investigating issues (quick â€œIâ€™m looking into itâ€ replies).
  â€¢ WebEx/Q-Track recordings were deleted â€” KT sessions to be re-run and recorded; setup doc to be updated.
  â€¢ Access issues mostly resolved (BigQuery/GCP); remaining access requests in progress.
  â€¢ Onsite attendance: Monday, Wednesday, Thursday expected.
  â€¢ Use the designated support channel for operational queries; escalate to Rajendra/Gaurav if necessary.


ðŸŽ¯Todo List

  â€¢ Sameer:
    
    â€¢ Send ICMB validation email with results to Regina, now (ASAP).
    â€¢ Share query and validated results showing effect of removing the feed-key filter; request Reginaâ€™s approval to remove or change the filter (Follow up: 2 business days).
    â€¢ Continue work on Quanto after ICMB email; report blockers if any (ongoing).

  â€¢ Sachin:
    
    â€¢ Follow up with Rachna for approvals on validation scripts and report back (by EOD / next business day).
    â€¢ Ensure team members send prompt acknowledgement replies to product-team emails (ongoing).
    â€¢ Set up KT sessions for training materials that were deleted; schedule recordings and share updated set-up document (this week).

  â€¢ Athar:
    
    â€¢ Prioritize closing sub-status items and coordinate to ensure sub-status is fixed (today â€” before leave starts).
    â€¢ If no response from Rajendra/Gaurav, escalate and coordinate with Saurav/Bhagwan as fallback.

  â€¢ Validation Team (Sameer, Hansi, Kondal, F1):
    
    â€¢ Validate remaining scripts (rewards integrity, DQID pending items) and update status column in emails sent to product team (update ASAP).
    â€¢ For Jira-blocked scripts (missing Maestro notification), follow up with infra/notification owners and update Jira tickets (by next standup).

  â€¢ Dev/Platform Owners (Kundan, Nirmal, Prithvi):
    
    â€¢ Investigate and fix feed-key generation logic to handle variable-length Julian date (modify code or normalize feed key). Provide plan and ETA (by next 2 working days).
    â€¢ Confirm source table (BC Non-premium) feed format and whether upstream change required; if API/front-end parameter changes needed, list and schedule deployment steps.

  â€¢ KT / Training Leads (Sachin, Kundan, Nirman):
    
    â€¢ Recreate KT sessions and recordings for Lumi/Q-Track materials and share recordings & updated setup doc (this week).
    â€¢ Provide step-by-step setup document and ensure trainees have access; coordinate with support for any access issues (ASAP).

  â€¢ Messaging & Communication:
    
    â€¢ All team members: start acknowledging priority emails with a quick "Iâ€™m looking into this" response when issue is observed (immediate).
    â€¢ Move operational queries to the specified support channel (step channel) rather than group meeting thread (immediate).

  â€¢ Escalation:
    
    â€¢ If Rajendra/Gaurav do not respond to sub-status or workbasket sync issues, escalate to higher management and tag Rajendra (as discussed) â€” responsible: Sachin / Athar (by end of day if no response).

Please update the "Current Status" column in the email thread for each item (e.g., build in progress, received sign-off, pending approval) so product team sees real-time status.


Transcription

00:00:01 - 00:02:12 Unknown Speaker:

Thank you. Okay, everyone joined? So, Sameer, you can continue on the one that we discussed last week. Yes, Sachin. On the Quanto one and... I completed the ICMB with data validation also. I just want to send an email. So... Yeah, I will send it now in a couple of minutes. Okay, just send that ICMB one and then, you know, start focusing on the Quanto one. Let me know if you face any blocker or anything. Sure, Sachin. And, uh, okay, and see an app. I guess where we are with respect to the validation. How many scripts have been left now? Yeah, so for the ICME, I had sent 15 to her. 15 she has given son of, plus one other script, so 16 scripts she gave son of. I don't know how many she can sign off.

00:02:13 - 00:03:26 Unknown Speaker:

Then a few scripts actually, Sachin, we saw an issue on Friday. The feed, which was generating starting this year, was coming as five digits. So because of that, in some scripts, we were using a substring of this feed key, seven digits, so that was failing. With that filter condition, if we remove, we were getting the correct results. So, I have validated those results, but I will send those results and query for her review to Regina if we can remove that filter condition. And did we get a response from her or we are still waiting for a response? Uh, yes, such in your audible. I was asking, like, the email that you have sent for her for the, you know, clarification did? She replied over that email. Or we are still waiting for her response on that?

00:03:26 - 00:04:29 Unknown Speaker:

Uh, no, still waiting. Actually. Friday I had sent one email, then today two more results I have shared with her, so yeah, okay, then just follow up with her, okay. So, how many validations left with you right now, if I consider your bucket? So, rewards integrity, one script, we are having, I think, 12 DQID server that is pending. Then, we had raised a Jira ticket for two scripts. We were not getting this maestro notification. I've seen that. Yes. Yeah, close to our... It's left for the validation. Rest all done from your side, right? You're awaiting a response from Rachna? Yes. Okay. How about you, F1, on the ICM event? Yeah, I just got a mail from Rachna. I just need to check that and I will update you, Sachin, how many she has approved.

00:04:29 - 00:05:27 Unknown Speaker:

Sign up. Okay, guys, please, you know, be ready with your... details before this stand up right. Otherwise, I won't be able to explain it to the product team. And did anyone check this one? The one that we received on last Friday? Yes, Sachin. Me and Hansi and Kondal also, we just together actually worked on this part. Some characters are difference, actually. So instead of seven, right, we are getting only five only. Uh, since we are adding the Julian date, right? So it is counting the after the year, it will counting the number of days, right? So now the year 2026 has started, right? So that's why we are getting a total five digits. So when we are verifying that particular feed key, right, it is mismatching actually the results due to this feed key.

00:05:27 - 00:06:52 Unknown Speaker:

From which table it is coming up. BC Non-premium. Okay, give me a signal, I'll take a look on that. And, uh, format should not break, correct, it should not break. But not sir. Uh, have you tested that? Yeah, yeah, that, uh. Ansi was shared the two reports to you, uh, previous year, and uh, that means the old date and the new date one that report. No. What was the issue? We will continue, let me, you know, wrap up with the rest of the. We'll continue on that. Let's close that immediately. Because if we are responding, you know, uh, uh, if you remember the feedback that Uh C has given, uh right, we should be very prompt. While, uh, we are giving a reply, right? So, uh, like, if I have asked you, at least give her a reply like, I'm looking into this one.

00:06:52 - 00:07:44 Unknown Speaker:

That is something we are missing, right? If I have just, uh, reverted you, please check this on priority. Uh, as soon as you see the email, just revert on this email that I am looking into this issue. Sindhuja, you are there, right? Did Sindhuja joined? No, not joined, okay? So so at least whatever your objection is, at least revert it to the you know product team so that they would know at least you are working on it. So that that's right. Why we are doing a retro if we are not, uh, considering the point that, uh, being raised in the in the retro? Okay, so just just revert on to this one and and see on this one, if we are done on this one, right? Whatever status update is right, uh.

00:07:44 - 00:08:36 Unknown Speaker:

Create another column here called Current Status. If you need a update from my app, take the update and revert on to this email. That all the deployment has been done. On this one. You can mention that build is in progress for rest of them. Like you already get a sign up for MGM one, right? So you can mention that received sign up. And wherever we stand, just revert on to this email. Yeah, I'll, I'll leave it on you and see on this one, okay, uh, and then be there on the call. Uh, we will discuss that Julian date issue. Okay, um, what is the uh conversation that we had with? Uh, why? He is asking us to check from our end? Yes, so we checked. We are not getting the event in the head work.

00:08:36 - 00:09:23 Unknown Speaker:

So he showed us the flow, basically the data which is, so the event which is triggered. It's not coming in the like, basically in the workspace from the RTF side. So that's why he wanted to be sure. No, no, forget about even whatever T2, even in the E2. Your status is not, uh, reflecting, right? This is not regarding this is regarding the RTF events. That was there for a sub status, for the sub status. What is the come come back to me with the sub status? What is the you know, issue with the substance? You didn't have the discussion with him, he was checking with the RTF thing. Oh man, uh, then then we messed up. This meeting then, in that case, right?

00:09:23 - 00:10:19 Unknown Speaker:

Our agenda was to, uh, understand why we are not receiving the sub status, why we have went to this RT event issue and again, you would need a connect with him today for the sub status. And the thing is, we discussed about the, you know, why the logs are not getting, you know, why the process is not getting No, man. No, no, no. Look, look, I really don't understand. First of all, your focus would have been to check whether, why we are not receiving the sub-status, right? Again, you would think over sort of here and what Raaz would assume that you already had a discussion on Friday and again, why do you need a connect with him? Right, he might not be having a time to connect with you guys on a regular basis.

00:10:19 - 00:11:19 Unknown Speaker:

First of all, you should have prioritized the sub status. One man again, you know, we would end up this week just figuring it out why sub status is not coming. And now let's, you know, leave the part this even aside, and let's focus on the E2 why the sub status is not good exactly. I, I really don't know why did you guys went for this RT event? One man? We already decided that we have moved to E2, right? Yeah, correct, then why? Why? These? These things have been discussed during that time, and now then your stories are not being not moving. Now I'm lost couple of sprint if I go to this valley. Right. Although you are working on here and there and other initiatives, but mainly they would chase out you for those dashboard and that ServiceNow integration.

00:11:20 - 00:12:44 Unknown Speaker:

Correct. And how would you ask Anand to accept your stories in the next week? Did Gaurav reply on to this one? No, no. I will ask him. He didn't came here, so I'm just calling him. No, no, did you had a connect with you? No, I connected with Kanchan. Okay, so, uh, whatever, she suggested, right, just to, you know, change the name, see? He told that, you know, uh. We are not going to merge it right now. The reason behind Sage? Also not sure whether we have to, you know, change it or not. She just pinged over there. They are, you know, supporting the UH incident only okay, so they are not, you know. Take down too much into the code. So Urad is the correct guy to connect. What is the other issue?

00:12:44 - 00:14:21 Unknown Speaker:

Other is this platform is one, so this work basket, right? The data is not getting synced. I provided him all the detail, I connected him over the call. Also he told I will check and update you. There's no response. I think... Okay, let's see. I tagged Rajendra. If they're still not responding, we will escalate over there. If you're getting a connect with Gaurav, make sure you are using these two by today itself. Do you have anything else in your bucket? Okay. And your rally stories are accepted in this... This report validates in the incident, yes. Please try to reach out to them. You would be on leave from tomorrow. So please make sure whether you connect with Saurav or you connect with Bhagwan, please make sure you are closing that sub-status because that is going to block Kundan for his user stories.

00:14:30 - 00:15:28 Unknown Speaker:

They have given their availability last Friday. We did not, you know, follow up on the correct, uh issue that we should have. Yeah, I mentioned it like earlier, also like, we'll be checking on this. So there was, uh, yeah, a gap from our side, also, like, we didn't follow. Yeah, exactly, let me just check with. Yeah, just check with them, because they are not going to give you give their availability on every week or every day, right? So when you are meeting a person makes sure that you are, you know, keeping a note that these are the things that I need to check with that guy. Anyhow, today sort of is not available, they mentioned on Friday. Only then check with the work, one man, if sorrow is not there. Check with whoever is required, right?

00:15:35 - 00:16:47 Unknown Speaker:

And if any changes is required for the API or in the front-end input parameter that needs to be deployed. Otherwise, you would be a kind of block, right? He is only from tomorrow right till 16. 19th is the last step when the sprint is getting over we don't have any delivery exactly. To close this one, Athar, on priority today itself. Okay. Because I don't want to disturb you during your holidays and all. That is your priority, Athar. Just follow with those guys to close it. Sameer, you are already working on that one. And send a revert email to that email that I have asked you. I will come back with those details that you are asking. And now... uh, yeah, hi Sachin. So the long training I completed and uh, the other ones I do not have access.

00:16:47 - 00:17:39 Unknown Speaker:

I've raised the access to a support result to raise access for all of them. Whatever that. Uh, how you guys are connecting to the VPN, um, you must have set up the OCTA in your phone, right? Yeah, then how come your, uh, you know, Google account has not been created? That is this surprising? Actually, it created, but, uh, some error is there, actually, uh, even the ticket. Uh, no, no, can you say? Can you share your screen quickly? But now it is resolved, GCP account is created and okay, you want me to share? No, you are able to access, uh, the big query and all right. Ah, that thing, I didn't test, but uh, can you share your screen quickly? I didn't raise a request to get access right, so I raised it in the morning.

00:17:39 - 00:18:39 Unknown Speaker:

Today I have got the session I can access the big way. You can access the BigQuery, right? So then you have the access. Prithvi, do you check that? Yeah, for me, this WebEx is installed in the morning today. It's asking me to quit and reopen it for me to share my screen. Okay, okay, that's fine then. Just coordinate with Nirmal. You guys are there in office. Then if you are there in office, you can check with Kundan also. Yeah, yeah. I'm not in office today. Because, uh, there was some, uh, communication gap. Sorry, that's my bad, my dog, but uh, yeah, next, uh, uh. Please make sure what are the day you have planned? Essentially, as you mentioned, right? Those we have to maintain that. Uh, the same day, yeah, Monday, Monday, Wednesday, Thursday.

00:18:39 - 00:19:43 Unknown Speaker:

Okay, please make sure you guys are there in office on Monday, Wednesday and Thursday. Yeah, okay, until unless you have something urgent and I'll talk to you guys in the evening. Let's see, uh, where he goes. So Nirman, you're done with Lumi training, right? And did you start it on this one? Uh, those training. I don't have access, I guess. Uh, it's not showing you the videos for those, uh, even in Q track also. No, no, no, hold on a minute. Okay, you are waiting for this approval. Have you went through this one? For training? There is no approval is required. There is nothing restricted with respect to that. Go through these training videos, which I already tagged you over here, right? So it is not working. That is something known to me.

00:19:43 - 00:20:38 Unknown Speaker:

What about Q track? You type? Also showing me the same issue which is no man recording you are looking for doesn't exist or no longer available. Oh, it can let me see, give me a second, I'll just cross check. And one more thing I would like to discuss, like, don't utilize this group for any of your issues, right? Because here only I am available, right? I might not be able to give you a prompt response over here. So, utilize this step channel so that whenever you put a query, if someone else is there, they can just cross check and then help you with that. Anyhow, first of all, point of contact with Kundan, but even if, like, you know, if Kundan is busy, either Samir Ansi or whoever is free, they can take your request and help you guys out.

00:20:40 - 00:22:54 Unknown Speaker:

So this two tracks would get open. Man, this is surprising. Oh man, what happened to this one in that case? Samir, these were over my praise or someone else's praise? This is Q-Track, Sachin. These recordings, man. These recordings were over Rose WebEx. These were over my WebEx only, right? I think I have a no no no no kundan was right those recordings were being removed look right now i can see only the latest one yeah deleted recordings were deleted, man. These are the meetings. Surprising, man. How come they delete these? Okay, no worries. We will set up a KT plan for this week and then with me and uh uh, I'll set up those KT plan and then we'll record it again. I think those all recordings were being deleted. Okay, let's move to some archive.

00:23:12 - 00:26:07 Unknown Speaker:

They have deleted this one, man. Might be having a retention policy. Even the latest one, they might have removed it. Everything has been deleted, man. We will create these sessions again and we'll update you. Okay, till the time, what you can do other than that? So then where is that set up document for that? Uh, okay, just share that document, let them at least set up that, uh, that document needs to be, uh, modified. Yeah. Once they will, you know, uh, face anything, we'll update that one and, you know, integrate that one. Okay, okay. And now I think I'm done, what is that? Can you tell me? Shall I share my screen? Uh, oh, bad. Okay, okay, you carry on. Sorry, let me restart. Okay, you're getting it from Lumi T, right? Not this.

00:26:07 - 00:27:25 Unknown Speaker:

Uh, actual Lumi schema, right? Ah, yes, correct. Then we have to go over there, right? No problem. Can somebody stay on your screen? That's fine. I have to stay on your screen and, you know. Let me do it. Let me do it. Give me a second. No. Okay. Okay, not this one, right? This is not this one. This is a data profiling table, actually. So you have to go to below on that this one yes yes this is the environment actually i'll change it i'll change it if you want you can drop up kundan a pen and she can stay back. This is the feed key that we are receiving, right? This feed key is correct, Sachin. What we are generating, there is the problem. We are generating only 5-digit feed key.

00:27:25 - 00:33:10 Unknown Speaker:

Here you will see in our code, we are looking for 7-digit feed key. Brother, do we need a full description? What? Do we need a full description? We got the CLs from there. From there? Yes, we got 82 CLs. Why is it not enough? Is this enough? If we run it, it will come here. We need to modify it. It will come here. Once we run it, it will go to the next agent. Yes, the next agent is that one. And with the reasoning information.2026-01-14 Data Persistence and Backup Strategy Discussion

Creation Time: 2026/1/14


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-14 18:35 (Duration: 2631 seconds)
  â€¢ Location: Virtual meeting / Screen share
  â€¢ Attendee: Sachin, Anand, Upadhyay (Aryan Raj?), Nancy, Srishti Panda, (and other team members â€” names mentioned: Man, Sachin, Anand, Aryan Raj, Nancy, Srishti Panda)


ðŸ“’Meeting Outline


Data persistence & backup strategy for use-case tables

  â€¢ MX Quest history table / use case folder Discussed that the MX Quest History table (under the use-case folder, referred to as "use case one" / USCS) should not be modified. This table contains November history data; the DT (data transfer) flow must sweep data into a separate backup table rather than into this history table.
  â€¢ Action required: create backup table and target DT flow Clarified that instead of backing up then running an operation on the same table, the team must create another table (backup table) and run the DT flow pointing to that backup table. Confirmed the process: create table â†’ run DT flow targeting the new table.
  â€¢ Data sharing / import concerns & storage limits Discussed sharing data with customers by importing from the use-case folder if persisted there. Noted production storage limit of 10 MB; estimated that a 50M-record dataset would exceed 10 MB (needs size check). Team member will check actual size (MB) of the dataset.


Linkup / Managed Data Dependencies & Access

  â€¢ Linkup page / dependency view Reviewed the managed-data "link up" landing page which shows table dependencies and execution details. Demonstrated selecting a project/environment to view all table dependencies and clicking a dependency to view dependency and execution details (ingestion events, timestamps, notifications).
  â€¢ Access controls & who can create dependencies Discussed access: only some team members currently have access; Nancy can create dependencies herself. Need to confirm whether feature allows others to add resources by default. Decision: if not, provide access to one on-site member and an additional HTR member. Action to check with Aryan Raj / Upadhyay for access status.


DQ (Data Quality) / Master tag JSON-driven DAG triggering flow

  â€¢ Master tag JSON config & Maestro topic Reviewed the mechanism: a master tag reads JSON configuration and stores it in an Expo variable (Maestro topic). If no subscription found, tag stops; otherwise it triggers the corresponding DAG. If data anomaly alert occurs, flow stops. On success, results are stored in Google Cloud and an email is sent.
  â€¢ Retry logic & failure handling If the DAG fails, retries will occur; alerts are raised on failures. Discussed that basic flow was reviewed previously and no further questions at this time.


High-level design: Lumi â†’ Lumi Warehouse â†’ IS/ACE flow and SFTP handoff

  â€¢ BigQuery DQ results table, DAG, file creation High-level design: a DAG queries BigQuery DQ results table (persisted pass/fail), prepares a file and transfers via SFTP to MX/ACE consumer.
  â€¢ SFTP usage and schedule Confirmed SFTP is used instead of cloud S3. Producer job uploads file to SFTP (e.g., run at 10:00 for 13th Jan data), ACE automation checks SFTP on an hourly scheduler (e.g., 11:00 check) and processes available files.
  â€¢ SFTP folder semantics and file discovery Explained SFTP folders: inbox / outbox / send (or similar). ACE looks in both outbox and send to ensure it finds files moved between folders. Retry logic on download (up to three attempts); if all attempts fail, Slack/email alerts are sent to support team.
  â€¢ Empty file handling If file contains zero records, the system raises a notification to business users indicating no records for the date. If file contains records, processing proceeds.


ACE BPMN processing & human task flow

  â€¢ Delegator vs. main process File processing creates a delegator process which spawns child processes (one per record/task). Child tasks can be PASS or FAIL:
    â€¢ PASS: task marked complete automatically (no human action).
    â€¢ FAIL: human task gets created and exposed in the ACE UI for manual remediation.
  â€¢ Control Center / Process Monitoring Demonstrated ACE Control Center: select deployment tenancy / process, view delegator instances, view child process status (active/completed), view process variables and task details (table name, rule description, status code).
  â€¢ Resume & alerts If the whole process fails (system exception, ASTV downtime, null data, etc.), alerts are generated and team can resume jobs from Control Center once root cause is fixed.
  â€¢ Human task UI Shown how a human task displays variables (TQID, table name, rule date, rule description). Users can mark false positives, delete rule, or close tasks; closing a human task updates ASTV status and completes the flow.


ACE implementation details, repo & configuration

  â€¢ BPMN configuration files Reviewed repository locations: as.yml contains BPMN files; service manager config holds service bundle/class registration. Deployment config JSON includes jar version, deployment ID, and service dependencies (email, Java classes).
  â€¢ SFTP util code & configs Showed code that handles SFTP connection (host, username, secret key stored in vault), folder list (inbox/outbox/send), file name extraction, POJO creation, and retry logic. Mentioned exception classes, constants, and configuration (ECM/email/SFTP).
  â€¢ Clear context data & reuse Explained clearContextData bundle clears saved context data to avoid persisting unnecessary DB context. Advised that teams can import the repo as a Jar to reuse classes in future projects.
  â€¢ ACE framework classes Noted iBundle and other ACE-provided classes available to use; service bundles can include multiple Java classes. Emphasized need to register classes in service manager config and test with hands-on ACE training.


Operational items, incidents, and next steps

  â€¢ ServiceNow incident Srishti Panda asked to raise a ServiceNow incident for an environment/dependency issue (ASTV or platform availability). Team agreed to raise incident; expected SLAs uncertain (approx 24+ hours mentioned).
  â€¢ Next sessions & code walkthrough Plan to cover SFTP code and Lumi platform specifics in next call. One session planned to dive into the AS (ACE?) code part and SFTP mapping/config in detail. Anand scheduled a separate call at 7:30 to discuss further; meeting to be forwarded to relevant team members.


ðŸ“‹Overview

  â€¢ MX Quest History table must not be modified; create a new backup table and run DT flow targeting it.
  â€¢ Confirm dataset size vs. 10 MB production limit; 50M records likely exceed 10 MB â€” size check required.
  â€¢ Linkup/managed-data page shows table dependencies and ingestion details; verify and grant access where needed (check with Aryan Raj / Upadhyay).
  â€¢ Master-tag + JSON configuration triggers DAGs; flow supports subscription check, retries, anomaly handling, and email notifications.
  â€¢ SFTP chosen for file handoff from DAG to ACE consumer; ACE polls SFTP hourly and includes retry + alerting on failure.
  â€¢ SFTP folders (inbox/outbox/send) can move files between folders; ACE checks multiple folders to find files.
  â€¢ ACE BPMN creates delegator and child processes: PASS tasks auto-complete; FAIL tasks generate human tasks for manual remediation.
  â€¢ ACE Control Center used to monitor processes, resume jobs, and inspect process variables and task contents.
  â€¢ Repo/config review: as.yml, service manager config, deployment config; secrets stored in vault; JARs used for deployment.
  â€¢ Teams must get ACE hands-on training to work confidently with BPMN and service bundles.
  â€¢ Raise ServiceNow incident for platform/dependency problems; SLA may take ~24 hours.


ðŸŽ¯Todo List

  â€¢ Sachin:
    
    â€¢ Create the backup table for MX Quest History data and confirm table name and schema, by 2026-01-18.
    â€¢ Update DT flow to target the new backup table and run DT flow once created, by 2026-01-20.

  â€¢ Data size owner (Assign: Sachin / Anand):
    
    â€¢ Check actual size (MB) of the 50M-record dataset and confirm if it exceeds 10 MB production limit, by 2026-01-16.

  â€¢ Access / Linkup owner (Assign: Upadhyay / Aryan Raj):
    
    â€¢ Verify who can create/manage linkup dependencies; confirm current user access (Nancy, on-site member, HTR member) and provide required access, by 2026-01-17.

  â€¢ ACE / Dev owner (Assign: Anand):
    
    â€¢ Walk through SFTP mapping/config and SFTP-handling code in next technical session; provide code walkthrough and explain retry/exceptions, scheduled for next meeting (coordinate with team, suggested 7:30 session).
    â€¢ Share repo details and instructions for importing shared JAR and service bundle configuration to enable reuse, by next session.

  â€¢ Incident / Ops owner (Assign: whoever raised â€” Srishti / Anand):
    
    â€¢ Raise ServiceNow incident for the platform/dependency issue (ASTV / managed-data access) and share incident number with team; raise immediately and notify stakeholders. Target: raise now and report status within 24 hours.

  â€¢ Support / Monitoring owner (Assign: Support lead):
    
    â€¢ Ensure Slack/email alert subscription for SFTP download failures and DQ job failures is configured and the on-call rota knows how to resume jobs in ACE Control Center, by 2026-01-18.

  â€¢ Training owner (Assign: Team lead / Platform team):
    
    â€¢ Organize ACE hands-on training for team members to get comfortable with BPMN, Control Center, service bundles, and deployment steps, schedule within 2 weeks.

Notes / Clarifications needed:

  â€¢ Confirm exact names and schemas for the new backup table.
  â€¢ Confirm who will be the on-site member and HTR member to be given Linkup access.
  â€¢ Confirm SLA and expected turnaround for ServiceNow incident resolution.


Transcription

00:00:00 - 00:01:29 Unknown Speaker:

Call that triggering JSon job, right? While saving the record, you just need to change the table name. Okay, and how would you be? And would we be able to share the data? If that is being persisted in this use case folder, we can import it, right? Yes, uh, Sachin, I can see that data still persist in the our use case one, on use case one. But I don't want and to, you know, do any modification to this table. Man. It is the MX Quest. History is the name, right? Yeah, yeah, yes, yes, yeah, yeah. U.S. CS, right? So this is under our use case folder. Don't make any changes to this table data because this versus your, uh, November history data flow, but your DT flow should sweep the data into that backup table instead of this history table.

00:01:31 - 00:02:48 Unknown Speaker:

Okay, so here there are two activities. One, first of all, I have to backup this data and after that I have to run the... No, no, no. Not required? Not required. You need to create another table here. Another table here and I have to run the DT flow targeting to that table. That backup table and just confirm me if this is the table, right? If I am doing a query over here. Will I be able to import this data? Import this data to? Oh, because we need to share it to customer, right? So suppose this is the data save, so save is not possible, right? So it's under the production only 10 MB data we can store. Oh, okay, and what would be the size of these 50 million record?

00:02:48 - 00:06:00 Unknown Speaker:

Definitely more than, uh, let's move, let me check, but definitely more than 10 MB. But I need to check. How can we see? Check that? MB 10 MB 10 MB data. That's what we can do. About the history, like a history table prospector. We have run on November and December, it got failed. When I I try to do a group by with the AS update and run date, I get, I get only November. Okay, it's just now, am I right? Proceeding, the link up page where you know, I invited you yesterday. Also, the reason for, you know, showing the table dependency that we were creating. Can you share your screen once? You are following up with that Upadhyay, right? What was his name, man? Aryan Raj, right? Just check with him.

00:06:01 - 00:07:02 Unknown Speaker:

From our team, only I do have this access to this link up. Did you guys move that feature to add additional resources to this one or not? Nancy was created her own, I showed him, but she can able to create it. Then if that is the case, then let me know. We need to provide the access to one on-site member and then maybe one more member to HTRs. Yeah, so here I'm thinking probably whoever having the project access, right, respective environments, by default, they can able to create these dependencies. Okay, If it is not, let me know, I can open it. My and NCS said me that link yesterday, yeah, just might is open, just open this one. Okay, so here is the page we need to go for the managed data, right here, we need to click on the link up, so here we can see the view.

00:07:02 - 00:08:19 Unknown Speaker:

Our dependencies. Just like, you know, like, you know, these folks, Okay, yeah, so this is the landing page. Yeah, so if some of the projects having more than one project set, so we can do the switch and we can check the corresponding project. So once you select the project and which dependencies you are looking with the respective production environment, so select this one. And it will show the all the table dependencies here. So, example if we can click off any of the dependencies here, Uh, just click on here. This name, yeah, this is If's own name. We have shown this yesterday, just thought of, you know? If we have any additional details, then let's find them. Yeah, this is the dependency details and execution details, suppose you? If any data ingestion happened, right?

00:08:19 - 00:09:39 Unknown Speaker:

So it will show a dependency is made. So we will get a master notification message. So and so data got ingested and the timestamp details. Yeah. Any other things we need to cover Sachin? That's all. No, no, that's it man. This was, I was not having the access, right? So I just thought of letting them know. To this Maestro topic and saving it to that Expo variable, right? If there is no subscription found, then basically this tag gets stopped. Otherwise, whenever the corresponding tag from that master tag that read that JSON configuration, right? We we talked about having those all Master tag details, input details, and the tag that would run, right? If there is no match, it would stop it, otherwise it would go and know trigger the corresponding dag.

00:09:39 - 00:11:01 Unknown Speaker:

If there is a failure, retry would happen, otherwise it would, you know, go. For that business logic, if there is a data anomaly alert, no alert, it would stop. Otherwise, it would store the result in the, you know, Google cloud and then send an email to this one. So this is the basic, the basic flow that we we were discussing yesterday, okay, any questions from yesterday, or management tool? Okay. And this tool is basically having a data flow between Lumi to Lumi Warehouse. To IS TV okay, so the data is getting transformed from from the Lumi to IS okay. So if I go for a high level design diagram for this one, so basically what we were doing? Uh, we are going to do a big query to that DQ results table where those.

00:11:04 - 00:12:02 Unknown Speaker:

Data get persisted with failed result or past result. There is a DAG or okay, which would be corresponding, you know, responsible for running that query, getting the result and then creating a file. And then that calls SFTP protocol. So in MX, we are using SFTP for transferring a file from one system to another system. Okay. So once that, you know, query gets completed, we are creating one file and then we are sending it to the SFTP. So this piece of code, I'll cover in the next call. Okay. Maybe I'll proceed with the AS part today and then maybe in the next one, we will talk about this Lumi platform side. Basically, this is same as, you know, we were discussing yesterday, but still I'll let you know on the code part.

00:12:05 - 00:12:56 Unknown Speaker:

Calls the SFTP, we are storing it to the SFTP. Ignore this AWS S3. Earlier we thought of reserving it to the cloud, but now we have went through the SFTP. So once this SFTP received that files, this ACE platform becomes a consumer. So again, this is a schedule job, as I mentioned, right? Suppose it? It has been scheduled to run at. It would be picking up 13th Jan. Data. So this job might have been scheduled at 10 a.m, so it would run at 10 a.m. Place the file over the SFTP and then this automation job we have scheduled at a one hour interval. So that when it would go for that 11 a.m, right? It would be able to find that SFTP file and then process that file.

00:12:56 - 00:14:34 Unknown Speaker:

Okay, how the processing is done in the East Platform, I'll explain it, but but let me know if you have any query over this. Those are already being taken care by, you know, during this SFTP transfer. Those are secured file transfer. We are not applying because they don't. This is the ACE framework, how it works. Okay, BPMN we already, uh, talked about today morning, right? Uh, how the task flows so in is basically it is intact with the BPMN. Okay, so now there is a file in that SFTP. So when my ACE automation system would get triggered, what it would do? It would, first of all, download the that SFTP. Okay, so this is my very first bundle, which would be downloading the file from the SFTP. We have applied a RipTree functionality.

00:14:34 - 00:15:35 Unknown Speaker:

So if there is a, you know, breakage or some issues are there, then we can go ahead and do a retry for the second time and the third time, if it is not, you know, downloaded, even after third time, then we would be raising an alert so you would be receiving an alert on the Slack as well as over the email. So that, you know, uh, we as a support team again, would you know back? And, you know, try to re-trigger the process or analyze what is the issue, why the file is not getting downloaded. One thing you have to understand how SFT works, SFT is having, uh, just like email, right? It is having one is inbox, okay, file to the inbox, and as soon as the upload gets triggered, the files are available on this outbox folder.

00:15:35 - 00:16:18 Unknown Speaker:

Okay, so these are three different folder, okay? Now, the file resides in this outbox folder from where Ace is trying to download the file. So as soon as you start looking into this outbox file in your first attempt. If you are able to face the file well and good. If you are unable to face the file from the outbox, the file automatically moves to next time. If you are just searching for the Outbox folder, you won't be able to find the file over there because that is already been moved to the send folder. So we have already applied a logic so that it is not only searching for the file in the outbox as well as it is searching for the file in the send folder.

00:16:18 - 00:17:28 Unknown Speaker:

So wherever the file gets placed, it would be picking up that file and then, you know, uh, process that file. Any questions on this asset, TP, and then how this SFT mapping and all is being done? That is again a set of, you know, configuration that I need to explain, maybe we can explain it into the next session. Okay, any questions on this? No, I'll take it as a no. Okay, right? That file might be having 100 records or 200 records, right? But in case there is no record, that file is empty. In that case, it is intimidating us, as well as the business user, that there is no records to be processed for today's date. Okay, but in case that file is having 100 records, in that case, it would move to the next step.

00:17:28 - 00:19:29 Unknown Speaker:

And for those hundred cases, it would create those. Suppose that file contains 10 records, right? Out of those 10 records, So here we used to call this as a delegator and this as a main process. Okay. So now out of these 10 tasks, the task one might be having a power status, right? This task two might be having a failed status. This again might be having a failed status. Pass status. We are not going to do anything. I have to show it over this BPMN, give me a second, so this USCDO is our tenancy over which we have deployed these BPMn. Okay, so this is the delegator I was talking about, right? It is calling this file for the SFTP, no email. Otherwise triggering the we are right now here, right?

00:19:29 - 00:21:23 Unknown Speaker:

Triggering the human task, so if for those 10 tasks, it would be creating a calling, this human task. Okay, so now, if you come back with the power status, we are just marking that task as completed, there is no activity being done over there. Okay, if it is fail, then in that case, we are creating a human task, using that human task, the UI gets represented where they can take any. Right. These are just the BPM and diagram. Right. So in order to monitor this one, we have this is control center. The UI representation from where you can monitor these jobs. Comes over here. We have to select this deployment ID. Okay, so six, one, two, three, four. So this is our tenancy. Okay, now you have to come to this process. Okay?

00:21:23 - 00:22:12 Unknown Speaker:

So this ACE utility provide you these feature mod process, case management, work management, different kind of rule assist. You can schedule your job through this scheduler, right? There is a security provision, you can create a robot task, you can save your secrets, and all over the secret. Management. There are forms manager to submit a form, bulk processing for, you know, doing a bulk activity on all those things. So once you go through that was training, you would be able to know these all different icons. With those details, I'll come up with this process management. So as we are into this tendency. This would show you my BPMN which is being deployed to this one, right? Let me select this delegator, right? Suppose I will try to select from 1st December till 15th.

00:22:12 - 00:23:07 Unknown Speaker:

So these are different different tasks, so this is delegated task which has been triggered. Let me check the latest one. These all are on 1 2 26, No. 1 12. This is the latest one that has been triggered. I think on 12th of Jan. So if I can click on View details, right? So this shows me. This task delegator, What is the status of this task? Right? Uh, what are the steps? Uh, all have been there. And then the process variable with this correlation key or whatever. So this would be having a child child key, right? Okay? So to this dependency, there would be this child classes, this task creation 2.0, suppose if I get, so, if that is having multiple records, right? So those many child process would be created.

00:23:07 - 00:24:02 Unknown Speaker:

Okay, so you can see some are active, some are completed, right? So few of them are marked as completed. Why? Because for this one, there would be no human task this would have. This record might have came with a status as pass. Okay, look, the status code has Park pass, right? So for the status code As pass. We are not making any activity, we are just marking this task has completed for the one which is failed, right? So that remains in the active state. Okay, and for that human task gets created. Okay, so if you go to this process variable, you would be seeing this status code as fail, okay? So for this one, this human task gets created, and now if you click on View details, these details would be having this task data which would populate this,

00:24:03 - 00:25:19 Unknown Speaker:

this table name, these variables, the rule description and whatever additional details that we were passing through this human task. Let me log into that portal. Give me a second. I'll reset post login. Give me a second. Till this point, anyone having any query such an, uh? If the process got failed, a human task would be created. Let's say there is a system failure, and in that case, uh, the entire thing got failed. Uh. So in the in those cases, do we get any alerts, like, yes, yes, for any? Any case that is getting failed, we will be getting else alerts and we have to check why it has failed. Okay, again. And then I'll let you know what type of issues usually come, so why those tasks fails.

00:25:33 - 00:27:29 Unknown Speaker:

There might be some system exception, like there might be some null data, right? And if you haven't handled that in your code, right, in that case that would fail, right? We are not calling any external API or something, right, so in that case you would not be getting any exception. Basically, don't know why there is no data, give me a second. I need to log into a different system basically. Been reflected and ASTV is down due to certain reason. Then again, in that case, your record would fail, but you can resume it. There is an option to resume the jobs also. Okay, now, let me reshare my screen. So suppose this is one of the work, right? We were displaying like this TQID, this table name and all, right? That gets visible over this one, this human task, right?

00:27:29 - 00:28:49 Unknown Speaker:

The table name, variable name, description, rule date and all, right? So these data gets displayed over here. And then once you take an action on any of this activity, right? And that marks this task as completed. Okay. Let me see if I can search with this task ID, then I'll demo it right away. Then these task IDs are unique for each run. Yes. These are all unique ID. Okay. I was just trying to view if I can close one of the tasks and you know, this has logged out me. So basically, you know, that flows like that and then once you mark that task as completed over here, that would get reflected over the ASTV and then that would be marked as completed. So this is the flow of the DQIM.

00:28:51 - 00:30:52 Unknown Speaker:

You can select false positive any delete rule and then once you submit it it listens okay and then that's closed. Okay, so if I go to that delegator, So this is how you look like, and the configuration would be USCDO. So let me show you where we configure the PPMN, so this is our deployment tenancy, or the repo where we are going to configure our BPMNs. If you go to this as.YML, okay, this is the file where you would be seeing your BPMN. This is the BPMN file list, Configure your BPMN over there. Okay, and then this RT event has been registered because we are using this human task. Okay, and some default configuration these have been set up by the platform team. Excuse me, okay?

00:30:52 - 00:32:39 Unknown Speaker:

So this is the service name using which we configure the vault and all configuration and over there, so this is a one of the configuration file. Now this BPMN comes up with the class name. Okay, I show you the delegator, so this comes with this class name, right? So this is the service name which gets configured over this service manager config. Json. Okay, so if you see this one, search it. So this is having this which gets called when the BPMN gets called over here. And this is a code that has been written Util. I have to take the update. So this is the SFTP host, right where we are creating a SFTP connection to connect over this one. Okay, so these are just, you know, line by line. Do you need our explanation?

00:32:39 - 00:34:35 Unknown Speaker:

Or I'll leave it, just let me know. SFTP connection. This is the folder list that we are fetching, right? The inbox, outbox and folder that I was talking about, right? So from there, it would be fetching that input file name, right? Once it fetched that input file name, it would read that file, iterate through that file and, you know, pull the data. Once it gets a establishment and then we would be creating a POJO out of it. So that extract the input from the SFTP. You can go through which has been there. Creating a template right and then it there is a retry functionality that we have added i talked about right so if the retry count is three then it in that case we have. These are the exception classes.

00:34:35 - 00:35:28 Unknown Speaker:

These are constant and then some config that ECM config if you are doing or email config or the SFTP config that we are doing. So for establishing the SFTP, we are passing the hostname, username and the secret key is nothing but the password for that. Okay, and I'll leave one bundle at the end that is this clear context data, right? So whatever details you're saving in the data, we are clearing. Because when you are selecting the context data, that means you are saving the data to the database using that context data. So clear context data, if you go to this one, this is the class, right? So here, if you see this clear context data, here we are clearing the context data, unnecessary context data over here. Okay. Any questions on this?

00:35:29 - 00:38:11 Unknown Speaker:

So, And I want to reuse. You just need to import that repo as a jar and you can include into your next project to reuse this class. Okay. Okay. And some of the class you might have seen this iBundle class, right? This is being provided by the ACE framework, right? If you go to this one apart from iBundle class, there are lots of other classes and methods are there, right? So those have been... balance here to you. Created over this is a work manager, right? So these are some predefined classes and method that they are providing which we can live this when we are making it. These are, you can call these all as a service bundle, right? For all those service bundle, you have a configuration over this Service Manager Config.

00:38:11 - 00:39:15 Unknown Speaker:

Under that Service Manager Config, you can define the Java classes which you are going to build. Because for few of them, like, you know, and it's not necessary that, you know, there would be only one single Java class intact. To this service, you can have multiple classes over the same service bundle. Okay, so this is the configuration that you need to do with BPMN with this Service Manager Config, and then you need to add that BPMN into this h.YMN. And if you want to place some, you know, properties, right, you can place remote host or whatever it is. Password is being saved in the world, that's why you won't be able to see the password. So these are some of the configuration that you can configure over here, okay, and then in this is deployment configuration.

00:39:15 - 00:40:26 Unknown Speaker:

This is our JAR file, that is the report, right for that, we have released that third version, so we have, you got my point, right? This is the place which I was showing you over this intelligent right. For this, we use create a jar, and that jar would be configured on this dependency. This is the EDS version, this is our deployment ID, and here in this dependency, this is this dependency is for email. And and this is for the, you know, Java classes that we have written to run it. So this would be there in this is deployment config. Json, so you need to consider this one, deployment config, and then this, this one, and then the PP. Modify something or go through the code. Then only we will have more confidence anyhow.

00:40:26 - 00:42:53 Unknown Speaker:

You have to have some hands-on, right? Without having a hands-on, it would not be, you know, straightforward that you'd be able to understand. So you need to go through that ace training and then post that you would be able to understand how it have some hands on on this ace part or not. Like this pipeline, like I see, Anand has set up a call at 7.30, let me forward it to you, let's connect at 7.30 to have a discussion with. And we need to raise this instant, right? Like, I don't know like how much time they will take once we raise that incident. Also, because that is a dependency, one like that is not in our hand. So okay, Anand has actually shared this. Yes, yeah, waiting. I have such. I just forwarded it to you.

00:43:00 - 00:43:25 Unknown Speaker:

So Srishti Panda is actually asking us to raise a ServiceNow incident, right? I'll do that. I'll do that. I'll do that. Yeah, like what I'm trying to say is like even once we raise that incident, it will take a couple of days, I guess. Yeah. 24 hours a year2026-01-16 Composer Environment and Airflow UI Overview

Creation Time: 2026/1/16


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-16 09:31 (duration 338 seconds)
  â€¢ Location: Google Cloud Console / Virtual session (screen share)
  â€¢ Attendee: [Not explicitly listed â€” presenter and participants]


ðŸ“’Meeting Outline


Composer environment overview

  â€¢ Composer environment discovery and metadata The presenter showed how to find Composer environments in the Google Cloud Console by searching for "Composer" and selecting the project. The Composer environment list displays metadata such as creation time, logs path, and DAGs path. Clicking an environment name opens the environment details page.

  â€¢ Monitoring dashboard The environment page contains an Airflow component monitoring dashboard showing status and metrics for scheduler, workers, webserver, database, and other components. Visible metrics include DAG statistics (successful runs, running tasks, DAG size), DAG parsing time, and CPU/memory/disk usage for components.

  â€¢ Logs and configuration The Logs section contains component logs (scheduler, worker, DAG processor manager, database operations). Platform/engineering configuration items (environment settings, environment variables, labels) are present but not required for general users; these are primarily for platform engineering.


DAGs storage and deployment

  â€¢ DAGs folder (GCS path) The presenter explained the DAGs folder (GCS path) where .py DAG files must be placed. Files in this location are parsed by the Airflow scheduler to create DAGs. Example: multiple .py files present in the demonstrated bucket.

  â€¢ Upload / CI/CD process Recommended practice: upload DAGs via CI/CD into the Composer GCS bucket and the DAGs path. To update or override DAGs, follow the same CI/CD or upload process to ensure changes are picked up by the scheduler.


Airflow UI walkthrough

  â€¢ Accessing Airflow UI From the Composer environment page there is a link "Open Airflow UI" which opens the Airflow UI for that Composer environment. The Airflow UI landing page shows the Composer name and a listing of DAGs with details.

  â€¢ Airflow DAG list details The DAG list displays information per DAG: owner (example: "airflow"), run count, last run time, active/paused status. DAGs can be triggered manually from this list.

  â€¢ DAG views and navigation Clicking a DAG opens its page in the default Grid View. The presenter demonstrated switching to Graph View to visualize task nodes and dependencies. In the demo, one DAG had no task dependencies defined so the Graph View showed unconnected nodes. Presenter noted that some actions or uploads might be restricted to admins (access limitations for non-admin users).


ðŸ“‹Overview

  â€¢ Environment page shows creation time, logs path, DAGs path and other metadata.
  â€¢ Monitoring dashboard provides component-level metrics (scheduler, workers, webserver, DB) and DAG parsing/usage statistics.
  â€¢ Logs section includes all component logs relevant to Airflow operations.
  â€¢ Platform/configuration items exist but are primarily for platform engineering and not necessary for basic users.
  â€¢ DAGs must be stored in the Composer GCS DAGs folder (.py files); scheduler parses these to create DAGs.
  â€¢ CI/CD to GCS is the recommended method to deploy and update DAGs.
  â€¢ Airflow UI is accessible via "Open Airflow UI"; it lists DAGs with status and allows manual triggers.
  â€¢ DAG pages support Grid and Graph views; Graph View shows task dependencies (not present in the demo DAG).
  â€¢ Some UI actions or uploads may be restricted to admin roles.


ðŸŽ¯Todo List

  â€¢ Presenter / Platform team:
    
    â€¢ Provide list of attendees (names and roles), due: ASAP
    â€¢ Share the Composer environment GCS DAGs path and access instructions (including CI/CD example or template), due: 2026-01-19
    â€¢ Share any role-based access controls and which actions require admin privileges, due: 2026-01-19

  â€¢ Dev/Users:
    
    â€¢ Prepare DAG .py files and CI/CD artifact to deploy to the shared Composer GCS DAGs path, owner: Dev team, time: next sprint (proposed within 2 weeks)
    â€¢ Validate deployed DAGs in Airflow UI (verify Grid and Graph views and parsing times), owner: Dev team, time: within 48 hours after deployment

  â€¢ Platform engineering:
    
    â€¢ Provide documentation or runbook for reading component logs (scheduler, worker, DAG processor manager, DB), owner: Platform Eng, time: 2026-01-23
    â€¢ Document monitoring metrics (where to find DAG parsing time, CPU/memory/disk per component) and alerting thresholds, owner: Platform Eng, time: 2026-01-23

If any of the above owners or deadlines need adjustment, please reply with updates.


Transcription

00:00:02 - 00:01:05 Unknown Speaker:

Hello, everyone. Welcome to another session of Google Cloud Composer and Airflow Introduction. In this session, we'll talk about the best practices of Composer and Airflow, and we'll go through the demo of Airflow UI and the Composer UI as well. With that, let me share my screen. So, when you log in to Google Cloud Console, choose your project if you search for Composer and click on it. If you have a composer environment created in this particular project, you will see this kind of a where your composer environment. Instruction. Now, the creation time, the logs path, the tax path, all those details will be visible. So from here, if you go and click on the name of the computer environment, you will land in this particular page.

00:01:05 - 00:02:04 Unknown Speaker:

Where you will see the monitoring dashboard of the airflow components like scheduler, workers, web servers, now the database and all those things. And then you can see your DAG statistics, right? How many successful diagram completed diagrams, then how many tasks were running at a certain point of time, what is the DAG-DAG size, how much time it is taking to parse all of your DAGs in the environment, all those details you will see. And you can see the CPU, the resource usage, CPU and memory and disk usage of each of the components. Then if you go to the log section, you can find all the components, logs, scheduler worker, DAG process manager, database operations. All the underneath component that Airflow is using to run your tags, those logs will be available here.

00:02:06 - 00:03:01 Unknown Speaker:

Then there are certain tags which are for the platform engineering team. As a user, you don't have to understand these things. These are nothing but the configuration of your Composer and Airflow environment, and the settings also available in this, and the environment variables, certain labels. Those are available. Those are not important to us now before we move on to the Airflow UI. So this is the DAX folder, as I mentioned earlier, the X folder are nothing but the path. Where we need to keep our Dot PY file or the tag files, which will be used by Airflow Scheduler to create your tags. So if you see here, I have kept a lot of Dot PY files and those PUI files has been used by Airflow Scheduler to create the tags. Right?

00:03:01 - 00:03:53 Unknown Speaker:

So if you want to upload the tag, you can upload by the CICD process to this particular GCS bucket and the Path. And if you want to override or update anything, the same process you need to follow. Now, let's go on, move on to the Airflow UI. So when you, when you are present in this GCP console, there is a link called Open Airflow UI. If you click on that, it will take you to the Airflow UI of that particular composer environment. And this is the landing page, where you will see the composer name, and then all the DAGs, how many DAGs are present, how many are active, paused, and then all the DAGs. These are the main landing page representation of your DAGs. So for example, this is one of my DAG.

00:03:55 - 00:05:36 Unknown Speaker:

Then the user owner is Airflow, how many times it has run, when was the last run, and all those details will be available here. And if you want to manually figure that DAG, you can do it from here also. And if you want to go inside and check each of the DAG, Then, for example, I'll go inside this particular tag. If you click on the tag name, it will open the by default a grid view. Right? This is the grid view, and here, if you want to change the view of your tag, you can click on the Graph View. Also, okay, and it will open this kind of node. The tasks here, I have not created the dependency between tasks, that's why it is showing like this. Let's open a proper one, or let us upload a date file and see this access users will not have, the admins will have, so I.2026-01-16 Lumi Platform Overview and Policy Training

Creation Time: 2026/1/16


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-16 08:44 (Duration: 1408 seconds)
  â€¢ Location: Video presentation / Lumi overview & policy training
  â€¢ Attendee: All Amex colleagues (audience of Lumi overview, onboarding and policy videos)


ðŸ“’Meeting Outline


Lumi platform overview & benefits

  â€¢ Lumi introduction and purpose
    Lumi is American Expressâ€™s next-generation big data ecosystem hosted on Google Public Cloud and developed/managed by Amex. Named after the Albanian word for river, Lumi symbolizes change and renewal and represents the companyâ€™s move to a modern, cloud-native data platform.

  â€¢ Business and user benefits
    Lumi streamlines data management, simplifies predictive analytics, provides real-time data access, and ensures data security. For individual users: smoother transition from legacy systems, accelerated outcomes, efficient data analysis, personalized user experience, simplified onboarding, and comprehensive training. For Amex: improved scalability, governance, operational efficiency, and faster implementation of capabilities.

  â€¢ Technical advantages
    Leverages Google native services (e.g., BigQuery) to accelerate offerings. Supports scalability without compromising performance, reduces latency, and unifies data into a single space for consistency and comprehensive views. Emphasis on simplicity via automated self-serve services for data ingestion and unified tooling.

  â€¢ Core architecture mantra: Capture â†’ Cure â†’ Consume
    Lumiâ€™s architecture is summarized as Capture (ingest), Cure (transform/clean), and Consume (analytics/consumption). This simplifies implementation of AI/ML and predictive analytics for targeted customer acquisition and faster insights.


Roles, personas, and responsibilities

  â€¢ Defined personas and their contributions
    Multiple personas introduced: project space developers, business users, data ingesters, product operations, Lumi governors, analytical users, and platform support. Attendees were asked to identify which persona(s) apply to them; personas can be multiple per person.

  â€¢ Role summaries
    
    â€¢ Project space developer: builds apps, dashboards, ML models; answers complex business questions via data and analytics.
    â€¢ Business user: interprets dashboards and model outputs into strategic decisions.
    â€¢ Data ingester: creates and maintains high-quality data pipelines; ensures data accuracy, reliability, and timeliness.
    â€¢ Product operations: provisions infrastructure, enables ingestion, and keeps Lumi functioning efficiently.
    â€¢ Lumi governor: enforces compliance, data integrity, and governance for pipelines and use cases.
    â€¢ Analytical users: generate insights and convey data stories to stakeholders.
    â€¢ Platform support: provides operational and technical support behind the scenes.

  â€¢ Preparation and learning
    Lumi Academy materials, knowledge assessments, demos, and success stories will support onboarding. Attendees encouraged to engage actively, keep up with training, explore the platform, and ask questions.


Lumi functions and capabilities

  â€¢ Data movement
    Unified DAB movement framework to ingest data from multiple internal and external sources, manage metadata, and migrate historical data.

  â€¢ Data transformation
    Automation of data transformations through products like HyperDrive and SchemaFactory, enabling self-serve ingestion and pipeline creation (DAB pipelines).

  â€¢ Analytics
    Large-scale analytics using BigQuery and other tools; supports dashboards, complex queries, and ML model execution.

  â€¢ AI / ML enablement
    Simplified access to AI/ML capabilities for pattern detection and predictive models, lowering barriers to advanced analytics.

  â€¢ Portal and user management
    Personalized Lumi homepage, portal functions to manage users and project spaces, and support for production and analytical workspaces.


Lumi usage policy overview

  â€¢ Purpose and scope
    The Lumi usage policy establishes frameworks, processes, and operating principles for onboarding and using Lumi for valid business purposes. It delineates roles/responsibilities (ingestion, security, consumption, management), guidelines for project spaces (analytical and production), user activity monitoring, and change management. The policy is a stable framework that guides the introduction of new functionalities.

  â€¢ Why the policy matters
    Ensures data security, consistent practices, and compliance with Amex management policies and technology standards. Clarifies expectations for users and stakeholders and helps protect data assets while enabling business objectives.

  â€¢ Policy coverage areas
    Data ingestion, data security, project space setup, data lifecycle management (creation through deletion), BI tool use, data sharing and transfers, data quality, and change management procedures.

  â€¢ Roles under the policy
    
    â€¢ Application owner: accountable for source applications, data flow to Lumi, and understanding business and technical contexts (per EAM-30).
    â€¢ Data ingestion owner: ensures feeds/tables comply with usage policy and IDL guidelines, manages ingestion and changes.
    â€¢ EAM 7E Activated Data Steward: manages critical data elements, defines meaning, lineage, and quality of CDEs.
      Each role must follow Lumi usage policy and AXP AIM policies/tech standards.


ðŸ“‹Overview

  â€¢ Lumi is Amexâ€™s cloud-hosted next-generation data ecosystem designed for modularity, scalability, and user autonomy.
  â€¢ The platform provides Capture â†’ Cure â†’ Consume flow with automated, self-serve ingestion and transformation.
  â€¢ Lumi leverages Google Cloud services (e.g., BigQuery) to enable large-scale analytics, AI/ML, and reduced latency.
  â€¢ Multiple personas (developers, business users, ingesters, operations, governors, analysts, support) are defined; individuals may hold multiple personas.
  â€¢ Lumi Academy and learning materials will support onboarding and role-specific training.
  â€¢ The Lumi usage policy formalizes responsibilities, security controls, governance, and change management; key role definitions (application owner, data ingestion owner, data steward) were presented.
  â€¢ Policy scope includes ingestion, security, lifecycle, BI tools, sharing, quality, and change management.


ðŸŽ¯Todo List

  â€¢ Product Operations / Platform Team:
    
    â€¢ Publish Lumi Academy onboarding schedule and materials to all users, by 2026-01-23 (Deliverable: link to training schedule and content).
    â€¢ Ensure portal personalization and user management features are ready for Getting Started video release, by 2026-01-30 (Deliverable: portal demo and access checklist).

  â€¢ Data Governance / Lumi Governors:
    
    â€¢ Finalize and publish role-specific checklists (Application Owner, Data Ingestion Owner, EAM 7E Data Steward) aligned with Lumi usage policy, by 2026-01-22 (Deliverable: role checklist documents).
    â€¢ Define monitoring and change-management procedures to be shared with teams, by 2026-01-29 (Deliverable: monitoring & change management playbook).

  â€¢ Project Space Developers / Dev Leads:
    
    â€¢ Prepare sample DAB pipeline templates and HyperDrive/SchemaFactory usage examples for developer onboarding, by 2026-01-26 (Deliverable: pipeline templates and example notebooks).

  â€¢ Data Ingestion Owners:
    
    â€¢ Inventory current data feeds and map them to Lumi DAB movement framework; identify feeds requiring remediation, by 2026-02-05 (Deliverable: ingestion inventory and remediation plan).

  â€¢ All Users / Attendees:
    
    â€¢ Identify your Lumi persona(s) and enroll in the recommended Lumi Academy modules within two weeks of this meeting (Target: enroll by 2026-01-30).
    â€¢ Review Lumi usage policy and confirm understanding via the knowledge assessment in Lumi Academy within three weeks (Target: complete by 2026-02-06).

If you need any clarifications or want these items assigned to specific individuals with calendar invites, please advise and I will update the Todo List accordingly.


Transcription

00:00:21 - 00:01:25 Unknown Speaker:

In this video, we will discuss how Lumi as a next-generation data platform enhances your work and outline its benefits. You'll gain an understanding of your role within Lumi and how to be prepared for your learning journey. Now, let's get started. Welcome to the Lumi overview video. As an Amex colleague, Lumi will become an integral part of your daily tasks, enhancing your work with Lumi. Why Lumi? It's simple. Lumi is Amex's next-generation big data ecosystem on public cloud. It is hosted on Google Public Cloud and managed and developed by Amex. Lumi, the Albanian word for river, signifies change and renewal. That's why Lumi represents our step into the next generation of data at American Express. This platform streamlines data management, simplifies predictive analytics, provides real-time data access, and data security. But it's more than just a data platform.

00:01:26 - 00:02:26 Unknown Speaker:

It's a platform for innovation and growth. Let's talk about what Lumi brings to us. For you, Lumi ensures a smooth transition from legacy system cornerstone, accelerates outcomes, supports efficient data analysis for decision-making, and provides comprehensive training. It's a complete platform, equipped with buy tools and automated features. For Amex, Lumi offers scalability, governance, efficiency, and faster implementation. The platform represents a significant advancement in data management. One of the biggest advantages of adopting Lumi is that we will have the ability to leverage some of Google's native services to accelerate our offerings quickly and efficiently. But what does all this mean for you? Well, the benefits are substantial. Scalability is a big one. Lumi scales the data to meet your needs without compromising performance or usability. Simplicity is at the heart of Lumi. With our automated self-serve services, data ingestion becomes easy.

00:02:27 - 00:03:22 Unknown Speaker:

Personalized user experience and easy onboarding of use cases are another aspect of Lumi's enhanced servicing. This platform is designed with you in mind, making it as user-friendly as possible. Lumi's architecture is as simple as the letter C. Capture, cure it, and consume. That's the mantra we follow. The platform simplifies the implementation of advanced technologies such as AI and machine learning, making them accessible and easy to use. Lumi provides predictive analytics for targeted customer acquisition, helping you reach customers more effectively. With real-time data availability, the performance of the data processing and analytics tasks is improved. The unified data experience offers various benefits for you. All data resides in a single space, ensures data consistency and provides a comprehensive view of data. And finally, reduce latency. With Lumi, you can quickly and efficiently access your data with minimal delays.

00:03:23 - 00:04:17 Unknown Speaker:

That's efficiency redefined. Understanding your role. Everyone will play a role in helping MX realize those benefits. Whether you're an analytical user that is writing optimized queries or a product operations user. Do you already know which persona you identify with? During your Lumi onboarding process, you will be introduced to various roles that can benefit greatly from using Lumi. If you have not identified one yet, we will explain what each one entails. It is important to note that you may identify with one or multiple personas. Remember, each persona plays a unique and crucial role in making the most of what Lumi has to offer. With everyone working together, we can achieve great things. As a project space developer, you're the driving force behind creating innovative applications, dashboards, and machine learning models.

00:04:17 - 00:07:13 Unknown Speaker:

Your ability to solve complex business questions through data and analytical tools is invaluable. In the role of a business user, you translate the insights from reports, dashboards, and model analysis into strategic business decisions. Your role is crucial in steering our business in the right direction. As our data ingester, you lay the foundation for data-driven decision-making by setting up quality data pipelines. Your work ensures our data is... I don't know if this is a disorder or not. What are these two? Are they chicken diets? I'll make a paratha out of this. Will they send it tomorrow? Prudhvi, will you come from here? Will you come from here? I am not in the mood to go to office. I will come here for training. Sister, please do Prudhvi. Accurate, reliable, and timely.

00:07:13 - 00:08:10 Unknown Speaker:

Our product operations team are the gatekeepers who enable data ingestion, infrastructure provisioning, and more. Your dedication ensures that Lumi functions smoothly and efficiently. As a Lumi governor, you uphold the integrity and compliance of our data, ingestion pipelines, and use cases. Your efforts ensure we maintain the highest standards of quality and accountability. Our analytical users, your ability to turn large amounts of data into plain insights is nothing short of amazing. You help everyone understand the story our data is telling. Last but not least, our platform support team. Your role might be behind the scenes, but it's no less important. You provide the crucial... operation and technical support that keeps Lumi running smoothly. How you'll be prepared? Learning Journey Your learning process will be reinforced with a range of materials, including knowledge assessments, demos, success stories, and resources from the Lumi Academy.

00:08:10 - 00:10:01 Unknown Speaker:

Prepare yourself for an engaging learning experience where you'll actively participate, learn new skills, and be ready to apply those skills in your daily tasks. Remember, achieve that, keep up with the training, explore the platform, and ask questions. Let's dive into Lumi together and redefine the way we work. By the end of this video, you are now able to identify the benefits that Lumi offers to you, understand how Lumi enhances your work, recognize your role for contributing to Lumi success. You are just beginning to dive into the exciting world of Lumi. Don't miss our next video. Lumi capabilities. At the end of this video, you are now able to understand what Lumi is. Describe Lumi platform, its purpose, the core capabilities, that wraps up our functions and capabilities. Let's explore this next generation big data ecosystem.

00:10:02 - 00:11:08 Unknown Speaker:

Designed with modularity, scalability, and intuitive data management at its core. In this video, we'll explore Lumi. And its functions and capabilities. What is Lumi? Hello there. Let's explore this next-generation big data ecosystem, Lumi. But before we dive in, let's take a step back in time. Imagine this. We were navigating our journey with a data platform that had served us well. However, recognizing the evolving needs of our business and technology advancements, it has become clear that there are areas for enhancement and new opportunities to explore. Thus, we decided it was time for a transformation. Work began on a new platform, Looming. Looming was designed with modularity and scalability as its core principles. With Lumi, data ingestion and transformation can be automated, allowing users to set up their own DAB pipelines.

00:11:09 - 00:12:13 Unknown Speaker:

In Lumi, data can be brought in from multiple sources, both internal and external, through a unified DAB movement framework. The data is then transformed and made easier to work with by using products like HyperDrive and SchemaFactory. But what makes Lumi stand out? Well, it's the autonomy it gives you. With features like self-serve data ingestion and automated data transformation, you're in the driver's seat, managing your data pipelines independently. This, coupled with easy governance and compliance, makes Lumi a game changer. Let's dive into Lumi. Lumi functions and capabilities. Now, let's take a tour on the Lumi functions and capabilities since it's packed with goodness. There is data movement. This is about getting data from point A to point B, including ingesting data, managing metadata, and migrating historical data. Next up is the data transformation capability.

00:12:13 - 00:13:14 Unknown Speaker:

This is like being a translator for data, altering it to fit the needs of other applications and systems. The analytics capability is about making sense of large volumes of data. This is achieved through tools like Google's BigQuery, which enables the processing and analysis of massive data sets. Now, Lumi brings the power of artificial intelligence and machine learning function. Within this platform, you can create dashboards and even machine learning models for complex queries. This can help you spot patterns or generate insights for decision making. Within the portal function, you can personalize your Lumi homepage. And manage users via the portal interface. And there you have it. Whether you're interested in data ingestion, metadata, or data analytics, Lumi has a capability to help you do it. That wraps up our functions and capabilities of Lumi Video.

00:13:14 - 00:16:32 Unknown Speaker:

In summary, Lumi represents a new generation of big data ecosystems. From automating data ingestion and transformation to providing a unified framework for data movement. Its user-centric features coupled with powerful analytics tools, like BigQuery, make it a standout solution in today's data-driven landscape. By the end of this video, you are now able to understand what Lumi is, describe Lumi platform, its purpose, the core capabilities, and key functions. Stay tuned for the next video, Getting Started with Lumi, where we'll delve into the Lumi portal and all it has to offer. Until then, have a great day. Welcome to the usage policy introduction video. We are excited to walk you through our usage policy, a guide to proper data management on our platform. During this session, you will be guided through an overview of the Lumi usage policy, its scope, and the roles and responsibilities.

00:16:33 - 00:17:39 Unknown Speaker:

Let Lumi usage policy overview. Let's begin with an integral part of our operations. The Lumi usage policy outlines the frameworks, processes, and operating principles necessary for onboarding onto the Lumi platform for valid business purposes. Additionally, our Lumi usage policy perform multiple key functions. It lays out the frameworks and guidelines to onboard the platform for a valid business purpose by the federated teams. It delineates clear roles and responsibilities for data ingestion, security, consumption, and management. It sets forth guidelines for analytical and production project spaces. It incorporates provisions for user activity monitoring. It defines change management protocols. It is important to note that the Lumi usage policy serves as a dynamic framework. While it does not adapt with each new functionality released, it sets forth... guidelines on how these new functionalities should be introduced, ensuring it remains relevant to the foundational Lumi platform capabilities.

00:17:40 - 00:18:34 Unknown Speaker:

The policy outlines the do's and don'ts for the user community, providing consistent guidance. Let's now explore the reasons why this policy is crucial for our users. At MX, we really care about keeping data safe. This policy spells out exactly what's expected. So, to make sure we're all on the same page, With this big priority, it's super important for you to understand your role, what you're responsible for and what the stakeholders expect from you. Keep data safe and sound by using the right access management practices. Use your workspace wisely to do your job. Stick to the rules, meaning the American Express management policies and technology standards. As a user, you're not just signing up for responsibilities, you're all. Also getting benefits from the Lumi usage policy. Here's what you're getting.

00:18:35 - 00:19:24 Unknown Speaker:

One goto document that bundles up all the processes and standards you need to know. A framework of what to do and what not to do on the Lumi platform. A clear breakdown of who does what, for both you and the folks you're working with. A full set of guidelines on how to use data without stepping over any lines. Don't forget, sticking to the Lumi usage policy keeps our data safe and our platform running smoothly. All play by the rules to hit our business targets. Let me use such policy scope. Now that you've got the hang of your usage policy duties and perks, let's check out the areas that fall within the user as responsibilities and scope. First off, we have data ingestion. This provides guidelines for the proper collection and import of data into the system.

00:19:24 - 00:20:32 Unknown Speaker:

Then, there is data security, which includes measures to protect data from unauthorized access. Project spaces are set up efficiently for users. Then we move on on to data life cycle management, which manages the flow of data throughout its life from creation to deletion. According to In 79 and In 08 guidelines, we also use business intelligence tools. These help us to analyze data and gain insights for a valid business purpose. Now, data sharing and transfers is another important area. This lays out the rules for transferring data between parties. Also there is data quality, which maintains the accuracy and integrity of data. And lastly, change management, which outlines the procedures for handling changes to the system or data. And that's it for the scope of Lumi usage policy.

00:20:32 - 00:21:24 Unknown Speaker:

It covers all these areas to ensure that you are making the most of your data, keeping it secure, and managing effectively. Roles and responsibilities. Time to uncover the distinct roles and responsibilities that owners have in the Lumi usage policy framework. As an application owner, you play a key role in managing data sources. Your job, based on the EAM-30 guidelines, is to look after the source applications, handle the flow of data to Lumi, and have a good grasp of both business and tech aspects. Next up, let's discuss the role of a data ingestion owner. Your job is to make sure the feeds and tables in Lumi follow the usage policy and IDL guidelines. You're also in charge of managing how data comes into Lumi, along with handling any changes to the data.

00:21:25 - 00:22:18 Unknown Speaker:

Next on our list is the role of the EAM 7E Activated Data Steward. This role is crucial to keeping data in good shape. You're responsible for handling critical data elements throughout our organization. This means defining what the data means. Tracking where it comes from and making sure the quality of the CDE stays high. Each role is key to using and governing data in our organization. Following the Lumi usage policy helps us use our data responsibly for valid business needs. In line with other AXP AIM policies and tech standards. Before we wrap up, let's take a moment to review what you are now able to. Understand the importance and benefits of the Lumi usage policy. Outline the main aspects of the Lumi usage policy, and identify your role and responsibilities. And there you have it.

00:22:18 - 00:22:34 Unknown Speaker:

You are now fully equipped to dive into the Lumi usage policy. Feel free to re-watch this video or check out our resources like Lumi Academy. Continue your exploration, and we look forward to your participation in our Getting Started with Lumi Usage Policy video.2026-01-16 SFTP Configuration and Lumi to ACE Data Transfer Process

Creation Time: 2026/1/16


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-16 18:40 (Duration: 2591 seconds)
  â€¢ Location: Remote (recorded session)
  â€¢ Attendee: Sachin, Kundan, Prithvi, Suman, Nirmal, Samir, (others referenced: Abhilas, Jitendra, Sikha, Saranya)


ðŸ“’Meeting Outline


SFTP configuration & mapping (Lumi â†” ACE)

  â€¢ Purpose and profiles Explained that file transfer between Lumi and ACE uses SFTP as a middleman. Two separate profiles were created: one for sending files from Lumi to SFTP (Lumi_TST / LUME TST) and another for ACE to read files from SFTP (CDO_TST / UCDO THT). Profiles act as credentials/user IDs for the SFTP access. Separate test and production profiles are maintained.

  â€¢ Viewing and inspecting profile details Demonstrated how to open the File Exchange (FileX) profile details through the portal: File Exchange â†’ Profile view â†’ FileX account â†’ select environment (test/cert/prod) â†’ click Details. Custodian/owner name appears in the profile (example: Sachin). Emphasized that the portal can show profile status (enabled/disabled) and certificate details.

  â€¢ File mapping and naming convention Showed File Exchange Map view â†’ inbound mappings. File mappings are created against file name patterns. Required naming pattern for Lumi files: filenames must start with "Lumi_DQResults" (examples provided: Lumi_DQResults_123, Lumi_DQResults_rejects_1). Mapping ties a specific filename pattern to a sender profile and a receiver profile (sender = Lumi profile, receiver = ACE profile).

  â€¢ Status, alerts, tracking
    
    â€¢ Profiles can be enabled/disabled; disabling will stop transfers.
    â€¢ Notifications can be enabled to receive email alerts on file transmission.
    â€¢ File tracking (Track Files / Base file) was demonstrated: the track shows lifecycle events (pushed to SFTP â†’ catalog â†’ file mailbox â†’ receiver inbox).
    â€¢ The SFTP inbox/outbox/send folder flow was explained: uploaded files land in Inbox â†’ Outbox â†’ Send folder; send folder files are auto-removed after 24 hours. FileZilla was used to show receiver view and manual upload for testing.

  â€¢ Operational notes
    
    â€¢ Certificates must be monitored for expiry.
    â€¢ Someone must ensure profiles are not accidentally disabled.
    â€¢ Manual uploads are used for tests, automation handles production transfers.


Lumi code walkthrough â€” how Lumi posts files to SFTP

  â€¢ Airflow / Tag scheduling and Dataproc The job is scheduled with a tag (cron). To access external services (SFTP), a Dataproc cluster is created. The cluster creation installs required packages via an initialization script (package list provided in the script/package.sh). Most of these config items are static but can be updated when needed.

  â€¢ Cluster job submission and job content After cluster creation, a job (PySpark) is submitted to the cluster. The job executes Python code that builds the CSV file and pushes it to SFTP.

  â€¢ Data extraction and file creation The get_data operator runs SQL (BigQuery) to read DQ tables from GCP/BigQuery. The query output is transformed into a CSV with a particular format (including trailing zeros added by GCP formatting as mentioned). The CSV file is stored locally (GCS/local path as defined in config).

  â€¢ SFTP connection and transfer The job uses a Python SFTP library (pysftp / paramiko-style connection via a helper SFTP connection module) that creates an SFTP connection using host, username, credentials, and host key. The CSV file is converted to a blob-like object (large file/object construct) and uploaded via sftp.put(local_path, remote_path) into the SFTP receiver inbox. The connection is closed after upload.

  â€¢ Blob explanation Clarified that "blob" in this context means a binary large object representation of the file used when transferring; it is not the same as Azure Blob Storage.

  â€¢ Exception handling gap Noted code observation: if an exception occurs, the SFTP connection may not be closedâ€”exception block should explicitly close the connection (add finally/cleanup).


ACE side (brief recap)

  â€¢ File download and processing ACE creates its own SFTP connection to read/download the file from SFTP, then proceeds with downstream processing. The ACE side workflow and code walkthrough were previously covered and only briefly referenced here.


Dashboard and Kibana (deferred)

  â€¢ Status & plan Dashboard code walkthrough (Kibana/Elasticsearch index creation, parent/child index, query tuning for full-text search) requires more preparation. Team agreed to park detailed dashboard/code walkthrough and handle on next call (Monday). Index creation and the query changes will need a dedicated walkthrough.


CICD, repo & deployment process

  â€¢ How BPMN and assets are built BPMN creation in the UI triggers backend builds. Previously an asset repo was used; builds now run via GitHub Actions. When a BPMN is saved, a tenancy selection occurs and build pipelines operate automatically.

  â€¢ Manual pipeline triggers & promotions Demonstrated Actions â†’ Run Workflow to promote builds across environments (DEV â†’ E2 â†’ E3/Prod). To promote to production, an RFC number is required.

  â€¢ RFC / deployment approvals RFC flow: submit RFC (contains implementation, backup, validation steps), approvals required from multiple teams (Lumi Big Data support, authentication team, etc.). Raise RFC at least 48 hours before scheduled deployment.

  â€¢ Platform team involvement for metadata/table promotions Some tasks (like promoting table metadata to E3) require the platform team because the project team lacks E3 access.

  â€¢ Reuse & repo segregation Discussed possibility of creating a shared utility repo (for SFTP utility) vs using the same repo for different projects (QTrack vs Razy). Action to check feasibility of separating repos and CICD segregation to avoid mixing use-cases.


Team responsibilities, training and follow-ups

  â€¢ Skill expectations
    
    â€¢ Suman, Prithvi, Nirmal: need to review code and understand main flows over next week (Python/airflow/operators).
    â€¢ Samir: focus on Lumi side if not comfortable with ACE side.
    â€¢ Developers asked to check exception handling and SFTP connection cleanup.
    â€¢ Python familiarity is important; Java experience is expected to carry over.

  â€¢ Knowledge sharing & upcoming sessions
    
    â€¢ Plan to run Razy overview starting Monday (7:00 PM / 6:30 PM depending on availability).
    â€¢ Additional session(s) planned: CICD demo and dashboard/code walkthrough Monday.
    â€¢ Schedule hands-on Cointreau/CICD demos for Prithvi and Suman to get practical experience.

  â€¢ Operational housekeeping
    
    â€¢ Update contact details and holiday calendar to avoid surprises.
    â€¢ Team members should add themselves to the project page/contact list.


ðŸ“‹Overview

  â€¢ SFTP is used as an intermediary for Lumi â†’ ACE transfers; two separate profiles created for send and receive (test and prod separate).
  â€¢ File mapping requires filenames to start with the agreed pattern (Lumi_DQResults_...) and mappings tie patterns to sender/receiver profiles.
  â€¢ Portals allow viewing profile details, enabling notifications, tracking file lifecycle; disabling profiles blocks transfers.
  â€¢ Lumi pipeline: scheduled Dataproc cluster, run PySpark job that queries BigQuery, writes CSV, converts to blob-like object, uploads via SFTP (sftp.put(local, remote)). Exception paths must ensure SFTP connection closure.
  â€¢ ACE pipeline: creates SFTP connection to read/download and then processes files (covered earlier).
  â€¢ Dashboard/Kibana walkthrough deferred; requires more preparation (index creation, query tuning).
  â€¢ CICD uses GitHub Actions; promotions require RFC and approvals; platform team handles E3 promotions where needed.
  â€¢ Team training: focus next week on code familiarity (Python/operators), CICD demo, and Razy overview.


ðŸŽ¯Todo List

  â€¢ Sachin:
    
    â€¢ Schedule Monday sessions: Razy overview (7:00 PM / confirm 6:30 or 7:00), CICD demo, and dashboard/code walkthrough. (Due: ASAP â€” before Monday)
    â€¢ Share RFC/deployment overview and walk team through RFC creation and approval process. (Due: During CICD session / Monday)
    â€¢ Ensure contact list and holiday calendar are updated; request all team members to add details. (Due: End of this week)

  â€¢ Kundan:
    
    â€¢ Share FileZilla screen and SFTP receiver view recording (or live) for team to verify file routing. (Due: Next meeting / Monday)
    â€¢ Update code to ensure SFTP connections are closed on exceptions (add finally/cleanup). Provide PR or patch. (Due: Within 3 working days)

  â€¢ Prithvi:
    
    â€¢ Complete ESCO services setup and confirm readiness; assist in Lumi/ACE setup if not completed today. (Due: By Monday)
    â€¢ Attend hands-on Cointreau/CICD demo and take ownership of initial hands-on tasks. (Due: Next week â€” coordinate with Sachin)

  â€¢ Suman:
    
    â€¢ Review Lumi codebase (Python/data pipeline) and raise clarifying questions within the next week. (Due: 1 week)
    â€¢ Update contact details on project page. (Due: Today)

  â€¢ Nirmal:
    
    â€¢ Review code (focus on Python operators used in the pipeline) and raise clarifications within the next week. (Due: 1 week)
    â€¢ Update contact details on project page. (Due: Today)

  â€¢ Samir:
    
    â€¢ Focus on understanding Lumi side flows (file creation & SFTP upload); review relevant code and ask questions. (Due: 1 week)

  â€¢ Dev/Platform team / Assigned Platform contact:
    
    â€¢ Check feasibility of separating shared SFTP utility into a separate repo (and separate CICD) to avoid mixing QTrack and Razy use-cases. Provide feasibility assessment. (Due: Next sync / by Monday)
    â€¢ Support promotion of metadata/tables to E3 (raise tickets and approvals as required). (Ongoing as needed)

  â€¢ All attendees:
    
    â€¢ Review the recorded session and go through the code (Lumi pipeline & SFTP connection). Post questions in project channel within one week. (Due: 1 week)
    â€¢ Ensure holidays and contact details are updated on the team page. (Due: Today)

If any attendee was missed in the attendee list or any action owner needs adjustment, please respond with corrections so the todo list can be updated.


Transcription

00:00:00 - 00:00:57 Unknown Speaker:

Getting a file and sending it to the SFTP and then this SFTP configuration, I'll explain it to them, how to do this SFTP configuration and all. And then this part, anyhow, we already covered up, so this will wrap up the first of all this Q track UI, and once we covered up this one. Then, uh, give the overview of this dashboard that you have created. Okay, or if, or if it takes much time, then we can park it for the next call. Uh, sure, okay, okay. So let me start the recording. Let me know once you're ready with the details so that I'll, uh, start the recording. Yeah, yeah, we can, uh, uh, okay. Should I start the recording? One second? Let me open the tag. Okay, yeah, yeah, that is the reason I was waiting.

00:00:57 - 00:02:04 Unknown Speaker:

Once you're ready, then only I'll start the recording. Open the tag, open the code base and give me those two profile ID. And and make sure one thing as we are recording the session, remove the password from that code for time being, just remove the password from there. I don't have it on local. Just give me a second. Just clone it. Then just clone it. And one more thing, give me those two profile ID. I have to show that SFTP mapping, right? So just give me those two profile ID. Okay, I have it opened. So you want me to do or you will do that? Anything is fine. No, no, you won't be able to show them the mapping, right? You can open the file zilla right but you won't be able to show that mapping.

00:02:04 - 00:03:09 Unknown Speaker:

I'll show that mapping just give me those couple of profile ID. Till the time Kundan is preparing himself I have asked Prithvi to set up the ESCO services. Hopefully, that should have been done by today. But if not, once he is ready, he would help you to set up the ESCO services. Okay, maybe on Monday connect with him. Okay, okay, that is required for this. Uh, you know this, uh. Q track UI, right? Because in case there are some issues and you know that need to be handled during the onset, then you can pitch in and then take care from there. Yes, such a like that Bp and BpM, right? Yeah, because last Thursday, myself and Kundan, we wished each other till 12 a.m. Okay. No, last Wednesday. Last Wednesday. Okay. We wished each other till 12 a.m.

00:03:09 - 00:04:17 Unknown Speaker:

So, there was an issue. So, going forward, what I'll do, if there is something issue, then I'll work with you. Or if some some other engineers comes in the USD house, then he can coordinate with you and then you guys can work during your working hours. Okay, such a you want product details, which one broad detail also SFTP project. No, no, no, this is fine. Uh, so you have shared one, is this USC? Do TST? And yeah, correct these two. Just give me a second. Okay, let me delete it, just give me one second before going for recording. Okay, do one thing in the time you are covering, uh, this one, right? Uh, code workflow, let me tell them the SFT configuration in between, okay? Yeah, yeah.

00:04:20 - 00:05:18 Unknown Speaker:

I'll share my screen and just let me know, guys, once you're able to see my screen. Let him be ready till the time I'll share you what SFTP configuration is there. Okay. This meeting is being recorded. So, This, you know, right? Like we are sending the file from the Lumi to SFT, and then ACe is reading the file from SFT. Okay, so for sending the file to the SFT from Lumi, or for reading the file from this Ace platform? We need a profile ID. Okay, it's just kind of a you know, user ID for logging to that SFT. So that you can place your file over there, okay, or to read the file over there. Okay, so we have created one profile for sending file from Lumi to SFT and another profile for reading the file from that SFT.

00:05:19 - 00:06:24 Unknown Speaker:

I hope this is clear. Or do you have any questions on this? No. Got it, right? It's just like a handset that is being done, right? You want to log into, suppose, any system, right? So you need a credentials, correct? So even for file transmission, that needs a profile through which they can send the file to this one or to read it. You can utilize the same user ID, but we would like to avoid it because these are two different platforms. So we created two different profile. So this USDCHT, let me go like this one. This Lume TST would be responsible for sending the file from Lume to SFT and then your CDO TST is responsible for reading the file from SFT. Okay. So now this is the SFT place where you would be able to see the mapping, the mapping that we used to do.

00:06:24 - 00:07:15 Unknown Speaker:

So if you want to view a profile details, you can click on this chat box. Okay. And suppose I would like to check the details of this Lumi one, let the chat box to open and then I'll send it to you. I don't know why it is extremely slow today. Okay, so from here you can view profile details, you can click on FileX account, just paste the user ID and this is for test environment, not for the production, so you have to select cert. Similarly, we have details for the production environment that is having a different profile, so we maintain a different profile for test and production. So if you can click on details, this should share you the details. Like, you know, uh, this is the file, family, uh, and this is the profile details.

00:07:15 - 00:08:23 Unknown Speaker:

Basically. Okay, uh, you can see this. My name, the reason being I have created this one, and, you know, custodian for this profile, that's the reason my name is being appeared over here now. How the mapping has been done so far? In order to understand the mapping, you have to go to this menu File Exchange Map View Profile. Okay, and then I have to click on File X Inbound Okay, the file that we have created for this. Give me a second. I think it starts with Lumi. What was that? The file that you are creating. One second, Sachin. It's lomi underscore bqresult. Bqresults. So this is the file mapping, right? So when we are creating the file, right? So with that file name, you would be creating a mapping over here, okay? Tag two.

00:08:24 - 00:09:09 Unknown Speaker:

Your file names should always start with Lumi DQ Results, post that you can provide any other abbreviation if you want to provide it. This is the file X space. Okay, and look, this is the file name file profile with which this file has been attached. Okay, so this profile would be able to send this file with this name, okay? And if you go to this one, you can see that post Lumi DQ result. There would be anything that you would like to place and which is the profile that would be reading this file. This is the UCDO THT, which is from the A side, so this configuration I have to do it over here. So this is your sender which would be sending this file, and this would be the receiver which would be receiving this file.

00:09:09 - 00:10:10 Unknown Speaker:

Okay, so your file names would start like Lumi Underscore DQ results post, that means I can send a file like Lomi DQ results. Pose that one two three. This is one combination. I can send a file I can send a file or DQ results underscore one two three four I can send a file. It's a rejects. Basically, I would say okay dig your results Underscore even such in Okay, any questions on this mapping? Can I take it as a No? Yeah, okay. So this is basically, uh, the mapping that we need to do in this SFT portal for, you know, sending and receiving the file. So this was the configuration that I would like to show you guys. It's a one-time configuration, it's a one-time configuration.

00:10:10 - 00:11:08 Unknown Speaker:

And maybe we need to see if certificates are getting expired that might be required in early ones. Yeah, means you have to keep an eye on these. Like, you know, somebody should not come and disable this profile, right? So right now this is an enabled state, right? Somebody can disable it, right, you can. Uh, if I try to take an exam, uh, Lumi, Lumi. DQ results are on the right. Um, yeah, DQ details. So I can even disable this one, so if I disable it one, that means your file transfer would get impacted and you would not be able to transfer the file anymore. Okay, and when there would be a file transfer, we can set the alerts also over here. Okay, I can, uh, suppose I would like to get a notification whenever there would be a file transfer.

00:11:08 - 00:11:58 Unknown Speaker:

I'll show you this one so I can get a notification like this one, like. You know, for this one, there was a file that has been sent over here. OK, for this test environment, we did not enable it, but I can enable the notification also. Like whenever there is a file that has been transmitted, it would send an email communication to me. This is number one. Second, using this one, right, you would be able to track the file also. Suppose I would like to track this file. OK, so again, go to this bot. Here is the track file option is there, or you can set it via base file. Okay, whichever is there, like, you know, you can select that one. Suppose I select this one with a ProD environment, so it would give me the details wherever it is.

00:11:58 - 00:13:37 Unknown Speaker:

Okay. So I think it's not founding because there might be some glitch with the name or something. Let me try with the base file environment, right? And I'm trying to fix the production environment file, so it would not be possible. So for that, I have to go to this SFT central that is the E3 environment for SFT mapping. So again, here the you know, same thing is comes. You can just click on this part Track Files, Base file, paste it, let's try if it fetch. No. This is pretty surprising. These are the three different files that has been sent it over. Okay, this is for today's data around this time. So when you click on this tracking ID, this would display where exactly is them. So file has been pushed to SFTP, right?

00:13:37 - 00:14:36 Unknown Speaker:

So source has sent the file to SFTP. Then it went to the catalog, then the file mailbox to the this Lumi PRD, which is the receiver, so even the file has been received to the Lumi PRD. It's inbox now. Any questions on this? Otherwise, I'll ask Kundan to share the file and then there you can see it. Let me know if you guys have any questions on this. No? Kundan, if you can log into FileZilla, just share your screen once you log into the FileZilla. So that is one way, guys, you can check your file routing. Through that bot, otherwise, this is, there is another mechanism called this file, or even you can track it via your Intellij also, so this is another portal. When you connect, right, you can see this one, right?

00:14:36 - 00:15:34 Unknown Speaker:

I have already told you guys, right? There are three boxes, one is Inbox, one is outbox and one is the send folder, right? So whenever file gets loaded, it comes to the first of all inbox. From inbox, it would move to the outbox, and then finally, once you did it, move to the send folder, post 24 hours, the file would be automatically removed from the send folder. Also, okay, for example, let me upload it, okay, so I'll see I'm uploading a file, so this is your sender. Okay, so if you see the username right, so it's sender username, so it started uploading and it's successful, right? So now if you'll click on inbox, it take few seconds. Okay. Now you have selected, I think send folder while uploading that might be there in the send folder.

00:15:34 - 00:17:05 Unknown Speaker:

Check that one. Yeah, it was in send folder. You have selected the same folder. So I'm going back to. Can you stop sharing? Can you stop sharing? Let me... I will share again. Just give me a second. Let me connect. Let me know if you are able to see the screen. So this is the receiver one, so whatever we have sent, you can see those five here. Oh, one second, just a second. This password, sir, it's correct only. So why is this? Manual upload is required. We cannot put any script there. Annual upload is not required. P.3. It was all, uh, through the automation. We were just letting you know that these are the tools which you can monitor. Okay, okay, so we are just explaining you the process. Okay, yeah, no, no, no, you can automate it and anything.

00:17:05 - 00:18:06 Unknown Speaker:

Yeah, so just for testing purpose, sometimes you need it. Okay, okay, so I connected with this receiver one, so in the outbox if I click on. You can see the file, right? Right now it is in Outbox. Once the automation consumes this file, it would move to the send folder. Now, Kundan, you can go ahead and then brief the Lumi code base. How we are creating connection to SFTP. So, you know, Now I'm testing you. The process where we are, you know, sharing the data from Lumi to Ace, so to directly we cannot share the data. So what we did is we created a middleman, you can consider it that is SFTP. What we are doing is whatever the data we need, we are creating it, and that data we are posting it over the SFTP.

00:18:06 - 00:19:00 Unknown Speaker:

So we created a tag. The name, if you will see it right, will upload file to GCP, upload file from GCP and send it to SFTP. Okay, uh, basically, if you will see, we have, we have different full uh files are there we are using. Uh, I will explain you one by one. So this is the tag ID, and this is the scheduled one, if you see this is the schedule we have given. The, you know, the duration is already mentioned, the cron expression and this is the tag you can provide it, and these are the operators we have provided. Okay, so, uh, to connect to, you know, the external services like SFTP or some other services, right? You have to create a cluster, so we created a Dataproc cluster here you can see, and within the cluster.

00:19:00 - 00:19:53 Unknown Speaker:

You have to provide the script to URI, so which URL, which script you want to basically execute, so this is the file actually, if you will see, this is the package. Sh. What we are doing is before going to create the cluster, right? So we are providing which all certificate will be needed, which all you know, the external packages will be needed for this particular, uh, you know, for to connect with the SFTP. So these are all the, you know, the packages that needs to be installed there, so while creating the cluster, these all packages will be installed. Till here, any, any questions? Yeah, this is the configuration one then that you cannot do anything. So those all things will be keep changes and you have to update it, but most of the time it will be static.

00:19:53 - 00:20:34 Unknown Speaker:

Okay, okay. So after creating the cluster, what we are doing is we are submitting this cluster. Okay, submitting this job. So this cluster is a job basically. So this job we have to, you know, submit it. So before going to submit it, we have to tell it within the cluster what we want to execute, so we are telling so within this cluster. We have a job that is called the, you know, the PI spark job, you can tell. And then what we are doing is we are creating this, uh, SFTP connection Dot PI, so we are maintaining that you have to, you know, run this particular PI script file, so to run this, we have this, you know, Python provides this library that is called the PI SFTP. Okay, and where we have this, uh?

00:20:34 - 00:21:45 Unknown Speaker:

Con output, which provides you to provide the host key and create the SFTP connection. So we are providing our host name, username, credentials, and other details. Okay, and then once the connection is established, okay, so okay, okay, fine. So what we are doing is basically we are reading a CSV file, so okay, fine, fine. So basically we are creating a CSV file and this CSV file. We are, you know, posting the data. So one second, uh, where we are creating the CSV file. Okay, send file from this bit. One second, just one second. I think I missed one thing, just a second. It creates a SQL file. Yeah, yeah, okay, sorry, I missed this one. So get data operator is there, so what we are doing is basically within this diagram, only these all methods are there.

00:21:45 - 00:22:36 Unknown Speaker:

If you'll see after creating the cluster, next step is to get data operator, so get data operator. There is a this SQL file is there, if you'll see this file, right? What we are doing is basically we are reading the data from our DQ digital table. Okay, and then what we are doing, we are creating a CSV file. If you'll see, this is the format of this CSV. Okay, and this is star, it will, you know, uh, this, uh, gc Gcp. Add some, you know, zero, zero, zero kind of thing at the end, what we are doing here in our SFTP connection. So we are providing the path of this particular file because it is going to be stored in this location. As we mentioned it here in this one, if you'll see the path we already mentioned here, right?

00:22:36 - 00:23:29 Unknown Speaker:

Okay? So that particular location, we are reading it, and that particular file, we are creating a blob object, and that blob object we are uploading. We are, you know, putting in a local file path and this is the remote file path. Okay, so what it doing is basically from this local file path, it pushing this particular file to this particular remote location. Inside the inbox of your uh, this one uh in sorry in box offered SFTP, okay, so here blog download this particular file name, and if you see it here SFTP dot. Put this local path to this remote file path, so from sender location, basically from your local to your remote location, and the file will be get transferred. And we are closing the SFTP connection here, so this is the entire mechanism.

00:23:29 - 00:24:38 Unknown Speaker:

While we how we are, you know, creating the file, creating the file, using the database, and that particular file. We are transferring it over the SFTP from Lumi, any question in this? What database they are using there on the on the Lumi side? It's a GCP, right? That, uh, through the big query only Google storage, we are pulling. Yeah, we are pulling the data using the GCP query. Anyone having any doubt please come up and post that we will take it up. So what is the purpose of Blob here? Such like, is it a container? Or like? No, no, no, see the file, you cannot send it normally. So what we are doing is basically we are, you can say, not encrypting exactly. Uh, blob is a data type actually.

00:24:38 - 00:25:39 Unknown Speaker:

So we are converting the file to that blob object, okay, and this Blob object, we are transferring it. You cannot transfer the directly file. Okay, you have to convert into the Blob object inside it. Okay, got it? If you want to, you know, Samir, send a file. Like object, right? Yeah, in that case, we have to go with, uh, this blob that is, uh, for handling the large object, right? Yeah, yeah. So that is the reason why I have asked you, because, like in Azure, we have a blob storage something. So I thought, like, something, uh, that we have seen that in Google also. Okay, so that's why I have asked, but this is different. Yeah, got it, yeah, this is different. So yeah, anyone, any question in this process? Okay, if no, pose this, you know?

00:25:39 - 00:26:47 Unknown Speaker:

When the file gets, send it over to the SFTP, right? The next step is to download the file, which we already discussed in the last call, right, Uh, the Ace side again. It would create a SFTP connection, read the file from the SFT and then download. And then the next processing code. Walkthrough is also done for that. So that is the steps for this one. So this cover of this Qtac DQ IAM portal code walkthrough. Okay. Now, let's move to the dashboard creation one. Okay, on the Kibana side, right? Okay, fine. Yeah. Do we need time for that to prepare it? Because that would be the code walkthrough. You can, you already walked him through the code, right? No, no, for this one, I did not let them walk through the code, so this would require a part of code walkthrough.

00:26:47 - 00:27:44 Unknown Speaker:

So if you require time, we can do it on Monday. Uh, no, let's close it, okay, then don't have too many things here. So yeah, yeah, it would not be here, man, it would be there in that, uh. Configuration file where we have configured the input configuration. Oh, you want to do those things. PI repository and all. Then we need time. Let's do that on Monday. Let me stop the recording. That I was talking about the config that we'd made and created a... Index. Index, right? We'll have to tell you all that. No, then parent and child index, right? For that, let's connect... I also need some time. Exactly exactly that, that index creation. And then then you have to explain, how did we twist the, you know, query over there in the deluxe text search to populate those data?

00:27:44 - 00:28:35 Unknown Speaker:

And all right, so that take time. So guys, do you have any other questions? Feel free to ask. Look, we would be working. And then anyhow, you guys always reach out to me, but I need you know if you. If you get a thorough understanding right now, it would be easier for you to speed up the work. And then you would not rely on anyone to, you know, multiple time chasing because every individual are having their own task, right? So they need to pitch in on their task to complete it, right? So I need Suman, Prithvi and Nirmal to understand a bit of pieces of even the code. You can go through the code. If you have any query, clarify within the next week because we have to speed up once. You know, this kid is being over.

00:28:37 - 00:29:40 Unknown Speaker:

And I have plans. I mean, I have included you also in these calls along with NC. The reason being, I'll suffer you guys now here and there. Okay. Maybe few folks may be dedicated to some modules, but like I pulled you to this incident creation, right? So don't restrict yourself. With incident creation only. Understand the flow, how it is working in DQIM so that you would be able to take care of the end-to-end process journey. Yeah, sure. Right, from delegator, from the file download, anyhow, this one, right, transferring a file from Lumi to SFT, you already did another piece of job for Razy, right? So you are aware with this stuff, correct? So at the same time, please make sure you understand the ace part also from the delegated itself. Don't restrict yourself in the service.

00:29:40 - 00:30:39 Unknown Speaker:

Now integration only okay, and Samir even not on a side. Uh, at least you can focus on the Lumi side, right? If you are not feeling comfortable at the A side, at least on the Lumi side right, if there is an issue. You should be able to troubleshoot that one. Yeah, sure. Okay. And Nirmal and Prithvi, I guess Python might be new to you. Spend some time to understand that one because that is again, you know, major project is being there. So we need your contribution over there. Okay. Apart from the Java one. Java, anyhow, I expect you guys to catch it up. Very quickly, having six, seven years of experience Python, I can still give you some time. But still, you know, spend some time, uh, to understand how these operators and all function and utilities are working.

00:30:39 - 00:32:04 Unknown Speaker:

Okay, uh, then do one thing. I have seen your code. One observation is there in the exception block, close that SFTP connection that is not being there. Number one is that, and second way, is that serious? When I tell you once you are sending the file, you are closing the SFTP connection, right? But if there is a exception, you are not closing that, so in that exception block, who closed statement? Although, what's that? Is one second now, as IAPA is also going to utilize this one, right? Create a utility for this. Um, and figure out one way if we can take it up to our. For last year, we have asked, I have asked them to, you know, use the same repo, but just check the feasibility if we can deploy it to a different repo.

00:32:09 - 00:33:12 Unknown Speaker:

Otherwise, what would happen, you know, we would end up missing RACI also with this QTrack project. Because your use case stands for RACI, right? This is fine until the time it is using for Q Track, right? We are still fine with that, but I don't want to mess up any Raisey. Use case code configuration in this Q track module. So you would be needing only that pipeline creation, right? CICD1, so check the feasibility. Let's, let's, let's segregate that one. Maybe on Monday, uh, give a demo on the CICD to these books. Okay, sure, oh, one one session may be on CICD so that they would understand how it is going on. Yeah, got it. So, um, that was your question today, right? So for the BPMN that we are creating, right, that is getting, uh, automatically built in the system.

00:33:12 - 00:34:36 Unknown Speaker:

Okay, when you click on Save or submit, right, a PR gets generated automatically in the backend. We used to have a asset repo, but now they I'm sharing my screen, let me know once it is visible. So when you are creating a BPMN, right, so once you create a BPMN, right, so this once you select this, uh. Save BPMN, right? It automatically ask for this tenancy where you are saving it. Once you create this one, this get build in the backend by itself. So they used to have this asset folder. This is framework asset is there. Okay. So that is being done. Basically, we are using GitHub link for this one. They used to build it. So now they have discarded this one. They're doing it internally through this GitHub actions. Okay.

00:34:36 - 00:35:20 Unknown Speaker:

My phone was quite older because I was not working on this from last one year. That is the reason you're not seeing the latest change. But on this actions, they used to build it. Let me show you one of our record that we are building. So suppose. You made the configuration, you updated your jar over here, right? And this is deployment config. Suppose we updated our jar over this one, right? Once that is done, you just need to go to this actions. Okay? And here, if we are deploying till E2, select this E1 E2, and we just need to click on Run Workflow, that's it. It would build it automatically. You need to just promote the pipeline to the next, next, next environment. So this has been deployed till E2, once I approve it to E2, it would deploy to E2.

00:35:20 - 00:36:11 Unknown Speaker:

If there is an issue you need to, that's fine. I only triggered it, I saved some APM, so once once I approve it, this would start building the your code base and everything. If there is an issue, you need to report it to the IS IS platform channel. Okay, so this is the CICD pipeline that has been configured. Uh, internally, whenever there is a repo gets created. Okay, okay, and uh, as I was mentioning, right? For India, they used to have a different pipeline because that goes to a different server. Okay, on our project, we are not using any India pipeline, the reason being nothing is being deployed on the India side. Okay, from our for our use case, but if some other use cases are there, they can have India tendency in India pipeline.

00:36:11 - 00:37:03 Unknown Speaker:

This is not enabled for us, so even E2 for till E2. And if you are promoting the changes to production environment, then you have to go through this E1, E2, E3. Here. We have to provide the RFC number which AirPA is creating on a daily basis, right? You might see AIRPA pointing on a daily basis that this is the RFC or something, right? So we have to. When we are deploying in the production, we need a valid RFC. Otherwise, it would not promote to the environment. OK. OK. And RFC creation and all you guys would get to know or maybe I'll ask you to give you a overview once. So he's basically providing the details. Suppose this is one of the RFC. Right. So suppose this is for the plant 19th. Right.

00:37:03 - 00:37:57 Unknown Speaker:

So if you go over here, I hope this would not get here. Sure. These are the basic details that he is providing our project, and you know who is doing this? And then once you go to this C task, this is the implementation where he is providing all the packages and all details. Like this is going to be deployed, right? This is the implementation that he would be going to do, and this is the backup instruction. In case deployment failed. Apart from this, there would be a validation task, which he would be doing post deployment. Okay, and then this needs a approval from different team, this is our team, okay, this is a Lumi big data support team, and then this is the authentication team that we need their approval. And this, this task or something.

00:37:57 - 00:38:51 Unknown Speaker:

This requires to be open at least 48 hours. All right, so this is what he is doing. As a validation log into this one, so we'll navigate to Imposter, validate all the tag, that's it, okay, uh. So for this one, we need to raise it at least 48 tasks prior to, you know, schedule it. That's the reason you can see he has already created for 19. That is on Monday, where we are for Tuesday. Man, go ahead and create it, or you already created it. Yeah, yeah, yeah, completely. Keep the deployment, you know, ready. You never know whenever there would be a deployment would be required. Are the few RFC if I, if you saw. These are some of the, you know, tables that we are promoting, so here we are not going to do any deployment.

00:38:51 - 00:39:48 Unknown Speaker:

So for this insurance when, like, I would be, you know? Promoting the metadata of this table to the E3 environment. So this task could be assigned to the platform team. Okay, so this is the platform team who would be promoting the table to E3 environment. We don't have access to E3 environment. And there again, it goes through multiple approval, sometimes we have to chase them to get the approval on this. So insurance one, it would be hardly one day, maybe in between. Next week I'll take it up, but most probably we would be starting with the Razy Kitty from Monday Monday. I know we have a packed meetings, so let me see the feasibility because I need Suman also in the call. Okay, it's a demo. Oh man, Monday, there is no grooming call, Anand asked for Monday, right?

00:39:48 - 00:40:53 Unknown Speaker:

This is surprising. Okay, see new to Tuesday. Everything okay, then let's do what. Uh, do it. On Monday, 7 p.m. Uh, we will start our uh raz overview. Okay, and on morning, ask Samir, let's close the control so that, uh. Prithvi can pitch in with his hands on Cointreau starting next print with you. Sure, sure, Sachin. Cointreau, we can give that video to Suman. But let's have this with Prithvi. And then at AZ, we can have a connect with Suman, Prithvi and Nirmal in the evening hours. Sure, Sachin. Don't worry. Set up a meeting maybe at 12. We can connect at 12 post DSU call. Sure, Sachin. And one more thing, uh, P3 in Nirmal and Suman guys. Please update your contact details over here so that, in case required, we can reach out to you where it is.

00:40:53 - 00:41:52 Unknown Speaker:

And such an uh, can you please add me on the meetings on Monday? Uh, today itself? If you have a plan to, uh, well, that would be seven. I know, man, that is the reason I kept it at 6 30, otherwise I would have, you know, I, I know it. It was just a surprise. Yeah, because if I'm planning daily 7 30, I have a call, that's fine, so it's just 7 o'clock, 6 30. Means you need, uh, can you? I got? I got your point, I'll, I'll take care, man, I'll take care. Let me send it. Today itself, Okay, I, I got your point, means, you know, basically it's glitch from my side. Actually. Take care of, okay, is that the business continuity planning? And guys? I hope you guys are updating your holidays.

00:41:52 - 00:43:02 Unknown Speaker:

It should not come to me as a surprise because, you know, we are working in Asile, right? Uh? So please make sure you are updating your holiday over here. Any any holiday that you are planning, please make sure that is being updated away. So here, oh man, why these people are there? Lots of people? Okay? Saranya did not join. Abhilas left, Jitendra left, Sikha left. Okay, uh, guys, do one thing on this page. Update your contact details over here. Okay, I'll bring it over this, make an entry of yours. Okay, any question, any queries that you guys have?