2026 Lumi Usage Policy Framework and Guidelines

Creation Time: 2026/1/26


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-26 11:57 (Duration: 1590 seconds)
  â€¢ Location: No location mentioned
  â€¢ Attendee: No relevant content mentioned


ðŸ“’Meeting Outline


Lumi Usage Policy Overview

  â€¢ Purpose and scope of the Lumi usage policy
    The policy defines frameworks, processes and operating principles for onboarding onto the Lumi platform for valid business purposes. It is a central guide for federated teams to follow when ingesting, securing, consuming and managing data on Lumi. It also sets guidelines for analytical and production project spaces and includes user activity monitoring and change management protocols.

  â€¢ Nature of the policy (dynamic framework)
    The policy is presented as a dynamic framework: it does not change with every new feature but provides guidance on how new functionality should be introduced so the policy remains relevant to core Lumi capabilities.

  â€¢ Do's and don'ts for users
    The policy provides consistent guidance for the Lumi user community, outlining acceptable and unacceptable behaviors to keep the platform and data safe.


Importance and Benefits

  â€¢ Why the policy matters
    Emphasized that data security is a high priority (Amex context). Users must understand roles, responsibilities and stakeholder expectations to protect data and maintain platform integrity.

  â€¢ User obligations and recommended practices
    Key responsibilities highlighted: use correct access management practices, use workspaces appropriately, and follow rules and related Amex Management Policies and Technology Standards.

  â€¢ Benefits to users
    Users gain a single reference document bundling processes and standards, a clear framework of what to do and not do, role breakdowns, and guidance on compliant data usage â€” all intended to keep data safe and platform functioning.


Scope: Areas Covered by the Policy

  â€¢ Data ingestion
    Guidelines for proper collection and import of data into Lumi.

  â€¢ Data security
    Measures to protect data from unauthorized access and breaches.

  â€¢ Data access, privileges and management
    Definitions of who may access data and at what level.

  â€¢ Project space enablement
    Guidelines to set up project spaces efficiently for users.

  â€¢ Data lifecycle management
    Processes governing data from creation to deletion, referenced to EAM-79 and EAM-08 guidelines.

  â€¢ Business intelligence tools
    Use of BI tools to analyze data and generate insights for valid business use.

  â€¢ Data sharing and transfers
    Rules for sharing and transferring data between parties.

  â€¢ Data quality
    Practices to maintain accuracy and integrity of data.

  â€¢ Change management
    Procedures for handling changes to systems or data.


Roles and Responsibilities

  â€¢ Application Owner (EAM-30 referenced)
    Responsible for managing source applications and the flow of data into Lumi; expected to understand both business and technical aspects of the source systems.

  â€¢ Data Ingestion Owner
    Ensures feeds and tables in Lumi comply with the usage policy and guidelines; manages how data enters Lumi and oversees changes to incoming data.

  â€¢ EAM 7E Activated Data Steward
    Accountable for critical data elements (CDEs): defining data meaning, tracking provenance, and ensuring high data quality across the organization.


Next Steps and Resources

  â€¢ Training and resources
    Users are encouraged to rewatch the video, visit Lumi Academy, and attend the "Getting Started with Lumi Usage Policy" session for further learning.


ðŸ“‹Overview

  â€¢ The Lumi usage policy establishes onboarding frameworks, processes, and operating principles for valid business use of the Lumi platform.
  â€¢ The policy covers ingestion, security, access, project enablement, lifecycle, BI tools, sharing/transfers, quality, and change management.
  â€¢ It is a dynamic framework that guides introduction of new functionalities while remaining anchored to core Lumi capabilities.
  â€¢ Compliance with the policy is essential to protect data and maintain platform operations, and aligns with Amex Management Policies and Technology Standards.
  â€¢ Roles (Application Owner, Data Ingestion Owner, EAM 7E Data Steward) were defined with specific responsibilities for source management, ingestion governance, and CDE stewardship.
  â€¢ Users were directed to Lumi Academy and follow-up resources for continued learning.


ðŸŽ¯Todo List

  â€¢ owner: Lumi Governance / Training Team
    
    â€¢ Publish the Lumi usage policy central document and ensure it is accessible (no deadline provided)
    â€¢ Schedule and publish dates for the "Getting Started with Lumi Usage Policy" follow-up session and Lumi Academy resources (no deadline provided)

  â€¢ owner: Application Owners
    
    â€¢ Review source application responsibilities and confirm onboarding plans for data feeds into Lumi (no deadline provided)

  â€¢ owner: Data Ingestion Owners
    
    â€¢ Validate existing feeds and tables against Lumi usage policy and identify any noncompliance items (no deadline provided)

  â€¢ owner: EAM 7E Data Stewards
    
    â€¢ Inventory and document Critical Data Elements, including definitions and provenance, and establish quality checks (no deadline provided)

  â€¢ owner: All Lumi users
    
    â€¢ Complete Lumi usage policy training materials (Lumi Academy / recorded session) and confirm understanding (no deadline provided)

Notes: The meeting transcript did not specify explicit deadlines or individual attendee assignments; action owners above are suggested based on roles referenced in the session. If you want, I can convert these into a task tracker with deadlines and assigneesâ€”please provide desired dates and names.


Transcription

00:00:08 - 00:02:13 Unknown Speaker:

Welcome to the usage policy introduction video. We are excited to walk you through our usage policy, a guide to proper data management on our platform. During this session, you will be guided through an overview of the Lumi usage policy, its scope, and the roles and responsibilities. Let's get into it. Lumi usage policy overview. Let's begin with an integral part of our operations. The Lumi usage policy outlines the frameworks, processes, and operating principles necessary for onboarding onto the Lumi platform for valid. Business purposes. Additionally, our Lumi usage policy perform multiple key functions. It lays out the frameworks and guidelines to onboard the platform for a valid business purpose by the federated teams. It delineates clear roles and responsibilities for data ingestion, security, consumption, and management. It sets forth guidelines for analytical and production project spaces.

00:02:13 - 00:06:09 Unknown Speaker:

It incorporates provisions for user activity monitoring. It defines. Change management protocols. It is important to note that the Lumi usage policy serves as a dynamic framework. While it does not adapt with each new functionality released, it sets forth guidelines on how these new functionalities should be introduced, ensuring it remains relevant to the foundational Lumi platform capabilities. The policy outlines the do's and don'ts for the user community, providing consistent guidance. Let's now explore the reasons why this policy is crucial for our users. At Amex, we really care about keeping data safe. This policy spells out exactly what's expected. So, to make sure we're all on the same page with this big priority. It's super important for you to understand your role, what you're responsible for, and what the stakeholders expect from you.

00:06:10 - 00:09:05 Unknown Speaker:

Keep data safe and sound by using the right access management practices. Use your workspace wisely to do your job. Stick to the rules. Thank you. Meaning the American Express Management Policies and Technology Standards. As a user, you're not just signing up for responsibilities. You're also getting benefits from the Lumi usage policy. Here's what you're getting. One go-to document that bundles up all the processes and standards you need to know. A framework of what to do and what not to do on the Lumi platform. A clear breakdown of who does what, for both you and the folks you're working with. A full set of guidelines on how to use data without. Stepping over any lines. Don't forget, sticking to the Lumi usage policy keeps our data safe and our platform running smoothly.

00:09:06 - 00:12:29 Unknown Speaker:

Let's all play by the rules to hit our business targets. Lumi usage policy scope. Now that you've got the hang of your usage policy duties and perks, let's check out the areas that fall within the user's responsibilities and scope. First off, we have data ingestion. This provides guidelines for the proper collection and import of data into the system. Then, there is data security, which includes measures to protect data from unauthorized access and breaches. Next up, we have data access, privileges, And... and management? In here, it is defined who has access to data and at what level. As for the project space enablement, it ensures that project spaces are set up efficiently for users. Then we move on on to data life cycle management, which manages flow of data throughout its life.

00:12:29 - 00:18:50 Unknown Speaker:

From creation to deletion, according to EAM-79 and EAM-08 guidelines. Oh, when you're fast, you won't pitch that. We also use business intelligence tools. These help us to analyze. As data and gain insights for valid business. Now, data sharing and transfers is another important area. This lays out the rules for sharing and transferring data between parties. Also, there is data quality, which maintains the accuracy and integrity of data. And lastly, change management. Which outlines the procedures for handling changes to the system or data. And that's it for the scope of Lumi usage policy. It covers all these areas to ensure that you are making the most of your data, keeping it secure, and managing effectively. Roles and Responsibilities. Time to uncover the distinct roles and responsibilities that owners have in the Lumi usage policy framework.

00:18:51 - 00:23:03 Unknown Speaker:

As an application owner, you play a key role in managing data sources. Your job, based on the EAM-30 guidelines, is to look after the source applications. Handle the flow of data to Lumi. Maybe they'll come. And have a good grasp of both business and tech aspects. Next up, let's discuss the role of a data ingestion owner. Your job is to make sure the feeds and tables in Lumi follow the usage policy and ideal guidelines. You're also in charge of managing how data comes into Lumi, along with handling any changes to the data. Yeah, that's a lot bigger. Next on our list is the role of the EAM 7E Activated Data Steward. This role is crucial to keeping data in good shape. You're responsible for handling critical data elements throughout our organization. This means defining what the data means.

00:23:18 - 00:26:10 Unknown Speaker:

Thank you. Tracking where it comes from, and making sure the quality of the CDE stays high. Each role is key to using and governing data in our organization. Following the Lumi usage policy helps us use our data responsibly for valid business needs, in line with other AXP AIM policies and tech standards. Before we wrap up, let's take a moment to review what you are now able to understand the importance and benefits of the Lumi usage policy, outline the main aspects of the Lumi usage policy, and identify your role and responsibilities. And there you have it. You're now fully equipped to dive into the Lumi Usage Policy. Feel free to rewatch this video or check out our resources like Lumi Academy. Continue your exploration, and we look forward to your participation in our Getting Started with Lumi Usage Policy video.2026-01-25 ACE Bootcamp Day 1: Fundamentals and Workflow Automation

Creation Time: 2026/1/25


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-25 13:04 (Duration: 336 seconds)
  â€¢ Location: Virtual session (ACE Framework Training â€” Day 1)
  â€¢ Attendee: Partha (host), Jairo Rubio (presenter), COE members: analysts, product owners, population in the COE, and other invited participants


ðŸ“’Meeting Outline


Introduction & Purpose of the Bootcamp

  â€¢ Bootcamp context The COE is migrating from the QOD platform (Blue Prism) to ACE. To support this transition, the COE organized a five-day ACE bootcamp to give analysts, product owners, and COE participants a closer, practical understanding of the ACE platform and how it can be used to deliver automation outcomes.

  â€¢ Bootcamp structure Five sessions are planned. Day 1 focuses on ACE fundamentals. Jairo will lead the fundamentals session; subsequent days will cover deeper or applied topics (not detailed in this meeting).


ACE Fundamentals â€” Overview & Objectives

  â€¢ Presenter introduction Jairo Rubio introduced himself as an engineer on the ACE team within Digital Data Intelligence and Marketing (Amex Technology). He welcomed attendees across time zones and set expectations for the week: to build a solid understanding of ACE.

  â€¢ What is ACE & who should attend Jairo provided context for attendees both unfamiliar and familiar with ACE: an invitation to learn what ACE is, what it can offer business units, and how it enables automation across enterprise workflows.


Business Processes and Workflows â€” Framing ACEâ€™s Value

  â€¢ Commonality of business activities Jairo described that all departments perform recurring business activities (e.g., routine vendor payments, communications, or complex processes like credit card/credit line applications). Despite variety, these activities share the need for clearly defined workflows to ensure completion.

  â€¢ Definition and role of workflows Workflows represent the ordered steps required to complete business processes. Workflows can be simple and linear (fixed step-to-step sequences) or more complex with conditional branches, external dependencies, and human interactions.

  â€¢ ACEâ€™s alignment to business processes ACE is positioned as the platform to model, automate, and manage these workflows so teams can achieve consistent outcomes, reduce manual effort, and support complex scenarios (e.g., leveraging CCP resources, awaiting customer inputs, handling external system interactions).


ðŸ“‹Overview

  â€¢ The COE is migrating from Blue Prism (QOD) to ACE and needs hands-on education to enable stakeholders to adopt ACE effectively.
  â€¢ A five-day bootcamp was established; Day 1 covers ACE fundamentals with Jairo as the presenter.
  â€¢ ACE is intended to model and automate workflows that underpin routine and complex business processes.
  â€¢ Workflows are central to completing business processes; ACE supports both simple linear flows and more complex, conditional processes involving external systems and human interactions.
  â€¢ The session set expectations for building a solid understanding of ACE across the week to support automation outcomes.


ðŸŽ¯Todo List

  â€¢ Partha / COE organizer:
    
    â€¢ Share the full five-day bootcamp schedule and agenda with all attendees, by [assign a date if known â€” otherwise: ASAP].
    â€¢ Provide access instructions (links, environment access, prerequisites) for ACE training instances to participants, by ASAP.

  â€¢ Jairo Rubio / ACE team:
    
    â€¢ Deliver Day 1 ACE fundamentals session (completed on 2026-01-25 13:04).
    â€¢ Prepare and present remaining sessions (Days 2â€“5) covering deeper ACE topics (demos, hands-on labs, migration guidance), schedule and topics to be communicated to attendees.

  â€¢ COE analysts / product owners / participants:
    
    â€¢ Review pre-read materials or environment access details once shared, and confirm readiness to participate in hands-on sessions, by prior to Day 2.
    â€¢ Collate and submit questions, use-cases, or processes they want covered in the bootcamp (especially for migration from Blue Prism to ACE), by [specify date or: prior to Day 2].

(If specific deadlines or owners need adjustment, please update the list with exact dates and responsible persons.)


Transcription

00:00:18 - 00:01:26 Unknown Speaker:

So thanks so much everyone for taking time out for this session. So just to give you a context, as a COE, Since we are moving across from the QOD platform, which is Blue Prism to ACE, it was really required for the analyst, product owner, population in the COE to have closer insight into the ACE platform. With that idea in mind, we thought of conducting a five-day bootcamp, which would pretty much give you a detailed insight into how this platform works. And, you know, how we can utilize it to deliver automation outcomes. So we'll be conducting five sessions. And day one, we are going to conduct a session around the fundamentals of ACE. So with us, we have Jairo who will be taking us through the fundamentals for day one. And with that, Jairo, I'll hand it over to you.

00:01:27 - 00:02:11 Unknown Speaker:

Okay. Thank you. Thank you, Partha. Thank you for the introduction. Hello, everybody. Good morning, good afternoon, and good evening to all. Welcome to ACE Framework Training. My name is Jairo Rubio, and I'm an engineer here on the ACE team within the Digital Data Intelligence and Marketing Organization in Amex's Technology Department. Today, we'll have the first session on ACE Framework. In this session, we'll cover the fundamentals of ACEs and what ACE can offer you and your business unit. Now, for those of you who are not familiar with our platform, you may be asking, what is ACE? Well, first and foremost, I want to thank you for hearing of ACE for the first time, for taking the time to learn about what ACE can do, and more importantly, what it can do for you.

00:02:12 - 00:03:02 Unknown Speaker:

For those who are familiar with ACE, thank you for joining us again, and I hope after this week of training, you'll leave with a solid understanding of the ACE platform. So, before diving into ACE, let's talk about something we as departments all know. We all have business activities that we perform as part of a frequent business process. These business processes can be as simple as maybe making a routine payment to a vendor using an external payment system or setting a routine of communication. Or something more complex such as completing necessary activities for handling an incoming credit card or credit line application, which may involve the leverage of CCP resources that need to then reach out to customers and communicate with them and also await customer inputs, right?

00:03:02 - 00:03:53 Unknown Speaker:

So while these processes require vastly different actions to complete, all business processes share, no matter how big or small, Key aspect that ensures its completion, and that is a workflow. So no matter what business goal we are trying to achieve as an enterprise, we have work that needs to be done. We have a certain way that we want to complete this work and we want to know the outcome of the work that was done. These workflows can represent routine activities with an unchanging flow, meaning we always go from one step to the next step. In these type of workflows, here within ACE, we point these type of business activities, with these type of workflows as business processes. Although routine, these processes can vary greatly in terms of their complexity. Some processes can be simple, such as those...2026-01-25 ACE Platform Overview and Automation Strategies

Creation Time: 2026/1/25


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-25 13:11 (Duration: 3,827 seconds)
  â€¢ Location: [insert location]
  â€¢ Attendee: [insert names]


ðŸ“’Meeting Outline


ACE Platform Overview

  â€¢ What is ACE ACE (Automation and Case Ecosystem / ACE Studio) is a foundational platform for business workflow automation (process management) and semi-predictable business case management (case management) across Amazon. It supports process, case, and work management, business rule execution (RuleAssist), RPA, process intelligence, and an admin portal (ACE Control Center). ACE emphasizes use of open-source libraries so teams avoid specialized proprietary tool training.

  â€¢ ACE Engine (Automation core) ACE Engine is the runtime/controller that takes the designed workflow and automates execution. It drives stepwise process automation, interprets design-time decisions, routes work, and orchestrates tasks (including service, user, and RPA tasks).

  â€¢ Automation modalities
    
    â€¢ Stepwise BPMN-driven process automation.
    â€¢ Business-rule-driven automation via RuleAssist (Drools-backed).
    â€¢ Robotic Process Automation (RPA) using Robot Framework to automate web, mainframe, and desktop interactions.


Process Management

  â€¢ Definition and capabilities Process management covers design, development, execution, automation, and tracking of workflows. ACE supports BPMN (industry standard) with alternate flows, gateways, human tasks, and robot tasks. Processes can be scheduled or triggered.

  â€¢ Sample process discussed Monthly scheduled payment process:
    
    â€¢ Step 1: Get payment details (service task).
    â€¢ Step 2: Process payment via RPA (robot task).
    â€¢ Step 3: Validate payment status (service task).
    â€¢ Gateway: On success -> end; on failure -> manual human task to complete payment. Demonstrated handling of pass/fail outputs, alternate flows, and combining RPA + human tasks.


Case Management

  â€¢ Definition and when to use Case management (CMMN) is for semi-predictable, non-linear, data-driven business activities where the order of tasks depends on inputs/conditions. Cases support stages, sentries (condition watchers), milestones, subprocess/process tasks, and human tasks. ACE supports CMMN and provides ACE Caselight (low-code/no-code) for simple case types and rapid onboarding.

  â€¢ Sample case discussed Credit line application case:
    
    â€¢ On case creation, collect applicant details (process/subprocess task) runs automatically (no sentry on that plan item).
    â€¢ Case data updates trigger sentries that evaluate conditions (approve/reject).
    â€¢ If rejected -> handle rejection stage executes; milestone updates accordingly.
    â€¢ If approved -> enter fulfill-application stage; create-account process task may trigger, update case data, then communicate to customer.
    â€¢ Milestones track completion state of logical goals; not all plan items must execute for case completion.

  â€¢ Clarifications about sentries and subprocesses
    
    â€¢ Sentries act like signal/condition checks evaluated when case data is updated.
    â€¢ Subprocesses are BPMN processes invoked within a case (or user tasks).


Work Management

  â€¢ Definition Work refers to tasks assigned to humans or human-like actors (bots). Work items pause process/case execution until completed or exited. Work management covers assignment, lifecycle tracking, and queuing.

  â€¢ Task lifecycle ACE follows OASIS human task lifecycle with selected states: ready, reserved, in progress, suspended variants, completed, exited. Examples discussed:
    
    â€¢ Ready -> Reserved (assigned) or Suspended Ready.
    â€¢ Reserved -> In Progress (start) or Delegate or Suspended Reserved.
    â€¢ In Progress -> Complete, Suspend/Resume, or Exit.
    â€¢ SLA handling: interrupting boundary events and SLA routes; breached SLA can release assigned user and return task to work basket for reassignment.

  â€¢ Work baskets and worker actions
    
    â€¢ Grouping/queuing of tasks.
    â€¢ Workers update status (in progress, suspend, complete, exit).
    â€¢ Admins can allocate tasks and view/filter by task state.


ACE Studio (Composer) and Tooling

  â€¢ Composer features
    
    â€¢ Visual designer for BPMN and CMMN models.
    â€¢ Save, share, collaborate, publish models.
    â€¢ Define inputs/outputs, work baskets, assignments, RPA script references, scheduler for processes.
    â€¢ Service catalog for reusable service tasks and RPA scripts.
    â€¢ RPA development integrated (Robot Framework used for RPA scripts).

  â€¢ Demo highlights
    
    â€¢ Building the sample payment process: add service task (get payment details), RPA service task (process payment on portal), validation, gateway to end or manual user task.
    â€¢ CMMN modeling: stages, sentries, milestones, process tasks.
    â€¢ Publishing models to assets for reuse and sharing across processes.
    â€¢ Reusable service tasks allow consistent reuse across multiple processes.


Business Rules (RuleAssist)

  â€¢ Purpose Business rules evaluate input data and produce outputs or decisions (e.g., reject applicant under age 18). Useful for validations, eligibility checks, and decision logic.

  â€¢ Implementation
    
    â€¢ RuleAssist is powered by Drools.
    â€¢ Supports Drools native language, decision tables (Excel), and domain-specific languages.
    â€¢ Supports rule usage tracking and auditing; rules can be updated without interrupting running processes/cases.


Process Intelligence

  â€¢ Purpose Capture and analyze event data to discover process patterns, identify bottlenecks, measure throughput times, and surface optimization opportunities.

  â€¢ Capabilities
    
    â€¢ Automatic acquisition of process event data across ACE and integrated systems.
    â€¢ Discovery, modeling of variants, monitoring and optimization recommendations.
    â€¢ Metrics for work distribution, completion times, bottleneck detection.
    â€¢ Helps teams prioritize automation or improvements.


ACE Control Center (Admin Portal)

  â€¢ Purpose Admin portal to manage processes, cases, rules, scheduling, security, and bulk processing.

  â€¢ Key sections
    
    â€¢ Process and case instance search and status dashboards (filter by dates, status, process ID, correlation key).
    â€¢ Work management: manage work baskets, allocate tasks, filter tasks by lifecycle state.
    â€¢ RuleAssist: view deployed business rules, usage metrics, and audit logs.
    â€¢ Scheduling: configure and review scheduled processes.
    â€¢ Bulk processing: define bulk policies to run processes for bulk datasets.
    â€¢ Security: manage access controls for triggering processes, creating cases, and picking up work.
    â€¢ Process Intelligence integration (insights dashboards) â€” deeper dive to be covered in later sessions.


Q&A Highlights

  â€¢ Difference between process and case:
    â€¢ Processes are directed, predictable flows (BPMN).
    â€¢ Cases are data-driven, non-linear, and not all plan items must execute (CMMN).
  â€¢ Sentries behavior:
    â€¢ Sentries evaluate on case data updates (signal-like).
  â€¢ Subprocesses in cases:
    â€¢ Can invoke BPMN subprocesses or user/human tasks.
  â€¢ RPA scripting language:
    â€¢ Robot Framework is used.
  â€¢ SLA/time boundaries for human tasks:
    â€¢ Use interrupting boundary events and SLA routes; on breach, task can be released back to basket for reassignment.


ðŸ“‹Overview

  â€¢ ACE is a unified platform for process automation, case management, RPA, business rules, work management, process intelligence, and admin operations.
  â€¢ ACE Engine automates BPMN-defined processes and orchestrates tasks including RPA.
  â€¢ Case management (CMMN) handles non-linear, data-driven business cases; Caselight provides low-code/no-code for simple cases.
  â€¢ RuleAssist (Drools) enables business-rule driven automation and decisioning; rules are auditable and updatable without downtime.
  â€¢ Work management supports OASIS-based task lifecycle, work baskets, SLAs, and task assignment/delegation.
  â€¢ ACE Studio Composer provides an accessible UI to design BPMN/CMMN, build RPA scripts, and publish reusable service tasks.
  â€¢ ACE Control Center centralizes admin tasks: process/case monitoring, work allocation, scheduling, rule management, bulk processing, security, and process intelligence insights.
  â€¢ Process Intelligence captures events to reveal bottlenecks and optimization opportunities.


ðŸŽ¯Todo List

  â€¢ [owner: Training Organizer / Presenter]
    â€¢ Create Slack channel for ACE training follow-up and questions (ASAP)
    â€¢ Share session artifacts (slides, sample BPMN/CMMN, RPA examples) to team (by next meeting)
    â€¢ Schedule Day 4 deep-dive for ACE Control Center and Process Intelligence (date/time TBD)
  â€¢ [owner: ACE Engineering / RPA Team]
    â€¢ Provide example Robot Framework RPA scripts and documentation for reference (within 1 week)
    â€¢ Publish sample reusable service tasks and catalog entries to Composer assets (within 1 week)
  â€¢ [owner: Business Rules Lead]
    â€¢ Share Drools rule examples and decision-table templates used in RuleAssist (within 1 week)
  â€¢ [owner: Security/Admin]
    â€¢ Prepare a short guide on Control Center security roles and access management (by next session)
  â€¢ [owner: Process Owners / SMEs]
    â€¢ Identify 1â€“2 candidate processes or cases for pilot automation or case onboarding (by next week)
    â€¢ Prepare sample datasets/events for Process Intelligence ingestion (by next session)

If you want, I can fill in attendees and location, convert the action items to calendar invites, or create the Slack channel and collect required artifacts.


Transcription

00:01:20 - 00:02:24 Unknown Speaker:

This data that's received. Some can also be dependent on human tasks, such as a CCP needing to complete some operational work in order for this process to move towards completion. Some of these processes could also have sub-processes that complete a smaller unit of work within the bigger main process. No matter how complex the designing, execution, and monitoring of these processes is known as process management. Other times, business activities aren't really able to be represented or completed in such a cookie-cutter, stepwise workflow fashion. While we may know at a high level what activities need to be done and what the end goal is, these activities can be data-driven, where the order of these activities can be dependent on. Let's say custom or enterprise inputs or communications, and we may have different steps to complete in order to complete this larger business activity.

00:02:24 - 00:03:28 Unknown Speaker:

Within ACE, we coined these semi-predictable workflows as business cases. Cases can also vary widely in complexity, with some cases even leveraging business processes, no matter how complex as well. The designing, execution, and monitoring of these cases is known as case management. So now back to the main question. What is ACE? Well, ACE, Automation and Case Ecosystem, better known as ACE Studio, is a foundational platform that provides the capability of business workflow automation and business case management across Amazon. So now let's take a quick glance into the features ACE provides today. So the first main feature of ACE is its automation capability. We'll talk about how ACE provides automation and what that can mean for your workflows. As we briefly discuss next, we'll talk about ACE's capability of process, case, and work management, and we'll go deeper into what these mean.

00:03:30 - 00:04:20 Unknown Speaker:

Then we'll talk about ACE Studio, ACE's provided user interface for the design of business processes and cases. This more serves as a developer toolkit for developing and designing these processes and cases. Next, we'll talk about ACE's capability to manage business rules and use them in both your automations as well as a standalone tool using RuleAssist. Then we'll briefly talk about process intelligence, which can provide key insights on how your process is performing within ACE. And lastly, we'll take a... quick look at the ACE Control Center, which is ACE's provided admin portal for the management and administration of all your business processes, cases, and any business rules you may be having. And so one thing I want to highlight is in all our applications, we align to use open source libraries.

00:04:21 - 00:05:16 Unknown Speaker:

So we, as an enterprise, don't need to spend our time or resources on specialized training to be able to use ACE. So we'll go into what we do use as we move forward. Let's go. So now we'll talk about automation. What is automation? So we all have a pretty, I'd say, general understanding of what automation is. So automation is the act of making something run automatically on its own. Now, how does ACE provide that automation? In terms of workflows, automation is what moves the workflow step by step from start to completion. ACE's automation is powered by ACE's flexible and powerful ACE Engine. ACE Engine is what you could call the core of all ACE has to offer. Through ACE Engine, our platform takes in your design workflow and automates it to execute as designed.

00:05:16 - 00:06:18 Unknown Speaker:

From meeting your design workflow to then knowing what actions to take when at a particular step in the flow, ACE Engine is the master controller of this process manager. Aside from stepwise workflow automation, ACE also provides automation in a few other ways. So another way ACE provides automation, which I've touched on briefly before, is by automating actions that would be taken when validating data against existing business rules through a feature called Rule Assist. So just to briefly touch on business rules, Business rules are essentially rules that the business has, that... represent a scenario, to excuse me, that represent that. Based on input, we want to proceed with a certain action so that business rules are defined for the business. We can leverage these using rule assist.

00:06:18 - 00:07:05 Unknown Speaker:

So one quick example could be the rejection of a credit card application, let's say the applicant age is not at least 18. With rule assist, this could be a condition as part of a bigger business rule, and when we see that condition being satisfied, we know to reject. So we'll take a deeper look into rule assist as we proceed with the training. Another way AI provides automation is through what's known as robotic process automation, or RPA. So what's RPA? RPA is the act of a bot completing actions that a human would typically do to complete tasks. An example of this could be... Going back to the payment portal example, an example of this could be going on a payment portal and making a payment, which instead of being done by a CCP, could be done by a bot.

00:07:06 - 00:08:11 Unknown Speaker:

With RPA and ACE, you could also automate business processes requiring any sort of web browser activity, accessing many different systems, as well as automating desktop application activities as well. Now we'll move forward to what is process management. So as we touched on earlier, process management is all the activities performed in designing, executing, and tracking your workflows. As we've learned, Ace provides this capability through Ace Studio Engine. Automation of a workflow is what process management is. This design of the workflow, the development of the workflow components, the execution and the automation of this workflow. And then tracking its results is what defines process management. So processes can also be designed to have alternate flows. In case of any signals or failures in the workflow, we want to be able to account for these things.

00:08:12 - 00:09:26 Unknown Speaker:

Workflows can also have tasks to be computed, to be completed by humans. Or human-like entities, like let's say another bot. ACE supports the automation of workflows by using industry standard business process model notation. So if a process is defined in this notation, we support it here at ACE. So let's now take a look at an example of a that could be used here at ACE. So here's a sample process use case. So in this scenario, we have a process that is scheduled to run once a month and complete some actions. Looking further into the design, we can see each of our defined steps along the workflow. So as you guys can tell, this workflow defines actions to be taken in order to complete a scheduled monthly payment. So once a month, this process has started.

00:09:27 - 00:10:22 Unknown Speaker:

The first step that we do is we take, is we are getting the payment details needed to complete this payment. So that is defined here in this next step, this service task, get payment details. Once we've collected our necessary details, we can go back to this. So this step is called process payment report. If you see with this little icon, this means a robot will be taking care of this task. So this step is leveraging an RPA task. As we discussed earlier, RPA tasks are used to leverage a bot to complete some actions on either a web page, a mainframe screen, or some desktop application. In this case, we are going to have the bot go to the payment website and try to complete the payment. I say try.

00:10:23 - 00:11:18 Unknown Speaker:

Because as we all know, sometimes things just don't work as expected. The website could be down, the payment part could not be responding properly, or just something could happen along the way where we don't get this payment completed. The bot is not able to complete this payment. So once the bot is done trying to complete its job, it'll produce some output, whether pass or fail. In this next task, we are going to... validate this payment status. So we're processing the output that they gave us in the previous task. So whether it may be pass or fail. Based on pass or fail, let's see that there are two things that can happen. Here, this is called a gateway. Once we finish this task, we go to this gateway, which then will check for a condition for us.

00:11:19 - 00:12:29 Unknown Speaker:

Based on the condition that we want, whether the payment was succeeded or failed, we want to either end this process or take another step, which in this case, if in case of failure, we want an actual human to go and manually do the payment as a fail-safe option. So this is a pretty, excuse me, well, this is a simple condition. In a real scenario, we may even be, let's say, extracting some other data from the payment task, right? And then performing another flow, another step based on that other data that we extracted. So, again, this is a pretty simple sample process use case, but it does show how we could have alternate flows. We could also have... human tasks in our workflow, as well as robot tasks in the workflow, and what sort of conditions we can check for when designing our workflows.

00:12:33 - 00:14:01 Unknown Speaker:

Okay, so as we wrap up process management, we'll move on to case management. So what is a case, and what is case management? So a business case... as opposed to a business process that we were discussing just now, is designed for situations where business activities are not necessarily able to be streamlined into a workflow. So case management provides problem resolution for these non-repeatable industry standard business. And these are not necessarily able to be streamlined into workflow. So case management provides problem resolution for these non-repeatable, unpredictable activities as opposed to the efficiency-oriented approach of process management for routine, predictable processes. Age supports business cases defined in the industry standard case management model notation. More complex cases, we also provide a feature called ACE Caselight, which is a low-code slash no-code solution that is used primarily for simple case types to rapidly onboard those simple case types.

00:14:01 - 00:15:07 Unknown Speaker:

So there are a few scenarios in which maybe we could use Caselight. Caselight is designed to handle. These are a few common scenarios, so let's go over. Just to real quick. So let's say scenario one. We have a servicing request that is designed to require human intervention. This request is simple. We just want to collect data from the customer, assign a task to a CCP to then follow some operational procedure. And then once they're done, they will go ahead and close this case. The assigned tasks need to follow some proper lifecycle. And communication may need to be sent to the customer, and the customer may need to provide some inputs as well. So for a simple case like this, Caselight is perfect for these manual sort of business activities, as well as the Caselight is also really good for, as I touched on, rapid onboarding.

00:15:08 - 00:16:10 Unknown Speaker:

Reason being is, so let's say a new product is launching in American Express. We're optimistic that the product will succeed, but we also want to limit our investment into this product just in case that we need to fail it fast, right? So we may be wanting to make a decision to handle more complicated servicing requests for this product in the early days manually. As I said, as I touched on before, manual handling is a great use case for case slide. So now let's take a look into a sample of a case management use case. So here we're looking at what's called a case model in the case management model notation. In this case model we can see that this defines the business case for approving or rejecting a new credit line application.

00:16:12 - 00:17:25 Unknown Speaker:

Based on this model, when we choose to start a case, one process task will kick off automatically. Based on this model, when we choose to start this case, one process task will kick off automatically. So that is the collect applicant details task. So now, how do I know this task will automatically start during the creation of this case? So if you look closely, there are two tasks in this model that don't have what case management model notation calls as a century. That's these little diamonds right here. Centuries. Are just conditions that are watchful that, once met, will then activate this. This um plan item task that we have defined here. So since, um, since, there was no century here when we started. When we create this case, this case will know to automatically take off this this task and collect the applicant details.

00:17:25 - 00:18:27 Unknown Speaker:

Now, if that's the case, why didn't create account Start automatically as well. The reason why is because the object that this task is within, we'll see it's within this bigger, oops, excuse me. It's within this bigger figure. This is known as a stage. So we see that this stage has a century. So stages are used for logically grouping certain tasks together. So that means we want these tasks within this stage to, kind of perform, either execute or perform at around the same time. So, yeah, if you guys have any questions on these shapes, just, they, these are all CMMN defined shapes. So if, as we're going through this, if maybe it's a bit confusing or you forget what the shapes mean, it's very well documented online.

00:18:27 - 00:19:58 Unknown Speaker:

You can always... reference a few other sample cases as well as learn the definitions of all the symbols. Just to pause and check with the audience if they have any questions so far on what we have covered so far. Anybody have any questions so far? Right, is it? Yeah. I have a question, though. Yeah, sorry, someone in the audience has a question. So the difference between a process and a case. So one, aside from the... modeling differences, right? So the difference between a process and a case is that a case, while we know high level what tasks we have, we don't always know the order of these tasks, right? So let's say we collect this applicant details, right? And I'll cover it as we go, this exact example, but a process is more for static workflows.

00:19:59 - 00:20:51 Unknown Speaker:

We know the flow, we know each and every step, and we know what order we're going to go along the way. For a case, this is all dependent on any inputs that this case is getting when being created. Or as this case is created, when this case is updated, then certain tasks will move forward versus in a process, no matter what, we're going step by step. Does that kind of answer your question? So let me just see if I can add a little bit of color here. So when we have a process, so what happens with a process is it's a directed flow. That means first something happens, and the next thing happens, and the next thing happens, and you can kind of follow along through the path that it's designed.

00:20:51 - 00:21:44 Unknown Speaker:

With cases, if you look here, you can see you can have multiple things that are happening in parallel. Anytime these sentries, these diamonds, whatever those conditions evaluate to true, the underlying task will execute. And these tasks include process tasks, which are also then directed, right? And it also includes stages, as you can see. It also includes human tasks, which I don't think are shown on this diagram, but those are there too. So what happens is, and then you don't have to actually execute all the things in a case to complete a case. So you may have things here in your diagram that never happen or only happen on occasion. That's okay. With cases, with processes, it doesn't really work that way. So cases generally are made up of sub-processes and human tasks for the most part, but they don't all have to execute as we do.

00:21:45 - 00:22:47 Unknown Speaker:

I don't know. Hopefully that adds some clarity. Yes. Yeah. Yeah, and going off the point that they said they... they won't all have to execute. Well, as we're going through this sample case, I'll cover that here in a second. So, yeah, back to the sample case. So, first, the applicant details are collected in this process task. This process task, right, this can be a sub-process. So, that's more of that directional flow that we were talking about. We may have a... a defined standard way of how we want to collect this applicant details, right? So this is a subprocess task. This could be a process within a case, as Mitch was saying. But yeah, so in this task itself, we are processing the applicant details, and then we are updating our case data once we complete that.

00:22:47 - 00:23:42 Unknown Speaker:

So upon the update of this case data, right? ACE engine case manager will see if any of these sentries have been fired. So let's say our sentries for handle rejection and fulfill application, which is this stage, are looking for data that says whether or not we want to approve this application. So we can be like, there can be something in the case data that says, approve, and then we can make sure if we see approve is false, we may want to go to rejection. If we see approve is true, then we may go into fulfill application. We want to go into fulfill application. So in the event, let's say the applicant is too young, or maybe the credit score is too low, right? Our collect applicant details task is updating the case data with a rejection.

00:23:42 - 00:24:37 Unknown Speaker:

In that event, our handle rejection task will then kick off and do its necessary actions. Right, oh, sorry, um, we'll do its necessary actions. Um, so, in that case, right, we'll notice that, hey, we didn't do any of these activities here, but that's okay. Um, because we. We completed the goal of this case, which is to, um, take it in, uh, take in and have, uh, sorry, take it in. Application, then approve or reject the application, and then close the case. So that's okay. But in the event that the application is good, then we'll go into this fulfill application stage. Once we get into this stage, we'll see that this create account process task will kick off. And we don't know if this communicates customer process task will kick off. In this case, let's say no.

00:24:37 - 00:25:39 Unknown Speaker:

Let's say this sentry here is waiting for this account task, create account task to complete. And maybe update the case data with account created. So, um, excuse me, so the sorry, um so. Then once we're in the stage, uh, this, uh, create account task goes on and it creates the account. Once that's done, it'll update the case data. Which then, as we, as, as we just defined this century, will be waiting for this account to be created, so then. Once this is done, the case manager will know to execute this task and communicate to the customer their account details and things of that nature. So while these are the tasks, you may be wondering what these figures here on the side are. So these are what we call in case management model notation as milestones.

00:25:41 - 00:26:35 Unknown Speaker:

So milestones are more so for, um, more more so for, like, tracking of the of completion of steps in your case, right? So, uh, maybe so. Once we first collect the applicant details, we want to say, hey, we know that the application has been accepted. Let's say we handle a rejection well, we will know to either. This milestone will will be updated to complete because we have then rejected the application. And then in the event that we proceed to fulfill the application, right? While we're running create account, we'll know that we won't see this mark as complete. Not until everything here is complete will we then mark it as approved. And then we can then also mark it as complete. So these are centuries just the same as these others.

00:26:35 - 00:27:47 Unknown Speaker:

So we could update the case data and look for some, value in that data in order to satisfy these milestones. So, yeah, that's an example of this case. Does anybody have any questions before we move forward? Yeah, I just wanted to ask, should we think of these centuries like RTF events, or is it like it's checking for these conditions at any time? Yeah. Not necessarily like RTF events. They are more so for when the case is updated. That's when the sentries will check the condition again. So we provide basically endpoints where users can then update their case information, right? And then upon that update of this case information, The case manager will check to see if any... centuries have been satisfied, right? So it's like a signal event, like, yeah, it's a signal, yeah, okay, okay.

00:27:47 - 00:28:55 Unknown Speaker:

And the the subprocesses, we can kind of think as the BPMNs, right? It's like separate BPMNs that would get triggered. Um, yes, yes, correct. Sometimes they can be, um, human tasks, though, so I'm in this model. I don't have a user task here, but these are all process tasks. But it can be either a BPMN or it can be just an actual user task that somebody needs to complete. Sure. Okay. And so now let's talk about work management. So in the context of ACE, work refers to tasks that get assigned to humans or human-like actors, so like bots, to complete, right? We've seen that workflows and so business processes and business cases can have tasks that need to be completed by these humans or human-like actors. So these work items halt the process lifecycle until the task is completed or otherwise exited.

00:28:55 - 00:30:05 Unknown Speaker:

So if these are required, then they'll need to be completed. So work management is the tracking and delegation of these human tasks. So how does ACE provide that for you? So ACE provides work baskets for the grouping and queuing of tasks for people in that work group to complete. ACE allows for task workers to then, once they are assigned some work, meaningfully update the status of their assigned tasks, whether it's in progress, they need to suspend it, or... they finished it, it's completed now, or they need to, for whatever reason, cancel this task. It should not be completed. These are all things that task workers can update the task status with. So this is what we call as a task lifecycle. So just to give you a quick glance, Work in ACE follows the OASIS human task lifecycle, which is pictured here above.

00:30:06 - 00:30:59 Unknown Speaker:

There is a bit of a modification. We don't use this created, failed, error, or obsolete states, but we do use ready, reserved, in progress, completed, exited. As we're discussing, when a task is ready, that means it's in the work basket and it's ready to be picked up. So from there, a task can move either to a reserved state, which means it's assigned to somebody, or it can move to a suspended ready state. So that means maybe we want to hold off on doing this task. It's suspended, but hey, it's ready for whenever we need to pick it up. Nobody has assigned to them. Once you go, if we go to reserve, we can also do the same thing. We can suspend a task, but it'll be in suspend reserve state.

00:31:00 - 00:31:45 Unknown Speaker:

So we know that, hey, this task is suspended, but somebody was working on it. And yeah, we can resume it and it'll still be assigned to that same person that was working on it. So from reserve, we can either go ahead and start the work or we can maybe, maybe we need to assign it to somebody else to complete. So that's we can reserve it to, it'll stay in reserve state, but delegate it to somebody else. Or we can start this, start the UM task, and then it'll be in progress. So from in progress, we could also suspend and resume just the same as which we did for reserved, sorry, as we did for reserved and ready. Or we can go ahead and complete the the task, or we could go ahead and complete the task and then mark it as complete.

00:31:45 - 00:32:43 Unknown Speaker:

Or if, in case of anything, we want to go ahead and exit this task. From any point of the life cycle, so either ready, reserved, or in progress, we can exit the task and not go forward with its completion. This is the only event that we need to cancel this task for whatever reason. Maybe it's not required anymore or whatever the case may be. We don't want to proceed with processing that task. We can go ahead and exit it. But otherwise, all tasks will be in either completed or exited state at the end of their life cycle. Any questions on this lifecycle here? No? So I just have one question. So when we set a human task, do we set a time for that? Say, suppose, like an approval that needs to come through from operations.

00:32:44 - 00:33:37 Unknown Speaker:

So suppose there's a time of, say, one day. Do we set such kind of time boundaries to... for that human task? Or, yes, yes, so, um, when we are defining our processes, we can define, um, interrupting, uh, boundary events. So these can be for use for, let's say SLA. So, like you said, if we wanted to, um, if, if we want this task to complete within a certain time. But let's say the the human worker did not finish it in time. Um, there will be an Sla uh route in your, sorry. There'll be a an alternate flow for when this SLA breach happens for your process to proceed forward in maybe a different way. Or either, in the case of human tasks, there's an SLA when that's breached, it'll move, if it's in, if it's,

00:33:37 - 00:34:33 Unknown Speaker:

I'm sorry, if it's assigned to somebody, we'll release that task from them and put it back into the work, back to the work basket. In ready state for somebody else to be able to pick it up and and resume the work on it. I see. All right. Thank you. So now we'll talk about Ace Studio. Now you may be asking, how would I design my business processes or business cases to be used in Ace Platform? Well, that's where Ace Studio comes in. So Ace Studio Composer is our in-house user interface that allows for the creation, publication, And optimization of your business process models and case management models. So, from Composer, we can save, share and collaborate on these BPMs and CMMs. So you know if you want to design with your technical team as well.

00:34:33 - 00:35:41 Unknown Speaker:

Based on their inputs. What we want to, how we want to complete a task, or how you want to move forward, all these can be done collaboratively here on Composer. So in Composer, ACE also provides units with the ability to configure a scheduler for defining when their business processes are to run. ACE also provides a web mainframe and desktop automation tool for developing RPA scripts, which, sorry, RPA scripts for use within your business processes. And then also ACE provides a service catalog for managing and maintaining common business tasks, including RPA developed scripts. So we'll quickly take a look at all these features here on our Ace Studio itself. If we have any questions along the way, we can do that. So this is Ace Studio Composer.

00:35:41 - 00:36:43 Unknown Speaker:

This is where... you can actually go ahead and design your BPMNs and CMNs as well as your creation of the RPA scripts. So we'll just quickly go and demo creating a new BPMN. Let's continue with the same example that we had in that scenario. So we have a start task here. From here, Let's say we want to just go ahead and add a normal. We want to go ahead and add an appended task to this start. Right. So we can easily go ahead and click this task icon, which will go ahead and automatically put an arrow that will connect the start to this task. So here we can go ahead and. Define our task So let's say this is our get payment details task that we saw in our example.

00:36:43 - 00:37:53 Unknown Speaker:

Um, and we'll know that, hey, from the start, our first step is this get payment details, define it as a service task. And then in our example, we also leverage an RPA script, so in this example, we're leveraging the RPA to go ahead. And um, go into the payment portal and process the payment, so I'll give it a name, process payment on portal. So we that that, excuse me, sorry. Now we have to find the RPA task here and again, we want to go ahead and go to the next task. Or we want to validate the payment status. Okay. And so based on this, since we have a decision to make after we see this status, we would use what's called a gateway. So from this gateway, I'm going to go ahead and append an end event because let's say payment goes good.

00:37:53 - 00:39:17 Unknown Speaker:

We want to just go ahead and end. But I also want to go ahead and add a new task, which is of type user. Okay. And then this is. For a manual payment. Okay, and then from this gateway, we can define our condition. So let's say payment failed equals. And then for this flow, we want to do this false. So that's how we would define the alternate flow for this workflow. In the event of a green flow, which means everything is good, we'll go ahead and end. In the event of a bad flow, we want to go ahead and do this alternate route where we do this manual payment. So this is just a quick demo on how you would create a sample. Your process is here. Cases are similar in the same way, and we can take a quick look.

00:39:17 - 00:40:39 Unknown Speaker:

But fairly user-friendly to be able to use the SACE Composer. All the shapes are here and ready for your use, and it's easy to go ahead and define. Uh, your your tasks as needed, with what inputs they may need and the outputs you're expecting from the task. Um, here's where you would define, uh, maybe like the work basket or somebody that, um, you want to assign to this task for you. In the case of user tasks, in the case of service tasks, in the case of user tasks, in the case of service tasks, which of type is tasks, including RPA developed scripts. So we'll quickly take a look at all these features here on our ACE Studio itself. If we have any questions along the way, we can do that. So this is a studio composer.

00:40:39 - 00:43:23 Unknown Speaker:

This is where you can actually go ahead and design. As well as your creation of the RPA scripts, right? So we'll just quickly go and demo creating a new BPMN. Let's continue with the same example that we had in that scenario, right? So we have a start task here, right? From here, let's say we want to just go ahead and add a normal... Let me do it here. We want to go ahead and add an appended task to this start, right? So we can easily go ahead and click this task icon, which will go ahead and automatically put an arrow that will connect the start to this task. So here we can go ahead and define our task. So let's say this is our get payment details task. We saw in our example.

00:43:25 - 00:44:33 Unknown Speaker:

And we'll know that, hey, from the start, our first step is this get payment details. Define it as a service task. And then in our example, we also leverage an RPA script. So in this example, we're leveraging the RPA to go ahead and go into the payment portal and process the payment. So I'll give it a name, process payment on portal. That, excuse me, sorry. Now we have defined the RPA task here. And again, we want to go ahead and go to the next task. We want to validate the payment status. Okay. And so based on this, since we have a decision to make after we see this status, We would use what's called a gateway, so from this gateway, I'm going to go ahead and append an end event. Because let's say payment goes good.

00:44:33 - 00:45:56 Unknown Speaker:

We want to just go ahead and end. But I also want to go ahead and add a new task which is of type user, Okay, and then this is for a manual. Okay, and then from this gateway, we can define our condition. So let's say payment failed equals true. And then for this flow, we want to do this false. So that's how we would define the alternate flow for this workflow. In the event of a green flow, which means everything is good, we'll go ahead and end. In the event of a bad flow, we want to go ahead and do this alternate route where we do this manual payment. So this is just a quick demo on how you would create a sample. Your process is here. Cases are similar in the same way, and we can take a quick look.

00:45:57 - 00:46:58 Unknown Speaker:

But fairly user-friendly to be able to use the SACE Composer. All the shapes are here and ready for your use, and it's easy to go ahead and define your tasks as needed with what inputs they may need and the outputs you're expecting from the task. Here is where you would define maybe like the word basket or somebody that you want to assign to this task. In the case of user tasks, in the case of service tasks, it's really just the input and the output parameters. And same thing with our PA task. Here is where we also include what script we're using and things like that nature. We just are defining all the inputs that we need for this process to be able to run. Okay. And likewise with CMMN, we are here.

00:46:59 - 00:48:16 Unknown Speaker:

We can use the shapes that I was discussing earlier. So here would be an example. For this process to be able to run. Okay. And likewise with CMN, we are here. We can use the shapes that I was discussing earlier. So here would be an example of a stage with a century right here. The stage, let's say I want it to be fulfill application as we did before. In the example, here is the collect applicant details. To drag this and then you can easily drag for as much space as you may need. Um, if you have a big case where you have a lot of tasks, you may need more space. So, yeah, the The Composer is really user friendly with being able to define your task easily and quickly.

00:48:16 - 00:49:26 Unknown Speaker:

And then, as we said, um, so you are able to download the the definitions to then share with your team. Or, um, uh, publish to, um, our assets, where it will then be available for use for your business or, sorry, for your business process or business case. And just because we're running toward the end of time, I won't go too much further in, but just to show you the catalog. So here's where we have reusable, you can create reusable definitions, basically, for tasks in your workflow. So they can be either RPA tasks or just service tasks in general, where, let's say, if you were to create a service task in one process, one of your processes can then leverage that same service task to easily reference the step that you're doing if the logic is all the same.

00:49:27 - 00:50:29 Unknown Speaker:

If the actions being taken is the same, you can, across multiple different processes, you can leverage creating a service task for that step in order to easily reuse that same task without having to define it and create it for every single process flow. Okay. And now we'll go back to presentation. Okay. So ACE also allows you to use business rules with business processes and cases, right? So we touched on rule assist earlier, but basically, yeah, business rules are a simple but effective way to process data and get an output based on that provided data. So this can be, like I was saying in the example earlier, if we want to do a simple data validation where, hey, we don't want to proceed with this.

00:50:31 - 00:51:35 Unknown Speaker:

Applicants application because one, they're maybe not of age or two, their credit history or score may not be up to par with what we want. So regardless of what the condition may be, business rules are a simple way to just go ahead and process data and provide that output based on that data. So ACE supports the following industry standard rule definition types. So rule assist is actually powered by what's called Drools. Drools is an engine that helps you manage these business rules. So we support, as it's running through with Drools, we support Drools native language. With Drools, we also have the capability to support decision tables, which are basically Excel files that have the conditions for this information. Sorry. The conditions for the defined business rules. Or we may be able to support domain specific language.

00:51:35 - 00:52:50 Unknown Speaker:

Where, let's say you're using business rules that are checking against. You know specific terminology that's used in a in a specific application that you're getting this data from, so. With rules, the usage data of all these loaded rules are provided. So as well as like in a way, an audit trail on when these rules were used and what happened when these rules were executed. So this data can be extremely useful for business units to see what business rules are actually useful or in use at that point. In order to optimize their resources. As business requirements may change, so may business rules. And so with Rule Assist, ACE makes it easy to update these rules without any interruption to your currently running processes and cases. Okay. And we'll move on to process intelligence.

00:52:52 - 00:55:10 Unknown Speaker:

So that's a pretty long one. And we'll move on to process intelligence. Okay. So today's information systems, right? So applications and just business processes and business cases in general all produce an unprecedented amount of data from both digital and physical sources. So what that means is every time something happens within ACE or just within our enterprise in general, most of the time, events, are emitted with a timestamp and event details. When we properly ingest this event data, this wealth of data can be used to discover patterns and insights that illuminate paths to better customer experiences and also to be more efficient as an enterprise. So process intelligence is the automatic and continuous acquisition of process data. Across any system in our enterprise with a goal to provide clear and accurate visibility into the state of these processes and also identify any potential for improvement of these processes, right?

00:55:10 - 00:56:18 Unknown Speaker:

So it helps us improve our operational efficiency. Process intelligence, with process intelligence, ACE provides business units with accurate information about what work items they have, who is completing the work items, how long it takes for this work to be completed. And also identifying maybe any bottlenecks that you may be facing along the way of your business process. So, um, essentially, this graphic is to, kind of, um, highlight, uh, how we what we do with process intelligence. So first, we go ahead and collect these events, this data from source systems such as A-Studio, any event logs that you may be publishing, as well as any events coming from legacy systems that we support. So then with discovery, we then model these processes and kind of identify any variation there. Then we try to optimize these things.

00:56:18 - 00:57:15 Unknown Speaker:

So see what tasks maybe we can automate. See what tasks maybe we can improve on. And then we monitor key metrics such as maybe throughput time and, again, these bottleneck detections. We want to see where the things are taking the most time, when things are getting done, how fast things are getting done, and all these things. So with process intelligence, ACE provides these type of insights for you in order for you to see where maybe you can improve with your process. Okay. And lastly, we're at ACE Control Center. So ACE Control Center is the admin portal where you'll conduct any management actions related to your processes and cases. Here, you can also manage your work baskets and tasks, such as allocating tasks to workers, seeing the status of their tasks,

00:57:16 - 00:58:09 Unknown Speaker:

seeing the status of tasks that are part of a specific process or case, or getting task data for tasks based on what status they are. So maybe I want to see what all tasks I have that are in progress or what all tasks that are suspended in progress or suspended, reserved, or whatever the case may be. I want to see this task data. With Work Management, we allow you to filter and get the data you need easily. Rule Assist for managing and viewing your deployed business rules. So as we touched on earlier, here's where you can see your business rule usage, data, and actions taken. So scheduling, here we have a page where you can manage your scheduled tasks as well as see what is coming up and what has been completed. Security.

00:58:09 - 00:59:08 Unknown Speaker:

So here you can manage your security accesses for your service. So this could mean whether it be who is able to pick up work from your work basket. This could be who is actually able to trigger one of your processes to run, trigger one of your cases to be created. All these things, who has access, can be used here for security. Then through Ace Control Center, we also have the capability of bulk processing. So bulk processing is a way for you to, say, take a bulk set of data and execute a process for each entry of this data. So bulk processing here in Ace Control Center is how you would define your bulk processing policy. So what you want to do. What process you want to run, any outputs you're expecting from this.

00:59:08 - 01:00:14 Unknown Speaker:

All this can be configured here in the bulk processing session of ACE Control Center. And then lastly, we have Process Intelligence, which, as we just touched on, can provide these insights on your process. We'll take a quick look at ACE Control Center in a further, in one of our later sessions, we'll actually take a deeper dive into it. But just so you guys can kind of familiarize yourself with what it looks like. Here's our ACE Control Center home. So from here, we have different sections for dependent, depending on what type of actions you want to complete. So let's say I want to look up, sorry about that. Let's say I want to look up on some processes that I may have ran, right? I can go ahead and select which one of my processes I want to see information for.

01:00:16 - 01:01:14 Unknown Speaker:

What, between what dates this process may have ran, and what status I'm expecting? And then I can get the data. So let me just go ahead and go and see if I can get some sample for this process. Okay, I don't have any. I don't think I have any examples here. But essentially that this is where you could get that process data. See if that process was completed, or maybe it failed along the way, and we could see where it failed, or we could, we could see all these things. We could also search by its process ID and or a correlation key that we have given for this process. So, um, this is where we we see that UM case management similar, similar functionality. We want to see the UM status of these cases and where they may be at along in their workflow.

01:01:17 - 01:02:18 Unknown Speaker:

We can do that here. Work management here is where we can define our work baskets and all these other things. So as we're running short on time, I'm going to go ahead and stop here with going through this. One of our sessions, I believe, day four is going to be going more into depth and using ACE Control Center. So, that wraps up day one of the ACE framework training. Do we have any questions? Hi, Jairo Manuel here. I had a question about what ACE, how the studio was built, or what language was it used? Like, for example, when a script is run on what programming language? Is it typed? Okay? So yeah, when a script is run? So for these RPA tasks, we use Robot framework.

01:02:18 - 01:03:23 Unknown Speaker:

Robot Framework is the um, yeah, the language that we use for RPA, that we support for RPA. Okay, thank you, I will look into it. Okay, and uh. Yeah, I'll also set up a Slack channel through which if you all have any questions, we can take them offline. We can go through them and try to answer to the best of our abilities. And yeah, we can continue for the next one minute if we have any other questions from anyone in the audience. So we have a pretty extensive topic list. So today was intro. So if it feels like you didn't get enough information about various subjects, that's okay because we're just trying to wet your whistle and then over the next several days we'll get into more detail on all the subjects you saw today.

01:03:25 - 01:03:35 Unknown Speaker:

Brilliant. Thank you very much, guys. All right. thank you everyone for your time and we should connect tomorrow thank you2026-01-25 Automation Process Naming, Versioning, and Trigger Strategies

Creation Time: 2026/1/25


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-25 17:07 (Duration: 7362 seconds)
  â€¢ Location: [insert location]
  â€¢ Attendee: [insert names]


ðŸ“’Meeting Outline


Naming and Versioning of Processes

  â€¢ Naming convention The team reviewed a standard naming format for automation processes. Internal automation processes should start with the prefix "ACES", followed by an underscore, the functionality/business process name, and a semantic version (major.minor.patch). Example purpose: easily identify origin (ACES/internal) and related business process.
  â€¢ Versioning rules Semantic versioning guidance was given: increment major for breaking changes, minor for non-breaking enhancements, and patch for bug fixes. Versioning is required before redeploying a changed process.
  â€¢ External (non-automation) use cases For processes originating from external business teams (with their own IDs), they should prepend their central asset or identifier to their use case / business functionality name. Feature IDs may be added but are optional; the version field remains important for internal automation.


Ways to Start a Business Process

  â€¢ Trigger types overview Multiple input channels to start processes were described:
    â€¢ API / application-to-application triggers: external systems call ACE APIs to start a process.
    â€¢ UI triggers: web UI actions can start a process.
    â€¢ Event ingestion (injections): processes start in response to events (e.g., merchant invoice arrives).
    â€¢ Bulk processing / file-based ingestion: upload files (Excel, CSV, flat/mainframe files) to trigger a process per row.
    â€¢ Scheduled jobs: on-demand, one-time, and recurring schedules (date, duration, cycle, cron).
  â€¢ Bulk processing details The file ingestion feature supports Excel, CSV, and flat files; configurable delimiter support (e.g., semicolon for certain markets); header-line skipping and header validation (field names/order). For each row, ACE will trigger the target business process. Files can be processed immediately or scheduled for later.
  â€¢ Transformation policies A transformation policy/editor maps input file columns to process input fields (e.g., column1 â†’ address1). This policy defines how ACE will interpret file data and feed it into the process.
  â€¢ PII handling and encryption PII column marking is supported but recommended to avoid explicit marking when possible. ACE performs whole-file encryption and process-level encryption; sensitive fields in processes can be encrypted by default per configuration. EDPP and platform security controls apply.


Scheduling and Run Controls

  â€¢ Scheduling options
    â€¢ On-demand (one-off) scheduling for one-time jobs (e.g., KYC refresh for 100K accounts).
    â€¢ Recurring schedules with cycle parameters or cron expressions for complex schedules (specific days of month, weekdays, intervals).
    â€¢ Duration: delay-based starts (e.g., start after 5 minutes). Cycle: repeat frequency with optional repetition limits or continuous runs.
  â€¢ Timer start event configuration Timer configuration supports Date (specific datetime), Duration (delay), and Cycle (recurrence with repetition/stop conditions).
  â€¢ Process orchestration and chaining One process can start another using a process task. Options:
    â€¢ Async: trigger downstream process without waiting for completion.
    â€¢ Sync (default): wait for downstream process to complete before continuing.
  â€¢ Concurrency and capacity Processes can run sequentially or in parallel. Parallel execution depends on deployment capacity and must be performance-tested in lower environments (E1/E2) to determine production sizing. Teams should benchmark volume and request additional capacity based on performance tests.


Event Ingestion and Listeners

  â€¢ Event ingestion ACE supports listeners (ACE listener component) that subscribe to external events (merchant invoice arrival, email bouncebacks, inventory events). When an event occurs, a process can be triggered per event.
  â€¢ Reliable event-based consumption The platform can poll or listen to event sources and start processes; examples include SFTP object arrival, secure file transfers, or message-based triggers. Configurations can check file locations periodically and replay transfers where required.


Input Channels and Storage

  â€¢ Supported input channels
    â€¢ Secure file transfer (Accepts/AS-specific secure transfer)
    â€¢ Email attachments (extract and process email content)
    â€¢ Object storage (AWS S3) for large files
    â€¢ Cornerstone/big-data platform integration
    â€¢ Direct API calls from external applications
  â€¢ Control Center Bulk upload and control center UI screens allow upload, scheduling, mapping, validation, and triggering of jobs. Detailed control center demo to be covered in next session.


Error Handling and Reliability Patterns

  â€¢ Exception categorization
    â€¢ Business exceptions (e.g., account already closed): handled by business paths (notify ops, log work item).
    â€¢ System exceptions (e.g., downstream API failure): logged and surfaced to support teams; SRE may monitor platform-level failures.
  â€¢ Retries and backoff Retry strategies should be part of process design. Recommended patterns:
    â€¢ Configurable retry count
    â€¢ Delay/timer between retries (exponential or fixed backoff)
    â€¢ Escalation/notification after persistent failure (email to ops or support, create work item)
  â€¢ Work baskets (queue analogy) ACE provides work baskets (user or ad-hoc) to queue/process itemsâ€”serves as equivalent to queues for manual/operational handling and to avoid accidental duplicate runs.


Security, Governance, and Ownership

  â€¢ Data protection & EDPP ACE adheres to Enterprise Data Protection & Privacy (EDPP) guidelines. Platform encryption and controls protect PII. Deployment and process-level access control aligns with EDPP.
  â€¢ Deployment IDs and isolation Deployment IDs segregate processes by BU/market or functionality. Access controls can limit who sees/interacts with a deployment ID; platform support has access for operations.
  â€¢ Operational ownership Ownership for failures is determined by which system (API/service) failed. Product owners and respective system teams own remediation for their components. ACE provides platform capabilities and logging to identify failing components for RCA.


ðŸ“‹Overview

  â€¢ Internal automation naming should use "ACES__" and follow semantic versioning; versions indicate breaking changes and releases.
  â€¢ Processes can be started via APIs, UI actions, event ingestion, file-based bulk processing, and scheduled timers (date/duration/cycle/cron).
  â€¢ Bulk processing supports Excel/CSV/flat files, mapping via transformation policies, header validation, and scheduling of immediate or deferred runs.
  â€¢ There are multiple scheduling patterns: one-time, recurring cycles, cron for granular schedules, and on-demand/manual triggers from Control Center.
  â€¢ Process chaining supported via process tasks with sync/async options; parallel execution is supported but constrained by deployment capacityâ€”requires performance testing.
  â€¢ Event ingestion uses ACE listeners; ACE can integrate with SFTP, object store (S3), email inbox, and big-data platforms like Cornerstone.
  â€¢ Reliability best practices: classify exceptions, implement retries with timers, alerting/escalation, and use work baskets for queued manual work.
  â€¢ Security: EDPP compliance, encryption of files and sensitive fields, deployment-level isolation, and controlled access to work baskets and admin roles.
  â€¢ Operational responsibility: the system owning the failed component typically takes ownership after RCA; ACE provides platform and logging to assist.


ðŸŽ¯Todo List

  â€¢ owner: Platform/Product Owner
    
    â€¢ Document and publish the official naming and versioning standard (ACES__), including examples and guidance on external team identifiers â€” due: [insert date]
    â€¢ Share semantic versioning examples and when to increment major/minor/patch â€” due: [insert date]

  â€¢ owner: Integration/Product Engineering
    
    â€¢ Provide Control Center demo covering bulk upload screens, transformation policy editor, header validation, and scheduling â€” deliverable: demo + brief guide â€” due: [insert date]
    â€¢ Add documentation and examples for cron expressions and timer start event configurations (date/duration/cycle) â€” due: [insert date]

  â€¢ owner: Security/EDPP Liaison
    
    â€¢ Confirm and document encryption & PII handling recommendations (whole-file encryption, field-level encryption guidance) and publish best practices for marking PII in processes â€” due: [insert date]

  â€¢ owner: Performance / SRE Team
    
    â€¢ Define performance-test plan for high-volume processes (E1/E2 testing steps, metrics to capture, required capacity estimation) and share template with product teams â€” due: [insert date]
    â€¢ Provide capacity-request process (how to request additional deployment capacity for production) â€” due: [insert date]

  â€¢ owner: Operations / Support
    
    â€¢ Create runbook for handling secure file transfer failures (replay options, SFTP checks, control center steps to re-run processes) â€” due: [insert date]
    â€¢ Document who to contact for API/global layer failures and post-failure RCA workflow â€” due: [insert date]

  â€¢ owner: All Product Teams
    
    â€¢ Perform E2/E1 performance testing for planned high-volume jobs (e.g., bulk 100K+ rows) and submit capacity estimates to SRE â€” due: [insert date]

Notes:

  â€¢ Insert specific dates and attendee names where placeholders exist.
  â€¢ Control Center and transformation policy deep-dive scheduled for next meeting/training session.


Transcription

00:00:01 - 00:01:31 Unknown Speaker:

Is d3 that i will seven with that i will hand it over to uh. We'll first go through the agenda, the left-right agenda from yesterday, and then we'll cover the day three agenda. So one part that we didn't cover yesterday about yesterday's agenda was how to name a process. So one of the key things that we have is that we have a standard format of how we expect the teams to name or design a process. So essentially what this means is. It just helps us understand where these automations are coming from and also give us an insight as to what is the business process it actually caters to. So typically, any automation use cases which comes from, say, low-pressure migration, those actually what we do is we have them start their process name with ACES.

00:01:32 - 00:02:32 Unknown Speaker:

And underscore the functionality name, the business process name, and a version. So the business process name is pretty straightforward. ACES indicates this is an ACES internal automation workflow. And the version is kind of critical in the sense that it indicates how many versions that the process has gone through. Is it 2 days? Yes, it is 2 days. I have recorded it. You have recorded it? Yes, I have recorded it. I have understood it well. I have written all the charts. Okay, good. It is easy. If you don't understand it, it is easier than this. It is a very easy process. Daily is not easy. It is a very easy process. The concept is easy. There might be some difficulty in coding. They will show you how to create a show chat.

00:02:33 - 00:08:23 Unknown Speaker:

They will show you how to create a show chat. They will show you how to create a show chat. They will show you how to create a show chat. I called him again in the morning and he said he was coming. And that's the extent of some of that. Okay, let's go. . . Now you make a change to the business process, then you introduce a new step to it. Now this is what we call as a breaking change. So there is a difference between how the business process works with 1.0.0 version and 2.0.0 version or however you want to name it. So the 1.0.0 typically means 1 is the major version, 0 is the minor, and the last 0 is the batch. So it's typically semantic versioning. If you're not familiar with it, that is fine.

00:08:23 - 00:09:22 Unknown Speaker:

But it's like if you are making a major change, then you change the major version. If it's a minor change, you change the minor version. And then if it's like a bug fix quick change, then you update that version. So that's what it is. And that is very important because if you change the business process and you are going to deploy a new version, you update the version and then re-deploy it. So that is the change that we are looking for and that's the format. So non-automation use cases where we have external business teams that work with us who have their own AMIDs or car IDs, okay, then essentially we ask them to add their central asset in front of their use case and their business functionality name. That's the format of it. So .

00:09:23 - 00:11:52 Unknown Speaker:

I'll take a pause. Are there any questions? Yes, okay, so you can add feature IDs. But the base for the expectation is that if it's internal automation and the version is very important for us. From our perspective, we just want to make sure that we try to do that. Lift it back to the use cases. So in day three, family wanted to. Right now, you have had some kind of an insight into what is this, what is the process, what is the case, but what it means to actually starting a process, starting a business process, right? So as product owners, product managers, you would be thinking, okay, how do I go about starting a process? So there are multiple ways. Uh, that we do, uh, that you can actually, uh, integrate with is, uh, okay.

00:11:52 - 00:12:48 Unknown Speaker:

There are a few that we are covering here, okay, one is called processing okay, and you can also do scheduled jobs, okay. So which means, um, I think the report covered it a little bit, so which is like scheduling, uh, business process, every day, every hour, every minute. Okay, so, however you want it, but application to application integrations, this is like, okay. Say, there's a merchant system that you have and you want to integrate that to Ace. So the merchant system can actually send a request to Ace. And from Ace. You can start a machine process, right? So that's your typical application to application. Or say, similarly, if their merchant system has a neat web application, a UI. And they want to actually trigger a process the same way you can actually trigger it.

00:12:49 - 00:15:52 Unknown Speaker:

The other interesting way for you to start a business process is responding to an event. We call it typically an injection. Injection, what this means is say, so the same way like say a merchant is actually going to send an invoice to American Express. So in this case, it's an event for American Express. So when this event occurs, then we want to actually take an action. So when this kind of event occurs, then we have to be ready to know that the event occurred, the first thing, and then actually take an action for it. So this is what the event injection is. Any questions? So, I talked about bulk crossing. What is bulk crossing? Because it's a very generic term. So, what is bulk crossing in maize? We have a specific... Any questions?

00:15:58 - 00:20:12 Unknown Speaker:

So, I talked about bulk crossing, I didn't give any, you could have watched it, it's okay, but... Because it's a very generic term, right? So, what does bulk crossing mean? We have a specific feature within case, okay? And what this does is... It is an easy way for a team. Start the process with the pilot. Thank you. Typically, when you are starting a process, the first thing that comes to your mind can be in different formats. And one of the most common formats that I've seen is they have files. So the file comes from a different system. The file is something that the ops team made ready or paid, comes from an external entity. So there are different ways to file. So what we do is we are supporting.

00:20:13 - 00:22:21 Unknown Speaker:

The most basic formats out of the box, we have support for Microsoft Excel, comma-separated values, CSVs, and also we are adding support for flat files. Flat files are basically mainframe files, which is basically, they are what you call, they are very flat files, they don't have delimiters. Nothing of that sort and they just have like say it's like 0 to 10 the field is named or something of that sort. That's what a flat file is. If you're aware of it, it's fine. Otherwise, the data can be in Excel or . So this, what we do with this is like we take the file and essentially there are say 10,000 rows of the file, then process for each row of the data. So, say, that is what we typically do, right?

00:22:21 - 00:23:59 Unknown Speaker:

So the first row is basically your data, the second row is your data for a different customer, the third row is a different customer, something of that sort, and then we try to actually trigger a business process for each and every row of the data. And we also provide easy options to execute the process immediately, like as soon as you upload a file, or you can also schedule a file to process later. Okay, so that week, say, if you want to schedule the process to run two days from now, you got the file. But you just want to schedule it and wait. So you can do that too. These are some screens, okay, from the bulk processing from RAS Control Center. I'll actually go over it a little bit in detail tomorrow when we cover Control Center.

00:24:00 - 00:24:52 Unknown Speaker:

But this is the file upload screen where you can upload a file. And then you can actually process a file from this way. And here, again, so one important thing is when we are trying to process a file, okay, so the first thing is that we need to understand this, okay? I have N columns in a file. So I have, say, 10 columns in a file. And the first column is name, the second column is address, the third column is account number. So this file, through my business process, how do I map it? So I need to actually provide a way to understand how Ace would actually interpret the file. To push this data to a business process. So that's what we are doing here.

00:24:52 - 00:30:35 Unknown Speaker:

This is called a transformation policy, a policy that we provide this editor for you to easily configure saying, okay, column one is address line one, column two is address line two, and this is how it needs to be set to the business process. So that's what this screen is about. Oh, Pompica! Any questions? Only I have one question. So when we say bulk processing, is it a part of an RPA task or is it something which is driven by the Java classes? This is not an RPA task, okay? This is just a feature. To take a file input and trigger a process, like a base process, right? In the AS process, you can have a service task, task, or RPA task, user task, whatever. That doesn't... but this is just to consume.

00:30:35 - 00:33:14 Unknown Speaker:

This is to read your input and convert it to a format. Where that gate, which is there in the pile, can be fed to your business. That's about it, an input channel. This is an input channel. Just like mapping the columns of the data. So we don't reformat. Okay, this is an out of box feature, right? So if you have to reformat, then you would have to say how to reformat and things like that, right? So it doesn't, uh, give a way to reformat it. But it has some features like, say, uh, like. If if you are looking at, say, if your file has two or three lines of headers, okay, you can configure and then your data. So you can configure how many lines to skip. So there are multiple lines of headers.

00:33:15 - 00:34:10 Unknown Speaker:

So there are even files with headers up to 10, 15, those merge headers with documentation, right? So you can actually tell how many lines to skip before your data starts. So those kind of configurations you can do. And there are different types of PHP files. So there are comma separated files. But then again, they are not gamma separated in some markets, right? We recently encountered a Germany market where it was semi-colon separated, okay? So we support that too, okay? And things like that, okay? So the basic features will be supported, but then one of the pictures had a validation, say if you want to validate. Say your first column is always address, the second column is the start of line 2, the third column is account number.

00:34:10 - 00:35:18 Unknown Speaker:

And if your header has these fields, let's bring address 1, address 2, and account number. Then we can validate so that it's in the same, the field name is the same, and also it's in the same. Because sometimes, see, systematically generated files, they're always that. But say if somebody is manually creating a file, then the orders can change something of that sort. So if you want somebody to validate that, so there is something called header validation, which you can configure to say it is in the same order and the same data is spreaded. Got it, got it. I also see PII data, so does it specify a column as a PII column? It does, but you don't have to use it. We recommend not using it anymore. I'll give you a . We do whole file encryption.

00:35:18 - 00:36:27 Unknown Speaker:

And what we do is when we start a project, we actually then send the data to the process. And what that eans is you don't have to market PI data . Es , specifically , but within your process . Kay uh , values say , uh , okay , this is my input field . Kay , uh . I i hink uh , group would cover it a little bit , but within the process , you would say , okay , this data is pi , okay . O then what happens is is that , uh , by default , would encrypt that data always , and , uh , store it . Kay , i'll cover that in a little bit . Kay , that's one of the. . Thank you. Any other questions? Okay, so the next way to start a process is scheduling a process. This, again, we looked at it a little bit, but we'll just go over them a little bit more.

00:36:27 - 00:37:25 Unknown Speaker:

So there are, when you're talking about scheduling, right, so you have different ways of scheduling, okay, so on demand, okay, which is something that you say, okay, I have this event that occurred, say I have to do KYC refresh for this market, and I have, say, 100K accounts to which I have to do it, and I need to do it just one time, okay. So this is like a one-time project, right? And you want to actually do this only one time, so you would actually have a scheduler which you can configure to start and execute this process only one time for your use case. Now, there are scenarios where you say, okay, I have to run this every day or every week, every month. These are called the recurring process, so where you actually have a schedule that is consistent and you can actually...

00:37:26 - 00:38:33 Unknown Speaker:

So these are the different types of scheduling that you can do. Okay, So the next way, uh, to start a process is scheduling a process. Okay, this again, we, uh, looked at it a little bit yesterday, but I'll just go a little, a little bit more. So there are, uh, when you are talking about scheduling, right? So you have different ways of scheduling. Okay, so on demand, okay, which is, uh, something that you say, okay, I have this event that occurred. Uh, say, I have to do KYC Refresh or this market and I have, say, 100k accounts to which I have to do it. And I need to do it just one time. Okay, so this is like a one thing.

00:39:07 - 00:41:28 Unknown Speaker:

You want to actually do this only one time, so you would actually have a scheduler which you can configure to start and execute this process only one time for your use case. That's scenarios where you say, okay, I have to run this every day or every week, every month. So these are called the reckoning processes. You actually have a schedule that is consistent and you can actually do that as well. So these are the different types of scheduling that you can do on it. A little bit, so I don't know how much you can see the screen, because on the right hand side there is something called time and cycle limit. So when you, actually there is a drop down and when you select the drop down, So these are the different types of scheduling that you can do.

00:41:28 - 00:42:25 Unknown Speaker:

A little bit, so I don't know how much you can see the screen. On the right hand side there is something called timer cycle event. So when you actually there is a drop down and when you select the drop down when you're creating timer start event. So this is a whatever that you see here is the timer start event. And when you select that and then you try to configure your timer you have these options. Date, duration and cycle. Date is to start a process on a specific date time. Okay. You would want to say, okay, January 1st, 2024, I want to start this process. Then you can specify the time exactly to start the process. Duration is basically, okay, I want to start this process with a five-minute delay.

00:42:26 - 00:43:20 Unknown Speaker:

So I'm going to start it now, but it needs to be done after five minutes, right? So that's what a duration would be. And cycle is what a recurring schedule is. What it means is like you would say I want to start this process with a five minute delay and it needs to run every five minutes. And within the cycle, the recurring process, you can also define if it is going to be running non-stop continuously or say I would have repetition of three times. I want to run this process three times and then I have to stop it. Okay, uh, so you can say I'm going to run this process every five minutes for three times, and then I'm gonna stop it. Or I can run it continuously for every five minutes, right?

00:43:20 - 00:46:42 Unknown Speaker:

So that kind of configuration is something okay, so these are the different ways to schedule a process in. Yes, okay. So that way, you can plan to execute your business process in, however, the way that you want. So we give you a number of options to do it. Any questions? Is there a way to trigger a process based on the outcome of some another process? So a process completes or terminates, fails, something like that. The next process can be scheduled like that. You can do that. Okay. So essentially at the end of the process, within the process itself, you can actually do a start process of the next process. So let's say... First step, and then you want to actually go to the end of your process, and then you want to start a new process, right?

00:46:42 - 00:48:01 Unknown Speaker:

So here, you have an option of doing a process task. So the process task would be most in the process, okay? And that way, it will execute. So here, you would say... And essentially... After these two steps, when it encounters this step, it will start this new process. And it has different flavors too. One is async. Async is, it triggers the process and then essentially it would finish this process. It doesn't wait for this process to complete. But say if you don't mark it async, then what happens is it waits for this process to complete before actually it stops or ends the first process. Does that answer your question? I actually have one more question. So some schedules are in a very cyclic fashion wherein, for example, the process is supposed to run five days a week or seven days a week, but some processes are more sporadic.

00:48:03 - 00:49:03 Unknown Speaker:

They might be required to run on a specific number of days in a month. So how can we do that kind of scheduling? So there's two options here, okay? When you're actually changing it to a time of start date, right? When you're doing a cycle. So I'll share the documentation. There's something called . The typical time of value that you would give us something like . So this is your. This is basically we need to run every five minutes. Okay? And there is also a cron schedule, which is, and you can say, I'm not sure if I'm doing the cron correctly. Okay, but what if this means is you can configure it like this? Okay, what it says is okay? Run on the first day, third day, 50, 80 of the month, something of that sort.

00:49:05 - 00:50:16 Unknown Speaker:

So the cron gives you more options for you to run it at a very granular level. Same way you can do it at a minute, hour, and things like that. Okay, I've already got it. Thanks. So it's called a cron scheduling. If you look at the documentation, it will be there. Yeah, just to follow up on that question. So processing is generally not sequential, right? Like we can run multiple processes at the same time as long as we use this async thing, right? Do you want to start multiple processors? Yeah, so I think the conclusion is that we are used to BP where one process is running on the server and then it starts the next process, right? But because all of these server starts will be running in the RPA web handlers, I don't think that is the case, right?

00:50:17 - 00:51:12 Unknown Speaker:

So does this make sense to have a process running sequentially in A's? You can run sequentially, you can run parallelly, you can run any number of processes. But what you need to take into account is, okay, what we talked about yesterday, right? There is a deployment ID, a specific cloud deployment that is done for a service, right? For a business process. And it has a specific amount of capacity that it can run with. So typically you would do this in your event testing, you do testing, right? That you would say, okay, how much volume can I push through? Okay, I can say. I can run 100,000 processes at the same time, right? So something of that sort. So you would have to have a benchmark to say, OK, how much you can actually utilize your particular capacity to run the processes.

00:51:13 - 00:52:19 Unknown Speaker:

But it's not like BP. It doesn't have to be one process that you use, and then you'll have to trigger the next process. Nothing of that sort. Yeah, right. Yeah, that's an excellent question, John. Thanks. Any other questions? Yes, I have a question. So do we allow a process that starts only when someone gives the order? It could be like any time on any day for a particular month. So there is no fixed time or fixed frequency for a process. But when volume is coming, someone... So like what happened in DP, we have processes where we will have someone to raise the request to the control room, then control room will run them. Do we allow the same arrangement? You can, you can. Okay, so this is called, this is on demand.

00:52:19 - 00:54:28 Unknown Speaker:

Okay, so here you don't have to do a start process. So this is like a start demand. Okay, so let me give you a demo on how we would do it. This is your control center i'll cover this in detail tomorrow but essentially. We don't have that much of ragh, we can leave now. What are you doing? I'm doing programming. I left my ragh, I'll come tomorrow. Sir, can I give it to you? I'll give it to you. I'll give it to you. Sir, can I give it to you? I'll give it to you. See, these trainings, is this Ace or Loom? Ace. Ace training is over. Next, Loom. After that, I'll tell you about the counter. Okay. If you understand this, watch it for 2-3 times. Okay. This is a complete summary, right?

00:54:28 - 00:56:52 Unknown Speaker:

Stop in the middle. After 60 minutes, stop and keep a recording. Keep it continuous. Okay. This is the process that ran just now and it got completed. If you look at it, it has the process. Yes, so the flow will be the same, right? . . . . Are there still operators to, for a case to start? Okay, so first thing, the time delay on this thing doesn't come into picture in case. Okay, you would have to probably, as you're looking at this, right? You would have to go away from thinking of this time delay or missed event, right? Because this doesn't do it, okay? Because you have infinite capacity. There is not a specific machine that is allocated for your processing and that machine was not available so I'm not able to process that.

00:56:53 - 00:57:51 Unknown Speaker:

That doesn't come into picture here. Now whether you want to actually react to an inventory coming in, that is what I'm going to cover in a little bit. Either basically event investment, like basically inventory coming in as an event. You can actually assume that as an event, okay, and then essentially you're going to start the process. Any other questions? Yeah, sorry, Marty. It might be a little bit unrelated, but is there any way, like, there's no such thing as a queue, right? So in Blueprints, we would load cases and people keep track of inventory, but I don't think there's any equivalent in age, right? So let's say I have a process that should run once a week, but not more than once a week. Is there any way to prevent that from happening?

00:57:51 - 00:58:44 Unknown Speaker:

So that's, for example, if it triggers the inventory more than once in the same week, that's... you know, there's some tracker somewhere or some data that it can use to prevent it. So it doesn't mean, it's not that there's no queue. What we have is, it's called a work basket, okay. So it can be a user work basket or it can be what we call it as an ad hoc work basket, okay. So it's typically akin to a queue, okay. So you put your work items into a work basket, and then you can pick and choose from the work basket to process them. So that's how you would manage a queue within ACE. So you'll create a work basket for your business process, and then you would read off that work basket.

00:58:44 - 01:01:01 Unknown Speaker:

Now to your question, if a process is supposed to run only once, it would only run once a while. Based on your schedule, if you set it up, it is not going to run more than once. Sure, yeah, I'm thinking an accidental retrigger or something like that, but it'll be more difficult, right, because instead of drag and drop, we'll need to trigger it manually through the postman, so there's this room for error. Okay, perfect. I'm sure all of you are going to know what we call AI. Do you want to eat birji pala? What? I don't want to eat birji pala. I don't want to eat birji pala. I don't want to eat birji pala. I don't want to eat birji pala. I don't want to eat birji pala. I didn't take any flint with me.

01:01:02 - 01:06:35 Unknown Speaker:

I didn't take any flint. I didn't take any flint. Wouldn't doubt. Is there an option to pause the video? There is a pause option. Hit the middle button. Hit the round button. Let's try again. Let's try again. Let's try again. Let's try again. Let's try again. Is exposed api's for them to trigger this particular process okay so that's pretty straightforward so it's the application has to call our application. How do you respond to a demand? So this is another way of wanting to start a process okay so um. Is it too much? No, it's okay. The design is a bit... We'll have to see. It's okay if you put it in now. No, it's because of you. So the first thing, understand that the event occurred, and then you want to trigger a business process for each event.

01:06:36 - 01:08:29 Unknown Speaker:

So that's what a business event can be, say, what we talked about earlier. Or be getting the inventory of a certain system, or say, experience in... Am I leaving to go to the hospital again? Why don't you take a helmet? I don't want to wear a helmet. Everyone is wearing a helmet. If I get a new helmet, I will wear a new helmet in a week. God will not let you wear a new helmet. So all of these can be, or say you send an email to an external customer, the email bounces back. So this is a bounce back from the email inbox of a customer. So you want to actually then trigger. Paper communication to the customer. So this would be an event again, okay, so these kind of events can be configured to be CM or shop.

01:08:29 - 01:09:38 Unknown Speaker:

Okay, so Ace has something called Ace listener component. Okay, and I will not get too much into the detail of how. How cruel is he? He bought clothes for the government. He won't change. He won't buy good clothes for the government. What will happen if he leaves the trains? I don't know. It will happen. He won't change as much as the youth. Because the CMs are big, right? Do you have a question? I've been shopping for 4-5 days now. People like me in one way, but they don't like me in another way. Okay, I'll agree to that. But we can't do anything other than shopping in the middle of the city. That's what the princes used to do. They used to say, with the wall right. Hey, the king of the tribune will lose his eye.

01:09:38 - 01:11:01 Unknown Speaker:

They used to say, with the wall right. How much is this night's payment now? Is it 3 double? It's in the trends. It can be configured to listen to specific events that occurs within NextBus and then it can actually deliver a business process. So that's what the event ingestion is. Do you want me to tell you something? Yes. You should always buy clothes. If you go in front of the festival, you will get wet. What is the problem if I go in front of the festival? It is not the festival, you will get wet all over the village. Now buy one get one will definitely be in the trends. Do you have any coupons? In the trends? Yeah. I have one, but it expired. How many do you have? I don't know. I'll try to buy one.

01:11:03 - 01:12:47 Unknown Speaker:

What do you want? I want to buy a t-shirt and pants, what else? I've lost all my money. What? I've lost all my money. I want to buy a t-shirt and pants. I don't know. I'll try. Do you have a coupon for a shirt? There are different types of coupons There are different types of coupons There are different types of coupons There are different types of coupons There are different types of coupons. This is a difficult example of... So, essentially, what happens is the merchant sends an invoice to Amadeus, and then it actually clears a merchant process, the invoices are processed with the NAS. Hi, it's me, Niko. I have a quick question. Sorry, I just wanted to check on when you mentioned about the APIs that are being used in these.

01:12:49 - 01:14:33 Unknown Speaker:

Space I should say if if there is a you know API failure or any specific reason and then we do the RCA and we figure out that there was an API failure. Who would pick up the ownership? Is it the ACE team or capabilities, basically the product owners? Okay, so how would you ? You understand which APIs fail, which system actually had failures. Okay, so the API can be API's, the API's can be your calling's global layer API's and global layer API's fail. So who wants that particular global layer? And they have to go to actually monitor and maintain their API's right. So those are more events depending on what actually happened. It's typical to any other application or a business process. You would have to investigate that the application failure was, which system failed, and eventually that system would take ownership.

01:14:48 - 01:16:56 Unknown Speaker:

So basically, from this standpoint, when the build is being done, the owners or stakeholders would be involved in the overall automation? We provide the platform for you to build on. You are the product owner and you are going to build on. Then you as the product owner need to be part of this business process. And your tech team whoever is actually working on it would actually be working with the other tech teams right so that's how it works okay. We provide a platform. Yeah, got it. Thank you. How many buildings are there in total? 6 buildings total. How did you build 6 buildings? It is the same building. I built 6 flats. Why did you buy it? I didn't know then, so I thought I should buy it. I thought I should buy something, so I bought it.

01:16:59 - 01:18:08 Unknown Speaker:

How many houses are there in the village? There are no houses. All of them are getting their heads cut from their heads in the village. There is a Maramba village next to the old one. The third one is next to it. The one next to my house is in Australia. There are two more houses there. The first one is for the elder one and the second one is for the younger one. The fourth one is about to start again. They are being built for Marambabhi. They are being built for Marambabhi. They are being built for Marambabhi. They are being built for Marambabhi. They are being built for Marambabhi. They are being built for Marambabhi. At that time, there was a big storm. There was no cement, right? Yes, there was cement.

01:18:12 - 01:20:33 Unknown Speaker:

When the storm stopped, they built a new one. Once again, this N was built. What was his name? His name was Anjali Mamma. Anjali Mamma was the one who built this building. Not just him. There was Buddha. There were 4 buildings next to him. This is the kitchen after the hall. This is the kitchen after the hall. Did he build a room in the cave? Yes. When he built it, the ghosts came and killed him. 2BHK or 1BHK? 1BHK. There is a quarry in HM Pallu. A granite quarry. They are there. Are they bachelors? Not bachelors. They are in higher positions. They are the system workers. How many are there in your village? There are 5,000 or so. 5,000 in your village? How many are there in your village? It's not good.

01:20:33 - 01:21:52 Unknown Speaker:

My father set it up by talking to him. How much does it cost in our village? Who lives in our village? In our village? How much is it? It's 4000. 4000? Yes. Hey, isn't it a few rupees a day? No, it's a small amount. Before that, there were people who were dead. They gave it to us for 6000. This book is very nice. What do you mean by the road that you gave us? It is not a new road, it is a distance road. It is not a main road, it is a road that connects you and us. It is not a main road, it is a road that connects you and us. We need a new town for HMPadu. We don't know what to do in Madigal. Is it on the road?

01:21:52 - 01:23:25 Unknown Speaker:

Yes, it is on the road. We can do it on the road. We can do it on the road. Do we have to, and if we do concurrent processing of those accounts, do we have to pre-book capacity in the workspace, or how do we manage the capacity there? So, before you process a 500K record, and the business process, you would have to test this in E1, E2, okay? So the volume that you're going to push in production, you need to do a performance test in E2. And then based on the performance test outcomes you would say okay i need two fourths okay. There is only one dog in our village. There is only one dog in our village. There is only one dog in our village. There is only one dog in our village.

01:23:25 - 01:25:13 Unknown Speaker:

There is only one dog in our village. There is only one dog in our village. There is only one dog in our village. I don't have any other advantage other than that, I don't have any other advantage other than that. I don't have any other advantage other than that. I don't have any other advantage other than that. I don't have any other advantage. He had a son and a son-in-law. He didn't have a son-in-law. He was the first son-in-law. His son-in-law was born in Bhaji. He had to go to Nelkanthapuram. He had to go to Nelkanthapuram. He had to go to Nelkanthapuram. He had to go to Nelkanthapuram. He had to go to Nelkanthapuram. So based on that, you can actually increase or decrease the amount of processing volume that you need. I see.

01:25:14 - 01:26:40 Unknown Speaker:

All right. So that estimation is something which we are going to derive from the lower environment testing, particularly the performance testing at a lower environment. That's correct, yes. All right. Do you have any idea about Chanchamma? I don't know. Do you have any idea about Chanchamma? I don't know. Do you have any idea about Chanchamma? No, he bought a house and bought a garden. What did you do with that money? I went to a puncture shop. Did you get a job? I got a job after I got married, but I didn't get a job. I got a job for Rs. 300, but I didn't get a job for Rs. 2000. I got a job for Rs. 1100 a day. What investment do you have? I don't have a sticker. I bought everything.

01:26:45 - 01:27:46 Unknown Speaker:

Still, we are earning 18 lakhs per year. No one else in our village. We are earning a small amount. All of these are the result of our hard work. We are earning a small amount. We are earning a small amount. We are earning a small amount. We are earning a small amount. We are earning a small amount. There are more than 30 of them. How many are there? How many are there? 1 in 20 people, 2 in 50 people have 2 eggs. Their wife has 1 egg and an 8-year-old has an 8-year-old. They have a master's degree and a master's degree. They have worked for 10 years. They have worked for 10 years. They have worked for 10 years. What is the price of a moustache? 10 rupees per moustache? 10 rupees per person?

01:27:46 - 01:28:59 Unknown Speaker:

10 rupees per person per moustache? They give it for free. If you give it for 300 rupees per moustache, they give it for 400 rupees per moustache. If you give it for 500 rupees per moustache, they give it for 500 rupees per moustache. What is the price of a moustache? 10 rupees per person? 5 rupees per person and 10 rupees per person. Is there any other price? What is the price? What is the price of a moustache? I don't know what to say. Not now, but when I was in Javan, I used to pay Rs.10,000 per hour. In that Rs.10,000, I used to pay Rs.10,000 per hour. In that Rs.10,000, I used to pay Rs.10,000 per hour. In that Rs.10,000, I used to pay Rs.10,000 per hour. So going forward.

01:29:13 - 01:30:41 Unknown Speaker:

The people who used to work in our village used to be the first mob. They used to work in our village and work for us. If we don't do anything about it, we will be in trouble. How many houses do you have in Ramakkam? We have about 7 houses in Ramakkam. How many houses? They will definitely do it. Who? They will definitely do it. They will definitely do it. They will definitely do it. They will definitely do it. They will definitely do it. They will definitely do it. What is this? I have never seen this. I have never seen this. I have never seen this. I have never seen this. I have never seen this. I have never seen this. I have never seen this. If they die, they will not be able to eat for 2-3 days.

01:30:42 - 01:32:05 Unknown Speaker:

After that, they will eat for 2-3 days. After that, they will eat for 2-3 days. After that, they will eat for 2-3 days. After that, they will eat for 2-3 days. After that, they will eat for 2-3 days. Continuously, you will be able to talk to the driver. What do you do to talk? Everyone on my side is a new generation. What happened to you when you were a child? Yes, I was born as a child. Now I am a normal person. Yes, you were born as a child. You were born once a year. Yes, I was born once a year. Now I am a normal person. Now I am a normal person. Now I am a normal person. Now I am a normal person. What about the time when the doctor was not there?

01:32:06 - 01:32:54 Unknown Speaker:

Do you know one more thing? In our village, everyone used to go out and work in the doctor's office. We used to go out and work in the doctor's office. We used to go out and work in the doctor's office. We used to go out and work in the doctor's office. We used to go out and work in the doctor's office. We used to work in the doctor's office. The one who has 60 acres of land is better than the one who has 10 acres of land. Is it the doctor's choice? No, it is not. If you are the first to start in the year, you should be the one to take care of him. This year is the year of Kothumal. They are in the rainy season. This year is the year of Kothumal.

01:32:58 - 01:35:06 Unknown Speaker:

Why should he be afraid? Why should he be afraid? Why should he be afraid? Why should he be afraid? Why should he be afraid? Different ways that you can get data into ASIC. So whether you pass the data as input to the... I have to take it again. If the doctor is full, we can do JC load. If the system is fully working, it won't be enough for JCV. It will be a down-label for the doctor. So, the system within American Express or outside of American Express via API, or you can also have these systems pass the files to us in a secure manner, which is secure... When I saw my machine, I thought it was my Rambabai. He was the one who was cutting my wounds. He was the one who was cutting my wounds.

01:35:06 - 01:37:27 Unknown Speaker:

He was the one who was cutting my wounds. Um um or even if you look at it we can actually get it from object store so object store is i'm not sure how many of you are aware it's a aws object store which is. They call it a strike. They call it a strike. We have to hire a driver every month. He has to pay Rs.2,000 to Rs.3,000 a year to maintain the vehicle. We have to pay Rs.1,000 to Rs.1,000 a month. My parents paid Rs.65,000 for this vehicle. It took them 3 years to pay Rs.5,000. After JC became famous, our Kishanadu Babu sold it to us. We took it. Our Ram Babu took it to our Kishanadu Babu and gave us a few lakhs.

01:37:28 - 01:38:20 Unknown Speaker:

Again, My Babu took a chain mission and gave us a few lakhs and gave it to our Kishanadu Babu and gave us a few lakhs. And gave it to our Kishanadu Babu and gave it to our Kishanadu Babu and gave it to our Kishanadu Babu and gave it to our Kishanadu Babu and gave it to our Kishanadu Babu and gave it to our Kishanadu Babu and gave it to our Kishanadu Babu and gave it to our Kishanadu Babu and gave it to our Kishanadu Babu and gave it to our Kishanadu Babu and gave it to our Kishanadu Babu and gave it to our Kishanadu Babu. How much does a doctor pay for 6 months? For 6 months, you have to pay Rs. 60,000 or Rs. 70,000. Second hand you will get a good one.

01:38:21 - 01:40:14 Unknown Speaker:

We are also making a lot of money. We have lost a little more than Rs.120,000. What do you think about this? What do you think about this? What do you think about this? What do you think about this? What do you think about this? What do you think about this? I don't know. I don't know. I don't know. How much is this? Rs. 50,000. How much is this? Rs. 150,000. How much is this? Rs. 70,000. We have email, email attachments, okay. Or Cornerstone is a big data platform within American Express. So you can actually go and get the data from Cornerstone. Or even if you look at it, you can actually get it from Object Store. So Object Store is, I'm not sure how many of you are aware, it's AWS Object Store, which is...

01:40:15 - 01:42:09 Unknown Speaker:

They call it S3. So it is basically a way for you to store large files and transfer and manage large files. So that is another option of storing and retrieving data. So these are the different ways we can have the data that you require for your process, whether to start a process or whether you need the data in between your process. So you can actually fetch and use the data properly. Is it okay if I do it like this? I think it will be better if you do it like this. Is it okay if I do it like this? Is it okay if I do it like this? If you open one tipper, it will take 3.5 liters. The mission is done. We bought the truck for 1800 rupees, but it took 2000 rupees for the tipper.

01:42:11 - 01:44:56 Unknown Speaker:

How much did it take? For the mission and for the tipper. That's the idea, isn't it? We didn't count the driver's salary. I don't know what to do with it. The mission is going to be very difficult. It's going to be very difficult. It's going to be very difficult. It's going to be very difficult. It's going to be very difficult. It's going to be very difficult. It's going to be very difficult. It's going to be very difficult. Why don't you come out of the village and make a living? Why don't you come out and make a living? What are you going to do? I am going to eat chicken. What? I am going to eat chicken with you. I am not going to eat chicken with you. I am going to eat chicken with you.

01:44:56 - 01:46:41 Unknown Speaker:

I am going to eat chicken with you. I am going to eat chicken with you. I am going to eat chicken with you. I am going to eat chicken with you. I am going to eat chicken with you. I am going to eat chicken with you. I am going to eat chicken with you. I am going to eat chicken with you. I don't know what to do with it. I'll just throw it away. I'll just throw it away. That's how data input files are executed. Okay, I see it now. Thank you. One box of curry and one box of papaya. One box of curry and one box of papaya. One box of curry and one box of papaya. One box of curry and one box of papaya. Essentially, yes. You can read off the .

01:46:41 - 01:47:54 Unknown Speaker:

We have API integrations that you can actually read their work baskets. So if you look at the service catalog, you will find all of those . Thank you. This is just an illustration part of within the data injection part. So the first example is Expedian collects all the data for AMREX and then they do a secure transfer. When I'm talking about secure transfer, it's accepted to. It's a specific functionality within AMREX for us to transfer securely large scale. And we have out of the box capabilities within ACE. Okay, if you look at it, go to Service catalog and search for Accept TV. You should be able to find it and you can read the data and using secure files, and then you can process the experience.

01:47:54 - 01:49:04 Unknown Speaker:

So one ingestion or input source or input channel that you can look for, okay, so that's what it is, and the second one is again. Merchant sends the data through email, via email to Amex, and then you can actually read the email content. Again, we have a lot of works features in the catalog. You can actually extract the email content and you should be able to process the merchant data. Only I have a question actually there. Specifically related to the signal of transfer. So, one of the processes that we have with us, at times it happens that the delegation process fails due to some reason, and we have to then go to event engine and re-trigger, I think the file transfer, then come back to ACE Control Center and start the process rather than having to change.

01:49:05 - 01:50:16 Unknown Speaker:

So, is there a way to do this action that we currently are doing from event engine via the ACE control center itself? So, one of the process has a SFT file transfer in them, Apollo process. So, in that particular process, At times it happens that the delegation fails, Wherein the SFD file transfer would fail. And then, during such circumstances, we are required to go in the event engine and re-trigger that particular action. And then we have to go back to a control center and manually start the process. Okay. So, is it right to do it from the control center itself? You can, okay. So if you're actually a step, that step, you can actually add it to one of your processes and then add that as one of your steps if you want to replay it.

01:50:17 - 01:51:23 Unknown Speaker:

Okay, so I'm not sure why the secure file transfer fails. Okay. That's something you, if you want, you can even configure SFTP to check the file transfer location every few minutes, right? Something of that sort. Right? So, and then see if the file has arrived at the location to . Okay, got it. I'll suggest. So, actually, you want to also think about how to enable the reliable automation. So, we covered some of this content a little yesterday. Okay, but what do you want to think about when you're building reliable automation in ASS? Okay, I have always things are going to break, right? So there are going to be exceptions, there are going to be scenarios that you didn't anticipate for, and there are going to be scenarios, say an Excel system goes down.

01:51:24 - 01:54:29 Unknown Speaker:

Okay, so how do you handle these scenarios? That is where we provide you with all capabilities to design your process in such a way that you can actually, uh, have reliable automation forward. Okay, so this is an example, okay, so in this case, okay, say, but actually trying to read the list of. You are trying to add a memo to the account? Business exception say the account got cancelled already. Okay, and what do you do at that point? Okay, so you can actually notify, uh, an email to the operations saying, Okay, this particular account has been. The automation has failed. So, whatever you want, however you want to want. So, you can handle this as a system exception. When you're updating it, their API failed. Then, essentially, that's a system exception at that point.

01:54:30 - 01:55:42 Unknown Speaker:

So, typically, system exceptions, the log it and capture it already. The SI comes to know immediately, so you don't have to worry about it. But, the thing is, If you want, okay, so this is just an illustration. You wouldn't do this. You wouldn't like to send an email to SRE. But if you want, okay, so you want to send an email to the production support or the SRE team, okay, which manages our application platform, right? So if you want to send a note to them, then you can actually do that, okay? So that's a way for you to design your process. This process, okay, so again, you will, if the API fails, okay, but then you want to actually think about, okay, I want to actually retry if there is a failure in the API, and do the same thing, right, say, send an email to ops, or... one four three one.

01:56:14 - 01:57:28 Unknown Speaker:

No, I'm not. I'll talk to you later. I'm not in the mood to talk to you again. I don't like your french fries, bro. You gave me french fries again? Why? Because I have so many of them. Yeah, that's what I'm saying. So here you can design a process in such a way that you can introduce a retry mechanism. And you don't want to retry like immediately, right? So when you think about retries, the retry . When you're thinking about retries, you might also want to think about, how long do I wait? Before I retry. So you can introduce a timer in between which says, OK, I will wait for five seconds before I want to retry. So that's what this illustration shows up. So when you're designing a process, you might want to think about these things.

01:57:34 - 01:59:28 Unknown Speaker:

Any questions? One of the other things that we want to touch upon is buildings in Europe. As a business thing, what are you thinking of? I'm getting this data and processing this data. How secure is my data? So there are JNS partners, there is PIA data that doesn't work, there is data that is very critical so that it doesn't, nobody else should actually see it. So you are thinking about all those things. So it gives you all of these capabilities out of the box. You don't have to... Think twice about whether my data is secure. So we give you all the capabilities to process your data in a secure manner, have your business process in a secure manner. So EDPP is a system or team with an American Express who are enterprise data protection and privacy practices.

01:59:29 - 02:00:34 Unknown Speaker:

They define how the data needs to be handled with an American Express. And it follows all EDPP security guidelines. So, for example, if you are thinking about deployment ID, okay? So, what we talked about, a deployment ID is a separate way for you to segregate the business processes at the business unit level or business workflow level, okay? So, you say, all my KYC processors goes to this deployment ID. All my India market data would go to this particular deployment ID. So you can have that kind of authorizations and people who have access to a specific deployment ID. That deployment ID, okay. And nobody else other than your platform support team. So we do have access to all deployment IDs because we support your... Diplomat IDs across the board.

02:00:34 - 02:02:31 Unknown Speaker:

But other than that, any other teams who try to access your Diplomat ID should not be able to access it. Same with your work baskets, your queues that we talked about earlier, and also the admin roles for your Diplomat ID. So you can configure all those. Your PIA data, if at a process level, if you are defending... When I was in Hyderabad, a girl called me and told me that her brother had brought her here. She asked me what I was doing. I told her that I was going to take care of the village. She asked me what I was doing. Okay, we'll be right back. All right thank you so much for me i hope it was informative. Thank you.2026-01-25 BPMN and CMMN Process Management Overview

Creation Time: 2026/1/25


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-25 15:38 (Duration: 5182 seconds)
  â€¢ Location: [insert location]
  â€¢ Attendee: [insert names]


ðŸ“’Meeting Outline


BPMN (Business Process Model and Notation / Composer)

  â€¢ BPMN definition and purpose The team reviewed what BPMN means: a visual, industry-standard language for modeling business processes so repeatable work can be automated, executed, controlled, and optimized across the process lifecycle. A "process" is defined as a business flow specifying activities in sequence.

  â€¢ Supported BPMN shapes and common uses The commonly used shapes were presented and explained:
    
    â€¢ Service task: automated business logic (API calls, system interactions).
    â€¢ User (human) task: work assigned to a person (manual actions).
    â€¢ Gateways: control flow (XOR, parallel splits/joins).
    â€¢ Sequence flows: conditional and default paths.
    â€¢ Start events: normal start, timer start (scheduled runs), and signal start (wait for external signal).
    â€¢ Intermediate and boundary events: for long-running processes and suspended waits.
    â€¢ Error boundary events and error end events: to catch and route error flows, enable retries or alternate handling.
    â€¢ End events: normal end and terminating end (aborts other parallel flows when reached).

  â€¢ Error handling and retry patterns An example diagram was shown: a service task calls an external system to get payment details. If the external system is unavailable, an error boundary attached to the task captures the error and routes to a retry service task or to failure handling. Timer boundary events may be used to wait before retrying. Multiple distinct error boundaries may be attached to a single task to route different error types to different recovery flows. Teams should standardize error naming and codes so reporting can aggregate and classify errors consistently.

  â€¢ Timer boundary for reminders and SLAs Example: user tasks (CCP tasks) can have timer boundary events to send reminder emails or notifications if the human task is not completed within a SLA window. Timer boundary events can lead to reminders, escalations, or alternate actions (cancel or reassign task).

  â€¢ Signal events for asynchronous interactions Signal start/boundary/intermediate events were covered for cases where the process waits for external inputs (e.g., payment confirmation). Using signal events makes the process async and allows resumption once the external event occurs.

  â€¢ Composer tool and design workflow Composer (design tool) was demonstrated: creating BPMN diagrams, adding start events, service/user tasks, boundaries, gateways, and referencing reusable service logic. Best practice: build reusable BPMN subprocesses and reference them where needed.


Error Boundaries and Reporting

  â€¢ Error boundary configuration details Error boundaries are configured with an error code/name and optional retry/wait logic. They are intended for recoverable errors that have a reasonable next actionâ€”automated retry, human intervention, alternate workflow. Multiple error boundaries allow separate handling for different error types.

  â€¢ Human tasks and boundaries User tasks generally don't have meaningful error boundaries because they don't throw automated exceptions. Instead, use timer boundaries (e.g., SLA timeouts) or signal boundaries (external input) on user tasks.

  â€¢ Standardization for analytics The group discussed the need to standardize naming conventions and error metadata (error codes, types, messages) so reports and scorecards can consistently classify errors across automations. This enables leadership and teams to track business exceptions and automation health.

  â€¢ Reporting platforms and approach
    
    â€¢ Current reporting (Blue Prism era) is via Cornerstone -> Tableau (scheduled / delayed).
    â€¢ ACE goals: replicate important Blue Prism reports and move toward real-time reporting using Elasticsearch + Kibana for more self-service, near-real-time analytics.
    â€¢ Data pipelines are in place; teams can iterate on report definitions. PO/owners may need to request specific report formats; the team (Jason, Abby, Jay) will assist in translating existing report sets to ACE.


Case Management (CMMN)

  â€¢ CMMN definition and when to use Case management addresses unpredictable, event-driven, complex flows where sequence is not fixed. Use CMMN when the process is not strictly sequential and requires human-driven decisions, parallel and dynamic activities, and can be long-running (30/60/90 days).

  â€¢ Supported CMMN shapes and concepts
    
    â€¢ Case model: the case definition and root container.
    â€¢ Stage: a group of tasks/processes activated by conditions; used to structure case phases.
    â€¢ Tasks: human tasks (manual activities) and process tasks (invoke BPMN subprocesses).
    â€¢ Milestones: markers to show progress or achievements during case life cycle.
    â€¢ Entry/exit sentries (conditions): event-driven triggers that evaluate to true/false to control case behavior.
    â€¢ Conditions use context/state (ACE calls it context); evaluation language is MVEL for expressing conditions.

  â€¢ Interplay between CMMN and BPMN
    
    â€¢ Case can reference and invoke BPMN subprocesses (process tasks reference a BPMN asset available in the environment).
    â€¢ You cannot embed a full case inside a BPMN process (standards are different), but subprocess reuse is supported (BPMN called from cases).
    â€¢ Subprocesses should be defined as reusable BPMNs to be referenced across multiple cases.

  â€¢ Use cases for case management Examples discussed: disputes, fraud workflowsâ€”long running, human-involved, parallel activities, document attachments, external interactions. These map well to CMMN with BPMN subprocesses for automated sub-flows.


Differences: Process vs Case

  â€¢ Process (BPMN)
    
    â€¢ Sequential, predictable, process-centric.
    â€¢ Suited for short-running, automatable flows.
    â€¢ Easier to fully automate end-to-end.

  â€¢ Case (CMMN)
    
    â€¢ Event-driven, dynamic, user/human-centric.
    â€¢ Suited for unpredictable, long-running, human-involved flows.
    â€¢ Can invoke BPMN subprocesses but is driven by conditions/events rather than fixed sequences.


Deployment and Naming Conventions

  â€¢ Deployment ID and purpose Deployment ID is a logical name identifying where an automation/case runs. It groups automations by infrastructure and non-functional requirements such as volume, geography, regulatory constraints, and integrations.

  â€¢ How deployments are chosen and managed The ACE managed services + engineering will recommend deployment based on throughput, geography, compliance, and integrations. The deployment decision is effectively permanent for the automation's lifetime (moving between deployments is possible but requires negotiation).

  â€¢ Scaling and capacity Deployments are sized and can be scaled. For large jumps in volume (e.g., 10 â†’ 1,000,000), teams should notify ACE so the deployment can be scaled or reallocated. Options include scaling a deployment, allocating a new one, or distributing load across deployments.

  â€¢ Naming conventions and reuse Teams follow naming patterns during onboarding for logical consistency (examples: ACE_SOMETHING, team-specific logical IDs). Reusable assets (the meeting mentioned 228 existing reusable tasks) and integrations (RTF, ECM, ELF, PI) are available to teams.


Next steps and scheduling

  â€¢ Plan to continue and finish remaining material in the next session. Proposed start time for follow-up: 9:00 Eastern to extend session and cover all material; Merle to start his portion at 9:30. Further discussion on reporting and deliverables scheduled for Friday.


ðŸ“‹Overview

  â€¢ BPMN is the recommended approach for repeatable, predictable automation; Composer is the design tool used.
  â€¢ Key BPMN shapes reviewed: service tasks, user tasks, gateways, start/intermediate/end events, error/termination behaviors.
  â€¢ Error boundary events: used for recoverable errors, configured with specific error names/codes and alternate flows (retries or human resolution). Multiple boundaries per task are supported.
  â€¢ Timer boundaries: used for reminders and SLA enforcement on human tasks.
  â€¢ Signal events: for async external events (e.g., payment confirmation).
  â€¢ Case management (CMMN) is for unpredictable, event-driven, human-centric, long-running workflows; supports stages, tasks, milestones, sentries (conditions).
  â€¢ BPMN subprocesses can be referenced from cases; cases cannot be embedded into BPMN processes.
  â€¢ Conditions in CMMN evaluate case context (state) using MVEL; they control when stages/tasks/milestones activate.
  â€¢ Reporting: transition from Cornerstone/Tableau to Elasticsearch/Kibana for more real-time, self-service analytics; standard report set mapping is being worked (Jason, Abby, Jay involved).
  â€¢ Deployment ID groups automations by infrastructure and non-functional needs (volume, geography, compliance). ACE advises on deployment selection and scaling.


ðŸŽ¯Todo List

  â€¢ owner: Automation/Product Owners (POs)
    
    â€¢ Review existing Blue Prism reports and share the set of current reports with the ACE reporting team (Jason/Abby/Jay). Time: before Friday session
    â€¢ Provide any specific reporting requirements or examples of required dashboards for migration to ACE. Time: by Friday meeting

  â€¢ owner: ACE Engineering / Managed Services (Mitch, Drupal)
    
    â€¢ Provide links and access to Composer/BPMN and CMMN documentation and supported shapes for all teams. Time: ASAP
    â€¢ Share naming convention guidance and onboarding checklist (deployment ID patterns, asset naming). Time: ASAP
    â€¢ Finalize mapping of commonly used Blue Prism reports into ACE (Tableau -> Elasticsearch/Kibana equivalents) and present during Friday session. Time: Friday session

  â€¢ owner: Automation Teams / Architects
    
    â€¢ Identify reusable BPMN subprocesses and register them with ACE asset library for reuse across cases. Time: ongoing; initial list by next week
    â€¢ Provide data attributes (context) and example condition expressions to ACE team for cases that will rely on complex sentries/conditions. Time: within 2 weeks

  â€¢ owner: Reporting Team (Jason, Abby)
    
    â€¢ Build initial Kibana dashboards to replicate top-priority Blue Prism reports; coordinate with POs for validation. Time: initial draft by next reporting checkpoint

  â€¢ owner: Onboarding / Platform Setup Team
    
    â€¢ Confirm platform onboarding steps and ensure new teams have required assets, environment access, and connections (RTF, ECM, ELF, PI). Time: ongoing during onboarding

  â€¢ owner: All Attendees
    
    â€¢ Prepare questions and examples (existing workflows, disputes/fraud examples, required SLAs) for Friday deep-dive on reporting and deployments. Time: by Friday session

(If any action item has a specific assignee or deadline you want recorded here, provide the name and deadline and the summary will be updated.)


Transcription

00:00:02 - 00:01:07 Unknown Speaker:

Day two is hello. So what does this mean? Is, uh, when? When someone's in enterprise, right? If someone wants to build, have some automations, they want to build a BPMN flow, uh, that involves in modeling, building the BPMN and automating it through using some tool, right? And and so the repeatable work can be offloaded. And executing it in the environments, and controlling through the data flow and everything, and optimizing the process management on the lifecycle of that. So that's what we mean, what does the BPM means, that is the definition. And so in this, we build a process. Process means nothing but a business flow, what activity should go on the flow. And uh, and we. We follow the business, uh, BPM in its industry. Uh, specification for business process.

00:01:07 - 00:08:11 Unknown Speaker:

Uh, this is the BPM means, the business process model. Uh, and notation. So quickly, let me go, uh, I've prepared some examples, uh, how to handle, uh, read rise and failure. Okay, quickly, I'll go through the documentation of what repayments we support. So here is the SO, here are the shapes, what we support currently. So the commonly used one is this task, which is called the service task. That means here you can build your your business logic of what you want to do in the business room. Like calling, getting information from any, any source, system, or somewhere, and building a user task, which involves some manual action when some user has to take some action. And there we support, like in the previous discussion, it was.

00:08:11 - 00:09:16 Unknown Speaker:

We can build the parallel flows, with the gateways we can build the parallel flows like XOR or where you can build multiple paths. And we have conditional and default sequence flow and these are meant to join to provide the path for the sequential flow. And we have different start events, like the regular Start event. When it has been invoked, it will immediately start the business flow and we have a timer start, which is nothing. But when you want to schedule a process and runs automation regularly, once in a month or once in a week, you can use this Timer Start event, which is meant to run everyday or once in a month. And we have a signal start event. You have started a process and you are waiting for some signal coming outside, like a payment or something.

00:09:17 - 00:10:22 Unknown Speaker:

And you can be using these sort of signal events, like signal event and a boundary signal event or intermediate. So these are the type of signals you can define based on the design. And we also have these intermediate events. These are for a long-running business process where you want to have intermediate debate for certain user to perform some action. And once it's done, you want to come back and resume the process, right? So these are intermediate and bound events which you can use. And we have error event. So this is the error diagram. So which how through which you can handle your free tries or or business failures, Strong failures or something. And we also have different types of end events, so the normal end event and terminating end event, so terminating end event is kind of powerful.

00:10:22 - 00:11:26 Unknown Speaker:

When you want, when you have parallel cases, and you want to, you want to end article flow when you reach the terminating end event, even though the other flows are still continuing. You want to have you use this kind of terminating of the end, so we can use this terminating end event. So this is also one powerful event. And quickly, I prepared few examples, maybe we can also design them, okay? So this is one example how you can build an handling, error handling and retries. So here in this example, I'm using a gateway and service task and error even. So when you want to get some details for the payment, you will be calling some system to get the details. And in those scenarios, if an external system is down or you are not able to reach out to that and get the details right.

00:11:26 - 00:14:58 Unknown Speaker:

You can use this error bound remit, configure it to handle those exceptions and do a retry. So you can build another task which does this retry and be joining the gateway. So in this way you can retry and perform business errors. And handle them through a business workflow. And then do the normal activities. This is how you can automate the business errors and retries using error boundary event. And I prepared one more example on sending reminder emails. So this is one of the examples where you are waiting for some user to perform some action. From some ccp has to get some details from paying on from the. And I prepared one more example on sending a reminder email. This is one of the examples where you're waiting for some user to perform some action.

00:14:59 - 00:16:22 Unknown Speaker:

Some CCP has to get some details from the customer. And you can use a boundary event. This is another kind of boundary event. It's a timer boundary event through which you can perform a set of action, send an alert, send a rebounded email to the user. To the CCP to get this work to be done. So this is how you can define the sending email, sending the email to the CCP. Quickly we'll go through compose and quickly there are other shapes which how you want to use maybe I can quickly help you understand what are these nodes. So this is the compose. So here we can create a new BPMN. So like in the previous shapes, this is a regular start event. And this also has other start events. This also has the other type of start events.

00:16:22 - 00:17:29 Unknown Speaker:

Like signal and a timer start event. So timer start event, like you mentioned, can be used in automations where you want to build process runs once in a day. So as an example, we are just going to use a normal start event. And you can select this type, pen type, and this will be a normal type and you can make this as service task. Make this one as your get element details something, and here you can. This is the cluster. This is the way you're going to get the information about the payment. Here. You can have an header boundary, you can also have a gateway here in order to handle the. This is an error-borne event. This is how you can define an error-borne event.

00:17:31 - 00:18:56 Unknown Speaker:

And based out of this error, you want to handle a retry or do some failure and have some other service tasks. And then you can, based upon this, you can join it back to the gateway. So through this, you can do a retry handling. Retries. So we can use in this way you can build a retry for this particular system where you are trying to reach out to get details. So this is error-bound remain and define the details. And quickly, let me also show the example where you can build reminder emails. So we are going to take a normal, normal event. And then select a user task. This is a CCP task, where CCP a user task to call call a customer and here based upon. This particular user wants the next action to be done.

00:18:56 - 00:20:23 Unknown Speaker:

So if the CCP needs some more work to be done, then you can have a reminder kind of flow. You can have a timer boundary, and then you can send a reminder in here. You can build which will, which will send a reminder to the CCP to follow up on the action. And there are different types of home, so we also have a task, so this is a repeat us. So which you can do your automations where you have, where you have certain user to perform some action, but which is which you want to make it automatic. And this is a web-based and you can. You can automate this by creating your own step using RPM, and then this can be also automated, so we also have a different other type of signal.

00:20:23 - 00:21:38 Unknown Speaker:

Like I mentioned, this is where you you are going to use. When you are waiting for a long running process and you are waiting for some payments from the other systems, right? So you can have this sort of, let me remove this. Yep, once after you're sending a request and you want to be waiting for a signal for the payment. And you can use a signal event. And this will make the process async. And then you can move to the next steps. So this is how you can make use of this signal event to make it async. And also, we also have the gateways, different type of gateways in order to have different flows, like shown in the previous example. Have a handle and gateway also and perform parallel actions like this way.

00:21:41 - 00:22:44 Unknown Speaker:

So this is how we can build a parallel gateway for performing parallel actions. Are there any questions in there? If not, Tripod can continue. I want to make sure everybody's understanding what the different shapes, their purposes, why we use them. The tool we're looking at now is the Composer. I think Jairo introduced it to us yesterday briefly. But this is our design tool, and we use it for composing BPMN. BPMN is just a visual descriptive language for describing automation, right? And so we have the flexibility to design our automations to do essentially anything we needed to do systematically. And so Drupad will continue telling us about what are the different shapes that we support and what their purposes are. I just want to make sure that everybody is with us so far.

00:22:52 - 00:24:12 Unknown Speaker:

All right, there's a question in the chat. So you were talking about error boundaries a bit ago, right? Maybe you can go back and show that diagram. Can we talk a little bit about how an error boundary is defined, what its purpose is, and what do we do with it when it occurs? Have the error boundary event and then rectify, do the retries. Or if your downstream system is failing and you want to wait for certain action to be done on the other side, right? So you can have a wait or fail to handle using error boundary event and then do a retry, have certain duration of wait and then you can perform that action. So, similar way can take an example, there is a system we are going to maybe to call to get the payment details, you are going to call the details, call some API.

00:24:13 - 00:25:24 Unknown Speaker:

And here you want to see. Of weight. And then you can perform that action. So the similar way. Can take an example. Uh, there is a system we are going to, maybe, uh, to call to get the payment details, you are going to call the detail call, call some AP and here you want. You are seeing these failures and you want to rectify here. So in order to handle those situations, you can build a boundary when and have certain sleep during here with a timer timer event. So that the all process might have you might see in this error and then can be handled like this. And then here, you can define define your custom failure to this type office and then hold the process for a certain duration and then do a retry.

00:25:25 - 00:26:14 Unknown Speaker:

Yeah, so pause on this for a second, Drupal. So, Shivra, is this answering your question? Oh, okay. Yes, thank you so much. Okay, great. So, when we look at this error boundary, I just want to make sure that it's clear, right? So, as Drupal is saying, right, we have these various activities that are occurring that we've configured in the VPN. We've configured an error boundary event called an error boundary. And this error boundary gets triggered any time an error happens in the task that it's attached to. And it's not just any error, right? We're looking for a specific error. So if you see in the right panel, we can specify what is the specific error we're looking for to occur. And we can drive what Drupal is showing right now. We can drive alternate flows.

00:26:15 - 00:27:04 Unknown Speaker:

Based on the particular error that has occurred. And so what you see here is you see one error, right? But actually you can have n number of error boundaries on a single task. All the different potential errors can be configured here as different error boundaries with different flows coming from them if you need that level of complexity. What we see happening most often is when we get these errors is that we have one or two, maybe three at most. And they're usually divided into different kinds of business errors. So the thinking here is that we configure these error boundaries. We configure them because they are recoverable. That means that there is some reasonable next action that can be taken either by the system or by a person as a result of the error occurring. Yeah.

00:27:05 - 00:27:54 Unknown Speaker:

So this is what Drupal is showing. So sometimes we have an unreliable system we're integrating with downstream and we want to do retry in five minutes when we think it will probably be available again. And that's the timer example, right? But it might be that we need to, you know, we've got some bad data that we weren't expecting. It can be fixed. We know it can be fixed. And we assign it to a human that's going to come. And do some action, right? Change the data and then re-trigger the task. So we see that very commonly also. One of the important things that I think, Drupal, you're going to talk about a little bit is we want to standardize across the portfolio, like the naming of these kinds of error boundaries, because what happens is in reporting,

00:27:54 - 00:28:38 Unknown Speaker:

when we want to see our scorecards at the end of the day, in reporting, we want to be able to recognize what was the error that occurred, what kind of error it was. And we want to be able to report on it so that our leadership can see, hey, we had this many business exceptions or this percent of our automation attempts had some kind of error in them. So we've been working with JP and Jay. I think JP's on the call. I don't think Jay's here today. But what are those standard naming conventions? What is the information that we need to have in those? Error messages so that we can get the right information into our reports and so on and so forth. So we're building out this entire ecosystem to say, reflect. The.

00:28:38 - 00:29:33 Unknown Speaker:

Firstly, be able to recover appropriately, and secondly, being able to recognize across the value stream. That is being able to see in the automation itself, and also being able to see in our analytics and reports what's happening and when these errors are occurring, report you can continue. Yeah, so, uh, in order to get into the report slides. Okay, you can have your own code here. What? Uh, the failure? And also define the name, what is this error type? And like? In example like which Mitch mentioned, you can define multiple and have a different error flows in order to handle different failures. And, uh, yeah, like the other scenario, if you can handle through some user and perform this action manually due to some knowledge, system or any failure.

00:29:34 - 00:30:36 Unknown Speaker:

And yeah, here you can define your error name so that can be going into the reports and get the details accordingly. So this is how we can do. So that's a really good question, if you don't mind. So user tests don't really have error boundaries. You can use them, but they're not really meaningful because user tests don't have automated behaviors in the same way that these other tests do. So what happens is with a user task is whatever the data is, it gets assigned to a person and that person can see the data. So there's not really a way it can fail. So there's no exception they have to perform that. Yeah. Now, we can use other boundaries on the human test. So we can use timer boundaries, for example. So say the task has an SLA.

00:30:37 - 00:31:25 Unknown Speaker:

It needs to be done within, let's say, an hour. I think we might have talked about this before. Let's say it needs to be done within an hour. We put a timer test, a timer boundary event. As soon as this task gets created, that timer gets initialized. And after an hour, we can take an alternate path. So we see sometimes a notification might be sent based on the timer or we might cancel the task and do some next step. There's all kinds of different things you can do here. You have the full flexibility of the system to be able to do whatever makes sense for your use case. We also have signals here. So signals are an outside notification to the automation that an action should occur. Yeah, so when we say signals, that's what we mean.

00:31:25 - 00:32:20 Unknown Speaker:

So we can put a signal boundary on this, and if that outside input comes to the automation, that signal can get activated, and again, you can take an action. You can have your automation do something as a result of that. So those are the things that we see on human tasks. Thanks. Hi Mitch, it's me, Nico. Mitch, I just wanted to check, you know, just one step back, you were just speaking about the reports, you know, which would be published from this platform itself. Now, we have a standard set of reports which are currently being published, you know, from the BPE space in itself as of today. And those are very specific reports. I guess we have like somewhere around 10 to 12 reports, which covers, you know, my PRSA requirement.

00:32:21 - 00:33:13 Unknown Speaker:

So when we move into the ACE space, I should say, do the POs have to put in a specific request for that report to be, you know, to be turned out? Or do we have a standard set of reports? Because I would have to, you know, then replace, you know, whatever reports would be published within the ACE space with, you know, with regards to the BPE world, which is being done today. Yeah, so you're actually one of my favorite subjects. I could talk about it for a full hour. In fact, I think I am on Friday. But let me give a quick answer, and then we'll talk more about it at the end of the week. So when it comes to reporting, the Blue Prism stuff, like you said, today, the automations, it's all going to Cornerstone and being reported through Tableau.

00:33:15 - 00:34:05 Unknown Speaker:

Jason and Abby, they're working on reporting, similar reporting that matches what Blue Prism produces for ACE automations. We have multiple reporting platforms that are available to us. And what our target is, is that eventually we don't rely as much on Tableau and Cornerstone because that data is delayed. But instead, we use our Elasticsearch database. Kibana over the top of it. So those are just two different technologies. We don't need to get into the details, but the interesting thing there is that the data is all available for you to build real-time visualizations of your own. And it's a toolkit that is self-service. But we're also there to help you get started on that. So it's not like a sink or swim kind of thing.

00:34:07 - 00:34:48 Unknown Speaker:

We're trying to provide a little bit more robust toolkit for you to get the information out of the system that you want to get. So the nice thing is that the data pipelines are all well established and there's no work that you need to do. But when it comes to what is it that you want to see in your reports, I don't really know the answer to that. And sometimes you don't even know, right? You have a view of it, but it changes, right? So the new toolkit should help you to... be able to iterate over that and generate the information from an analysis standpoint that's most meaningful for you. No worries, thank you. And I've been in touch with Jay and Abby as well because, yeah, I could share the similar reports which have been published by UBP and itself today.

00:34:48 - 00:36:22 Unknown Speaker:

So I guess Friday will be a good conversation then. So thank you so much. Yeah, no problem. Yeah, to continue on the next steps. Yeah, so we also support the, like Mitch mentioned, we also support signals and we can have a signal boundary event on human tasks. This is when you are waiting for some actions to be done by user and then you can have the signal event. Defined here and I'll provide the name and then this can be in movement as payment has been completed it can be moved back here and resume the process yeah so we'll jump on to next case management. What is case management? Case management is used where you don't have any non-reputable or predictable process. So these are the scenarios where you want to use case management instead of process management.

00:36:23 - 00:39:52 Unknown Speaker:

So when you have a complex case process which you have to define. And then these are not in a specific order. The process is nothing but which can be predictable. That means the flow you can define when designing the process. And then you can have a gateway that will run through the flow. But in case management, it's not driven through. It's driven through the events, based on the actions, what we perform. So in, we follow the industry standard specification CMMN MMN stands for case Management, Model and location. So, sorry, okay, and uh, quickly I'll go through, uh, the CMN supporter, uh, with water. What is the shape which are. Thank you very much. Thank you. . So these are the supported shapes in CLM. So we have something called a case model, which is nothing but the case definition.

00:39:52 - 00:40:54 Unknown Speaker:

And this is the starting of the case. You can define the items in here, what are the actions they're going to perform. So we also have a stage. Stage is nothing but a group of... Task or process where you want to have certain tasks to be performed, when you have, when you want to perform based on the condition. So we all have tasks, these are the tasks, these are the human tasks, when so most of the human task is complex. Due to when some CCP has to work on this user task. And then then they want to validate everything and do not oppose the case. And also here, but if you want, you can also build a process. You can also include process inside a case, but the other way is not supported case, you can't include a case into the process.

00:40:54 - 00:45:14 Unknown Speaker:

So we also have milestones, so these are the supported shapes in CMN. So we have something called a case model model, which is nothing but the case definition, and this is the starting of the case. You can define the items in here, what are the actions they're going to perform. So. We also have a stage. A stage is nothing but a group of tasks or processes. Where you want to have certain tasks to be performed when you want to perform based on the condition. We all have tasks. These are the tasks. These are the human tasks. So most of the human task is complex due to when some CCP has to work on this user user task. And then then they want to validate everything.

00:45:14 - 00:48:16 Unknown Speaker:

I do want to pull the case, and also the good part is you want to also build a process. You can also include process inside a case, but the other way is not supported case, we can't include a case into the into the process. Um, So we also have milestones. And it's a milestone, right? So during the case, case can be a long-running case. And during the case execution, you can have different milestones. Trying to, when a user is trying to create a new account, right, and there will be different steps in getting back and checking the current report and getting details from the user. So you can track. To drive all the events, all the actions by Milestone. By having this Milestone item, whatever the items achieved can be flagged to the Milestone.

00:48:17 - 00:50:07 Unknown Speaker:

We also have this entry and exit entry. So these are the main. How these actions perform quickly i'll i'll show create a simple cmm and walk you through each steps how to use. Yeah, so here we can create a CMN. When you create a case, when you want to create a new case, you will move back and get this definition. And here you can define your fact, your item, what action it wants to take. So as this is a task, there are two types of tasks. One is the human task. And you can also, there are all these cases, nothing but event driven. And when any event occurs, all the items, the task will be performed. So you can define based on the event you want to perform only certain action. So you can have this entry condition.

00:50:07 - 00:51:21 Unknown Speaker:

This is called entry. But here you can define your conditions based on the event you want to have some condition. Like here, psa.customer.verify. So, if a customer is verified, the customer is verified, you want to perform verification when creating an account. So, when the customer is verified, you want to perform setup action, which is a sequential action. You can select a process task and then perform the process based on this event and based on this data. And also when you want to choose a manual action to be done, you can use a task. This is nothing but a human will be working on it. When you want to group these both actions, So you can have a stage and have a entry condition here and then have, if you can.

00:51:21 - 00:52:25 Unknown Speaker:

Define human office task and also a human task in order to perform this action. So this, this is how you can have the conditions, these are the event reactions, which are through which these are, these are driven from. And also I mentioned, we also have milestones, you can have addition to the in order to get this active, and similarly, you can have multiple milestones. And these can be achieved with this. This can be completed or achieved with this condition condition, it can be achieved. So let's, um, let's talk a little bit about, um, conditions for a second, okay, so um. When we talk about conditions, right, these are evaluated to true or false, these conditions. That's what we're looking for. So what does that mean, right?

00:52:25 - 00:53:17 Unknown Speaker:

When we look at a case or even at a process, right, we have data that's associated with it or state. We use the term state to reflect the data that's associated with a case. So keep in mind, through the life cycle, when I start a case, I'm going to start it with some information, some data as an input. It may be, and we call that in ACE, we call that as context. But largely we're saying, look, in order to start a case, we need to input these data attributes. Anytime those data attributes come into a case, we have an opportunity to evaluate that those data attributes have particular values and those attributes can be part of these conditions. In the example, can you pull up that example you were just showing with the condition, the create account?

00:53:19 - 00:54:12 Unknown Speaker:

Right? Yeah, so if you look at, yeah, click on that sentry. Right, so this condition, PSA.CustomerVerified, what does that mean? That means that there's some data attribute that has already been passed into the case that is called PSA.CustomerVerified, okay? And right now we're checking to see, is the value of that equal to true? If it is, then we can trigger this sentry, which means that that process gets triggered. These conditions can be as complex as you need. So we're showing a very simple one. One attribute is equal to true, but we see some things where we've got multiple attributes or multiple scenarios that need to evaluate all to true in order to be able to trigger the condition. We can also have them evaluate the false, I guess, as well.

00:54:13 - 00:55:03 Unknown Speaker:

But at the end of the day, we're looking for this to evaluate either true or false or to trigger the condition. The language that we use for specifying the condition is something called MVEL, and it's a public specification. You can find out how to use it. It's fairly simplified. The syntax is a little bit like Java syntax. But it's not as complex as having to know a coding language. So anyways, I just wanted to kind of bring this to your attention and make sure you're aware of what we're talking about when we say conditions, because that's a critical piece of this. Can you also click on the process task? Because I think we missed one piece of this. So when you see we're going to trigger a process, but you see that we've got that in detail, we've got this thing highlighted on the right panel called reference.

00:55:04 - 00:56:06 Unknown Speaker:

So what does that mean, right? So what we're saying with the process task is the reference is going to be some BPMN document that we created that is available in your environment. So remember, you know, a few minutes ago, Drupal was showing us how to configure BPMN diagrams and automations to do whatever activities we need to do. So once we've defined those, we can use those here as subprocesses. And those can be triggered as process tasks here in case management. Okay, thank you. Yeah, so they can be referred into the case and whatever is defined that can be added here in the reference and can be viewed as part of case flow. These are the main task which can which we are supporting. Uh, quickly want to have.

00:56:06 - 00:57:25 Unknown Speaker:

If anyone has any questions, we can quickly get on more to the next topic. So, yeah, yeah, I have a question. Uh, so yeah. It may be a little elementary, but I just want to understand cases like a superset, right? And if. Within a case, as you mentioned, there's a process, right? So can a BPMN fit into that process? I mean, a process BPMN can fit into a case, basically? A definition, you can't include that as part of a process because there are two different standards. One process is the business process model annotation. And case supports only case management notation. So these are different. So we can't include but you can refer them as part of case. Like, we process, we solve, get payments, and then we try, and then we succeed.

00:57:25 - 00:58:20 Unknown Speaker:

So something similar, what would be a real case? Case-by-case? I mean, in general. Well, so we have a, if you guys play with disputes workflows, that's probably a good example of something that we use for companies. For fraud. We use cases for those also. So what happens is I have a number of activities that happen with disputes. I get notified that the dispute should start. Sometimes I have a receipt that I need to attach. I need to issue a request to a merchant to provide their feedback on whether the dispute is valid or not. I have to send a notification to the customer. I have 30 days to wait where I'm doing some activities about checking to see if the... If the financial adjustment is posted or if I need to initiate a chargeback, right, there's a lot of things.

00:58:20 - 00:59:18 Unknown Speaker:

And many of these things can happen in parallel. They don't have to be all sequential. Sometimes I need people to take an action, right, to look at things. So all those things, you know, disputes is a fairly complicated thing, but that's an example where we're very confident in the enterprise and using case management to handle disputes. So within a dispute, though, I have some sub-processes that are automated. For example, sending a correspondence to the merchant and asking them for documentation, that can be a sub-process with all the steps there as a BPMN that's triggered as part of the case. Got it. And all these sub-processes, they are reusable throughout the stream? Correct. Yeah, so that's how we define these, right, with case management.

00:59:18 - 01:00:26 Unknown Speaker:

We define the subprocesses in BPMN, and then we just put the reference name of the BPMN right here in the right panel, and that's how we define these complex cases that include subprocesses. BPMN and CMMN, there's a lot there. So I want to make sure that folks understand this is not meant to be a training. There's lots of material out on the internet if you're interested in learning more about how to use BPMN and how to use CMMN. We don't even refer to any specific one because there's so many of them out there. We chose an industry standard on purpose so that that way there's no mystery. The documentation that Drupal had referenced, I think on the second tab here in this browser, that's available to all you guys. We'll make sure that you have access to those links.

01:00:27 - 01:04:58 Unknown Speaker:

That talks to the shapes that we support in ACE. So even though the specifications for both of these things are quite robust, what we've learned in American Express is that we can accomplish what American Express needs by supporting a subset of the total spec. So we don't support the entire spec, If it turned out that we found a gap, right, there was something in the spec that we don't support that needs to be supported, the good news is that we're the engineering team that built this thing, so we can always add support for shapes as needed. Okay, I think we can move to the next subject. Yeah, so, okay. So quickly, I mean, what are the differences between a case or a process? Yeah, like I mentioned, yeah, so case is a complex. Hello?

01:04:58 - 01:06:38 Unknown Speaker:

Yeah, I'll get them. Both are different, different, have different flows. So case can being complex and can be, uh, can having a human task which is a certain user has to perform more actions. And also we can automate that. Uh, we can have a certain process can be included, and case can have a process. And the other difference is the process is a process-centric approach, which is a sequential one, and case is event driven. So based on certain events, it can be changed, the flow can be driven. So like I mentioned in the previous example, we have a process, we have a case, and we have, based on events, the user has to perform certain actions, or due to some data change, it has to different case, right? So process is a sequential and it's a predictable.

01:06:38 - 01:08:02 Unknown Speaker:

So case is predictable. So based on the data chain, different conditions can be triggered and accordingly different tasks can be. And like mentioned, process is a sequential event, and also it's a predefined flow. But case is event driven, like mentioned. And the process, most of the process can be fully automated into it. That means the automations are running on daily basis, but case requires a human task decision. So these are the main differences for process and a case. You want to use case and you want to use a process. Cases can be long running 30, 60 or 90 days also. But process automations are quick. These are the main differences where you want to select. Let's move on to next. So there was a question about what is the deployment ID.

01:08:03 - 01:15:35 Unknown Speaker:

So deployment ID is nothing but a logical name for a use case. So this can be... teams can provide a logical name during onboarding. . . . Um um. I don't know. Hello? Hello? Um um um. Hello. The way to onboarding, as part of onboarding you get platform setup. So platform setup which is given to the onboarding team. And this removes all the management of the environment and all the which means you don't need to worry about getting a new DB instance, or getting a cloud environment. So you really need to focus on building your flow. Your flow can be either process or a case, and test it in the environment, and then your assets independently. So as part of this all onboarding of the platform setup, you will be given assets and can be working independently.

01:15:36 - 01:18:01 Unknown Speaker:

Like with the reusability part, we have 228 reusable tasks which can be integrated. And also we already have integration with different enterprise architecture platforms like RTF, ECM, and ELF. And also we have integration with PI for reporting so that we have a different data. More discussion of it. So this is the, this is what we need in the deployment area. And quickly as we are in time crunch, so with the naming conventions, we have certain standards of what we want. For automation teams, we use some name like ACES, ISON. Logical name for use cases. Like an example, we have an example of fraud use cases which are built by automation teams. So they use the ASA cycle, GSDF and fraud, or the dispute.

01:18:02 - 01:19:32 Unknown Speaker:

So for non-automation use cases, the naming pattern is card ID, that is common for any team, and card ID is a logical name. We have a use case, something called as a banking use case, so they have card ID and their life savings banking use case. So that's how these, uh, this, naming conventions are before we jump on any more questions. So over time, we fielded a lot of questions about .90s. So I want to make sure that people understand what this is. Every time we deploy an automation or a case, deploy within a deployment. I'm using the same term like six times. What does that really mean to us? That means we take and we group the automations in a way that it runs together in the same infrastructure. So this is a little bit technical in nature.

01:19:32 - 01:20:33 Unknown Speaker:

Now, as the ACE team ourselves, the managed service provider is part of it, we help you to determine which deployment is best to run your automation. We make this determination largely based on criteria like what's the volume of it, what systems does it integrate with, what's the geography it's covering, what's the regulatory considerations. Because sometimes data needs to be segregated for reasons outside our control, like India, for example. Or sometimes we have, you know, a million rule chain checks in in an hour's time, like with collections and eight court outside placement. So in these examples, right? We want to have the flexibility to be able to run these in in an analogy environment. That makes sense. To be able to handle the volume or handle the regulatory concerns or compliance needs, or whatever.

01:20:33 - 01:21:27 Unknown Speaker:

We group these things together and we call them as deployments. So what will happen is we decide up front what is the deployment that an automation will run in. And that name of that deployment that is associated with that automation basically for the lifetime of that automation. So we talked a little bit about reporting a little bit ago. Anytime we look at the data that comes for reporting, every automation will have the associated name with it. This is meaningful because different deployments have an identification of what's happening, right? We see that people are interested in what's happening and what deployments they can identify, you know, what's running, where it's running, you know, who does it belong to, and so on and so forth. So deployments, you know, are a consideration of...

01:21:28 - 01:22:31 Unknown Speaker:

Generally, what infrastructure it's running in, that's a technical thing, but understanding that your organization is running in a particular deployment is meaningful in a business context, because when you're looking up information about what's happening, you need to look at this automation. Hey, Mitch. Yeah. Great. Not to get too technical, also, but is this just a way of grouping capacity? Mostly it's about how the infrastructure should be allocated to be able to handle the requirements, the non-functional requirements of the automations. I know that's getting a little technical, but every automation has some non-functional requirements like we're talking about. It has a certain number of volume or a certain amount of throughput or data segregation. And we need to make sure that automations are... in an infrastructure or map to an infrastructure that can handle whatever those non-functional requirements are.

01:22:31 - 01:23:18 Unknown Speaker:

Once we determine that an automation should be in a particular deployment, that is pretty much a permanent decision. Technically, can they be moved from one deployment to another? It's technically possible, but it never would happen without you know, a lot of negotiation and discussion. So the reason I say this is because there's been a lot of conversation about, well, what if you guys arbitrarily change our appointments? What happens to our reports, right? We don't, we consistently see what's happening across the life cycle of these things. And it turns out that we don't do those changes, right? And if we did have to, it would be something that we would raise and everybody would be aware and we would make sure that whatever. Reporting needs were associated with it were met before we made those changes.

01:23:20 - 01:24:19 Unknown Speaker:

Makes sense and just follow up to that. It sounds like there is no limitations, but just from the original limitation within a deployment, say, like volume triples, quadruples over time, there is no limitations for that. There so we can so with our cloud resources we can scale the infrastructure in a way that it can grow with the volume of the automations as they grow. But if something were to be a meaningful change, for example, today we're doing 10 and tomorrow we're going to do a million, right? We'd want to know that so that we could make sure that we mitigate any potential risks there. Would you scale that deployment idea or move it to a bigger deployment idea? The last thing, a lot of things are very dependent.

01:24:19 - 01:25:29 Unknown Speaker:

There's a lot of science there, but also a little bit of art there. It really depends on the circumstance, but we have both things available to us. We can scale the deployment if we want to. Or we can say allocate a new deployment if it makes sense, or we can spread the volume across multiple deployments if that makes sense. So there's a number of different ways to solve these kinds of problems if the circumstances change in a meaningful way. Thank you. Extend it tomorrow or to cover the rest of the topics and pick up tomorrow's agenda? Yeah. So, you know, we're talking over Slack. I think what we'll do tomorrow is right now we're scheduled to start at 9.30 Eastern is our current schedule. 9.30 Eastern is what the schedule is. We can start at 9 instead.

01:25:32 - 01:26:07 Unknown Speaker:

We'll add an extra half-hour. We'll start at 9. Drew Bud will come and we'll finish up your material, first part. And then Merle will start with his material, second part, at 9.30. In the meantime, Merle and I will address any... Thanks, everybody. Thanks, everyone. Thanks.2026-01-25 Creating a New BPM/RPA Script Demo

Creation Time: 2026/1/25


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-25 12:58 (Duration: 54 seconds)
  â€¢ Location: No location mentioned
  â€¢ Attendee: No attendee names mentioned


ðŸ“’Meeting Outline


Creating a new BPM / RPA script demo

  â€¢ Demo setup The presenter confirmed this segment follows prior creation of RPA scripts. The intention is to quickly demonstrate creating a new BPM (Business Process Model) starting from an existing "start" node/task.

  â€¢ Adding and linking a task The presenter showed how to add a new task by selecting the task icon. Clicking the task icon automatically creates and connects an arrow from the start node to the newly added task (pinning the task to the start).

  â€¢ Defining task details After linking the new task to the start, the presenter began to define the task properties (no specific properties were enumerated in the recording; speaker began to describe but did not finish).


ðŸ“‹Overview

  â€¢ The demo is a continuation of RPA script creation work.
  â€¢ A new BPM creation was demonstrated starting from a "start" node.
  â€¢ Adding a task is done via the task icon; the system auto-connects the start to the task with an arrow.
  â€¢ The process for defining task details was initiated but not completed or specified.


ðŸŽ¯Todo List

  â€¢ Presenter / Demo owner:
    
    â€¢ Complete definition of the new BPM task (no deadline given)
    â€¢ Provide a follow-up/demo that details the task properties and remaining BPM steps (no deadline given)


Transcription

00:00:00 - 00:00:49 Unknown Speaker:

As well as your creation of the RPA scripts, right? So we'll just quickly go and demo doing a creating a new BPM. So we have a start task here right from here, let's say, um, you want to go ahead and add a new Uh, pin the task to this start right? So we can easily go ahead and click this uh task, uh icon, which will go ahead and automatically put an arrow that will connect the start to this task. So here we can go ahead and, um, define our tasks, so let's say.2026-01-26 BigQuery Access and Data Management Best Practices

Creation Time: 2026/1/26


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-26 13:06 (Duration: 2731 seconds)
  â€¢ Location: No location mentioned
  â€¢ Attendee: No relevant content mentioned


ðŸ“’Meeting Outline


Access & Onboarding to BigQuery

  â€¢ Access levels The meeting described three BigQuery access levels:
    
    â€¢ Project access level: Grants access to BigQuery as part of the GCP console. Typical approval time ~4 hours after requesting via Identity IQ (IIQ).
    â€¢ Data reader access level: Allows read/analysis access to AXP Lumi (foundational tables). Typical approval time ~2 business days after IIQ authorization.
    â€¢ ONCOP / PII access: Special elevated access for sensitive data with column-level restrictions to protect personally identifiable information; only authorized personnel should have this access. Emphasis was placed on requesting appropriate access before exploring data.

  â€¢ How to access the BigQuery web console Step-by-step path was provided:
    
    â€¢ Go to Lumi portal: lumi.exp.com
    â€¢ Select BigQuery on the opening screen
    â€¢ Click "Go to BigQuery" then "Continue"
    â€¢ Log in to GCP BigQuery using Amex credentials and complete Okta verification
    â€¢ After successful login, you'll reach the BigQuery initial screen but must have the required approvals to explore. It was noted that detailed access instructions will be emailed once approvals are granted.


BigQuery Data Management Concepts

  â€¢ Datasets, tables, and collation Datasets are top-level containers used to organize and control access to tables and views. Collation settings can be configured at table or dataset level and are useful for analytical users.

  â€¢ Query modes and caching BigQuery supports interactive and batch queries. Batch queries are queued and run when resources become available. Query results are cached for roughly 24 hours â€” re-running the same interactive query within that window can use the cache to return results faster.

  â€¢ SQL capabilities BigQuery supports SQL-based scripts, stored procedures, user-defined functions, and table functions. These features were noted as part of data management and querying capability.


Best Practices: Data Load, Table Design, Query Optimization

  â€¢ Data load strategies
    
    â€¢ Handle very large tables (especially >20 TB) by breaking them into manageable chunks.
    â€¢ Snapshot/delta loads (daily deltas of ~5â€“10%) are best applied via MERGE from staging to main tables.
    â€¢ For full-refresh loads where incoming volume is similar to existing data, either TRUNCATE & LOAD or MERGE is recommended.

  â€¢ Table design
    
    â€¢ For tables >1 GB, use partitioning and clustering to improve performance and cost efficiency.
    â€¢ BigQuery supports partitioning by DATE, TIMESTAMP, and INTEGER types â€” choose based on data characteristics.
    â€¢ When clustering, consider query patterns and choose columns accordingly. Multiple columns can be used for clustering.

  â€¢ Query optimization
    
    â€¢ Select only necessary columns; in inner queries prefer selecting essential fields. Use SELECT EXCEPT to exclude unnecessary columns when many are returned.
    â€¢ LIMIT does not reduce read cost â€” it only restricts returned rows.
    â€¢ Join strategy:
      â€¢ Join larger tables first and reduce data before subsequent joins. Manual ordering by table size (largest first, then decreasing) is recommended; BigQuery may reorder under certain conditions but not always.
      â€¢ Aggregations should be performed as late as practical but used to reduce data sent to joins when beneficial.
    â€¢ WHERE clauses:
      â€¢ Place the most selective/â€œinterestingâ€ expressions first to avoid expensive evaluations.
      â€¢ When fetching latest records, use array_agg patterns for efficiency.
      â€¢ Avoid expensive expressions (e.g., LIKE on entire column) where possible; narrow the expression to target subsets (example given: apply to specific user content rather than whole column).

  â€¢ Reference materials A Confluence page was recommended for more best practices and deeper guidance.


ðŸ“‹Overview

  â€¢ Project access, data reader access, and ONCOP/PII access are the three access levels; each has different approval times and permissions.
  â€¢ Access path to BigQuery: lumi.exp.com â†’ BigQuery â†’ Go to BigQuery â†’ login with Amex + Okta verification; approvals required before exploration.
  â€¢ Datasets organize tables and views; collation can be set at dataset or table level.
  â€¢ Use interactive or batch query modes; query results cache for ~24 hours.
  â€¢ BigQuery supports scripts, stored procedures, UDFs, and table functions.
  â€¢ For large tables, prefer partitioning and clustering (partition by date/timestamp/integer); cluster based on query patterns.
  â€¢ For heavy data loads: use MERGE for deltas, TRUNCATE & LOAD or MERGE for full refreshes.
  â€¢ Query optimization principles: select minimal columns, careful join ordering, defer aggregation where appropriate, and structure WHERE clauses to avoid expensive operations.


ðŸŽ¯Todo List

  â€¢ Access requester:
    
    â€¢ Submit access request in Identity IQ for appropriate BigQuery level (project / data reader / ONCOP-PII), time: as soon as possible (approval times: ~4 hours for project, ~2 business days for data reader).
    â€¢ Confirm receipt of approval and await access instruction email, time: upon approval.

  â€¢ Data owners / Engineers:
    
    â€¢ Review large tables (>20 TB) and propose partitioning/clustering plan (deliverable: partition/cluster design document), time: 2 weeks.
    â€¢ Implement MERGE workflows for daily delta loads from staging to main tables (deliverable: ETL/SQL scripts + runbook), time: 3 weeks.

  â€¢ Analytics / SQL developers:
    
    â€¢ Audit commonly used queries to apply best-practice optimizations (select minimal columns, order joins by size, use array_agg for latest-record patterns), time: 2 weeks.
    â€¢ Update queries to avoid full-column expensive expressions (e.g., broad LIKE) and use targeted expressions, time: 2 weeks.

  â€¢ Documentation owner:
    
    â€¢ Publish consolidated BigQuery best-practices and onboarding steps on Confluence, including access request procedure and Okta/Amex login steps (deliverable: Confluence page), time: 1 week.

If any attendee needs clarified assignments or to claim ownership of the above items, please respond with name and the specific item(s) to be owned.


Transcription

00:00:01 - 00:03:52 Unknown Speaker:

Big query This video. We will show you the different access levels, where to access the platform, and some data management elements. Let's get into it access levels. First. Things first, you'll need to gain access to the Google Cloud Platform console. This is your ticket to use BigQuery as part of the GCP console. So let's now explain the three access levels for BigQuery. Once granted project access level, users can access the BigQuery service as part of the GCP. Cp console it usually takes about four hours to get approval after making a request through identity iq. As for data reader access level, this allows users to access AXP Lumi, also known as foundational tables. You'll be able to read and analyze the data stored. In these tables. It typically takes around two business days to get approval after your request is authorized in IIQ.

00:03:53 - 00:08:07 Unknown Speaker:

Now, if you're dealing with sensitive data, you'll need ONCOP and PII access. This is a special level of access that provides for. What are you talking about? What are you doing? And column level restrictions. It's all about ensuring that sensitive data, like personally identifiable information, is well protected and only accessible by authorized personnel. Access BigQuery web console. Ready to git? Started with BigQuery? Head over to the Lumi portal by typing in your web browser, lumi.exp.com. Once logged in, select BigQuery on the opening screen. Then click on the Go to BigQuery button. Press Continue, and log in to GCP BigQuery using your Amex credential. Don't forget to verify yourself using Okta Verification. And voila! You're at the BigQuery initial screen. But remember... You can't start exploring until you've requested and received approval for the appropriate access level.

00:08:08 - 00:09:56 Unknown Speaker:

Don't worry, we'll send over all the access instructions via email once you're approved. BigQuery Data Management Now, let's talk a bit about BigQuery. Data management. It's all about handling, organizing, and maintaining data in the most efficient and effective way possible. You'll work with things called datasets, which are essentially top-level containers used to organize and control access to tables and views. Think of them as a way to group related data together. Within BigQuery, you'll learn about collation settings, which can be changed at the table or dataset. Level 1 required. This can be quite handy for you analytical users out there. And let's not forget that you'll have the option for interactive. Or a batch query. The batch query option queues up the queries and runs them when resources become available. Leslie, about scripts and the query data cache.

00:10:15 - 00:12:12 Unknown Speaker:

BigQuery supports SQL-based scripts, stored procedures, user-defined functions . And table functions. Plus, if you rerun the same interactive query within approximately 24 hours, the data in the cache will be used. This allows the system to retrieve the data more quickly by not having to fetch or recalculate the data again. Before we wrap up, let's take a moment to review what you've learned today. Successfully navigating BigQuery is all about understanding how to access the platform, the different levels of access, and key data management elements. It's important to remember that while gaining access may take some time, it's a necessary step to ensure the right individuals have the correct permissions. Do you recall the three different types of permissions? You should be thinking about project access level, data reader access level, and on-go PII access.

00:12:12 - 00:15:13 Unknown Speaker:

Once inside, you'll be able to explore the wealth of data and features that BigQuery offers, From working with data sets to utilizing scripts and managing query data. By the end of this video you are now able to identify the three different types of access levels for BigQuery web console, outline the steps to access BigQuery, and understand data management within BigQuery. Keep expanding your knowledge like this. And that's it. You're now ready to get started with BigQuery. Keep exploring by checking out the best practices for BigQuery video. Hello again. This video purpose is to help elevate your skills in BigQuery. Prior to watching this video, you've already taken your first steps understanding and accessing BigQuery. Now we're diving into the heart of efficiency. The best practices for data load, table design, and query optimization.

00:15:15 - 00:18:23 Unknown Speaker:

Data Load In our data load journey, we often encounter tables that are overwhelming. Especially those exceeding 20 terabytes are trickier. We break down these tables into manageable chunks snapshot data. Those daily deltas that are just five to ten percent of the actual table are like small puzzle pieces. We handle these. In the merge option, it's our data traffic controller that smoothly directs these pieces from the staging area to the main data repository. Full refresh data usually comes in similar volumes to our existing data. In these cases, we either opt for a fresh start with truncate and load or use our dependable merge statement. Table design. Now, let's talk about designing our data building, or table design. For buildings larger than 1 gigabyte, Use partitions and clusters to help you move around efficiently.

00:18:24 - 00:20:38 Unknown Speaker:

Do you know BigQuery allows us to partition with datetime stamp edit? Did your data types well? This offers us the flexibility we need when deciding to cluster by several columns. We consider our query patterns first, query optimization. Now, let's talk about designing our data building or table design. For buildings larger than one gigabyte. We use partitions and clusters to help you move around efficiently. Do you know BigQuery allows us to partition. In our data load journey, we often encounter tables that are overwhelming, especially those exceeding 20 terabytes. Our trick here? We break down these tables into manageable chunks. Snapshot data, those daily deltas that are just 5 to 10% of the actual table, are like small puzzle pieces. We handle these using the merge option. It's our data traffic controller that smoothly directs these pieces from the staging area to the main data repository.

00:20:39 - 00:22:11 Unknown Speaker:

Full refresh data usually comes in similar volumes to our existing data. In these cases, we either opt for a fresh start with truncate and load or use our dependable merge statement. Table Design Now, let's talk about designing our data building, or table design. For buildings larger than 1 gigabyte, we use partitions and clusters to help you move around efficiently. Do you know BigQuery allows us to partition with date timestamp and integer data types? Well, this offers us the flexibility we need. When deciding to cluster by several columns, we consider our query patterns first. Query optimization. And finally, Query optimization. Now, let's talk about designing our data building, or table design. For buildings larger than 1 gigabyte, we use partitions and clusters to help you move. Around efficiently, do you know?

00:22:11 - 00:32:40 Unknown Speaker:

BigQuery allows us to partition with date, timestamp, and integer data types. Well, this offers us the flexibility we need when deciding to cluster. By several columns. We consider our query patterns, first, query optimization, and finally, we're on to query optimization. We aim to select only the essential columns, particularly in inner queries. If we have many columns to return, we use select, except to exclude the unnecessary ones. Remember, the limit clause doesn't reduce the crowd, it merely controls who we see. We join the large tables first and reduce data before performing joins. Thank you. Aggregation should be your showstopper move. We use it as late and not often as possible. Performing aggregations early reduces the amount of data. This is sent to the join resulting. In a smaller data set and improve performance speed.

00:32:41 - 00:35:49 Unknown Speaker:

When using joins consider the order which you are merging the data. It is recommended to order your join tables largest to smallest. The best practice is to manually place the largest table first, followed by the smallest, and then by decreasing size. Only under specific table conditions, BigQuery automatically reorder optimize the table based on its size. Our where clauses should start with the most interesting expressions, and when fetching the latest record, we use array act for efficiency. BigQuery soon. That the user has provided the best order of expressions in the where clauses and does not attempt to reorder expressions. The optimized code example you are seeing now in screen is faster because it doesn't execute the expensive like expression. If you don't want to do it, you can do it on your own.

00:39:21 - 00:42:38 Unknown Speaker:

On the entire column content, but rather only on the content from user, Bob. And that's it for this video. You've come a long way. By the end of this video, you are now able to understand best practices for table design, query optimization, and data load within BigQuery web console. These best practices are your keys to unlocking a smoother, more efficient data journey. If you would like to learn about more best practices, Please refer to the Confluence page. Together let's redefine the way we work. All right.2026-01-26 BigQuery Data Transfer and PII Handling Overview

Creation Time: 2026/1/26


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-26 17:33 (Creation Time), Duration: 4549 seconds
  â€¢ Location: Screen share / remote (presentation + demo on BigQuery and Hadoop Gold)
  â€¢ Attendee: [Not specified in recording â€” presenter and attendees on the session]


ðŸ“’Meeting Outline


BigQuery import/export overview and CLI stopgap solution

  â€¢ Out-of-box capabilities and limitations The presenter reviewed BigQueryâ€™s native UI import/export options (upload from local, save query results to CSV, etc.) and highlighted existing size limits: small-file limits (10 MB export, ~10â€“100 MB import depending on UI). These limits make BigQueryâ€™s native UI unsuitable for larger analytical transfers.

  â€¢ Stopgap solution using CLI + Hadoop Gold To support larger transfers (up to ~10 GB), the team developed a CLI-based workflow that moves data between BigQuery and usersâ€™ local machines via the Hadoop Gold cluster. Flow is:
    
    â€¢ Export: BigQuery -> encrypted export -> decrypt on Hadoop Gold -> CSV on Hadoop Gold -> download to local via FileZilla/WinSCP.
    â€¢ Import: Local -> upload CSV to Hadoop Gold -> CLI push from Hadoop Gold into BigQuery. The solution is intended for analytical users and personal datasets.

  â€¢ Security and PII considerations The presenter emphasized that the current demo/workflow is for non-PII data. Import/export of PII requires additional steps (encryption/enclave workflow): creating an Excel that enumerates PII columns and tags, passing that path to the CLI config, and following the PII-specific flow. PII support is not enabled by default and has additional limitations.


Configuration, prerequisites and files required

  â€¢ Configuration file details A shared config file (placed in the userâ€™s home directory, e.g., ads.home//LumiConfig) must contain:
    
    â€¢ user email
    â€¢ WireSafe token (unique per user; provided upon onboarding/analytic project access)
    â€¢ Cornerstone queue name (queue on Gold server)
    â€¢ GCP BigQuery project ID This single config file is used for both import and export operations.

  â€¢ Hadoop Gold file placement Users must create a destination (for export) or source (for import) folder within their personal directory on the Hadoop Gold server (e.g., /LumiImport or LumiDestination). Use FileZilla / WinSCP to upload local CSVs to Hadoop Gold prior to an import CLI run, and to download exported CSVs after an export completes.

  â€¢ CSV formatting requirements Files used in the workflow should be CSV with the first record as a header row. Non-PII datasets are supported in the current demo; PII datasets require the PII Excel and a different flow.


CLI script and execution details

  â€¢ Script and main parameters The CLI script used is cfcli.sh. Example parameters discussed for both actions:
    
    â€¢ action: import or export
    â€¢ -conf
    â€¢ -data_location
    â€¢ -table_name
    â€¢ -ingest_method <new | append | overwrite> For PII imports/exports additional parameters are required (e.g., PII columns Excel path, PII flags).

  â€¢ Import flow (demo) Steps demonstrated:
    
    1. Create LumiConfig with email, WireSafe token, queue, project.
    2. Place CSV into Hadoop Gold under a personal folder (e.g., LumiIMP).
    3. Execute cfcli.sh with action=import, conf path, data_location, table_name, ingest_method=new.
    4. Respond to CLI prompts (e.g., header row Y/N, PII Y/N). For demo: header = Yes, PII = No.
    5. The CLI triggers a job; presenter received a â€œjob initiatedâ€ notification and later an email from lumidatatransferalerts@aexp.com [lumidatatransferalerts@aexp.com] confirming completion.
    6. The user confirmed the table appeared in their personal BigQuery dataset and verified data (noting date handling caveat).

  â€¢ Export flow (demo) Steps demonstrated:
    
    1. Ensure table to export is in personal dataset.
    2. Ensure LumiConfig present and create a Lumi destination folder on Hadoop Gold.
    3. Execute cfcli.sh with action=export, conf path, data_location, table_name.
    4. Respond to CLI prompts (PII = No for demo).
    5. CLI returns a request ID and â€œjob in progressâ€; user receives email with a decrypt command.
    6. The user runs the provided decrypt command on the Gold server; job completes and CSV appears in the specified Hadoop Gold folder.
    7. Download file from Hadoop Gold to local via FileZilla and verify contents.

  â€¢ Notifications and channels Users receive status emails from lumidatatransferalerts@aexp.com [lumidatatransferalerts@aexp.com]. Slack channel exists for queries and support during the operation.


PII-specific flow notes (preview)

  â€¢ Extra steps for PII data If data contains PII, the workflow requires:
    â€¢ Creating an Excel file listing PII columns and PII types and specifying SDE tags as required.
    â€¢ Adding the PII Excel path as a parameter (e.g., PII_columns_Excel) to the CLI command.
    â€¢ Following the PII-specific prompts and encryption/decryption steps. Detailed PII flow will be covered in a follow-up video and document on Navigator.


Demos and validation observations

  â€¢ UI import/export small-file demo Presenter demonstrated BigQuery UI â€œAdd -> Local file -> Uploadâ€ and â€œSave Results -> CSVâ€ for small datasets, showing auto-detect schema and immediate table creation / download â€” reiterating UI limits for larger files.

  â€¢ CLI demos for both import and export Presenter showed full end-to-end import and export using the CLI; confirmed email notifications, job IDs, and presence of resulting files/tables. Noted minor parsing/date formatting differences on import that users should check.


ðŸ“‹Overview

  â€¢ The BigQuery UI supports small file import/export but has strict size limits (10 MB export; ~10â€“100 MB import), which are inadequate for larger analytical datasets.
  â€¢ A CLI-based stopgap solution leverages the Hadoop Gold cluster to transfer data up to ~10 GB between BigQuery and usersâ€™ local machines.
  â€¢ A single configuration file (user email, WireSafe token, Cornerstone queue, GCP project) is required and reused for import and export.
  â€¢ Workflow steps: place file in Hadoop Gold (for import) or create destination folder (for export) -> run cfcli.sh with appropriate parameters -> await email notification -> decrypt (for export) -> download/upload via FileZilla.
  â€¢ PII handling requires additional steps (PII columns Excel, tags, encryption); current demo covered only non-PII transfers. Detailed PII process to follow in next session and navigator documentation.
  â€¢ Slack channel and lumidatatransferalerts@aexp.com [lumidatatransferalerts@aexp.com] are the support/notification channels for the process.


ðŸŽ¯Todo List

  â€¢ Presenter / Data transfer team:
    
    â€¢ Publish the detailed step-by-step document (including sample cfcli.sh commands and config examples) to Navigator, due: ASAP
    â€¢ Prepare and publish the PII-specific mini video and documentation explaining Excel PII schema, SDE tags, and secure workflow, due: next session (date TBD)
    â€¢ Provide example LumiConfig templates (redacted) and sample decrypt commands for users, due: ASAP

  â€¢ Analytical user / attendees:
    
    â€¢ Create personal Lumi folder on Hadoop Gold (e.g., LumiImport or LumiDestination) and confirm access, time: prior to first transfer
    â€¢ Obtain WireSafe token and confirm LumiConfig details (email, queue, project) placed in home directory, time: prior to first transfer
    â€¢ For non-PII import: upload CSV with header to Hadoop Gold and run cfcli.sh with action=import (sample command provided in session), time: as needed per transfer
    â€¢ For non-PII export: ensure table exists in personal dataset, run cfcli.sh with action=export, run decrypt command from email, then download CSV via FileZilla, time: as needed per transfer
    â€¢ Validate data formats (dates and types) after import/export and report issues to the Slack channel, time: immediately after transfer

  â€¢ Support / Ops:
    
    â€¢ Monitor and respond to Slack questions during user transfers; ensure lumidatatransferalerts email notifications are delivered correctly, time: ongoing
    â€¢ Verify and confirm limits and capability statements (e.g., maximum supported size up to 10 GB) and update public documentation if needed, time: ASAP

If any attendee needs the exact example commands, sample LumiConfig template, or the PII Excel template used in the demo, request them in the Slack channel or reply here and the presenter/support team will share the artifacts.


Transcription

00:00:05 - 00:03:59 Unknown Speaker:

BigQuery import export using CLI. Welcome to Lumi import export functionality mini video session. So in this video session we will be discussing how we can import the data to the BigQuery or how we can export the data from BigQuery and analyze the output. So let me start my screen share. So I'm presenting here one PPT. So we'll discuss about this first. We'll go through the deck and we will implement the import feature and export feature. So this document is built to explain the users what are the import and export capabilities available on GCP BigQuery. . . . I'm sorry. You don't believe me? What do I do? What do I do? Um um. I'm going to lay a guy and he's going to get some of you on him.

00:06:18 - 00:15:00 Unknown Speaker:

So there is out of box import export functionality, which is available. And as you can see here, it is mentioned that it has a data size limitation of the 10 MB. So analytical users, when they make use of the import and export functionality, they expect the bigger volumes of the data to be able to export and import. So that's why we have developed this stopgap solution using command line interface. So with this solution, analytical users will be able to securely transfer the data up to 10 GB from BigQuery or to the BigQuery. So let's move ahead and see how we are enabling the data transfer using CLI. So if you see here in the schematic, we have mentioned this Lumi at the right. Thank you. And this is corresponding to BigQuery.

00:15:00 - 00:15:53 Unknown Speaker:

So if you have the data in the BigQuery and you would like to export more than 10 MB of the data, then you can make use of the command line interface using Qt or the terminal. You can connect your Hadoop Gold in the cornerstone and using CLI, you can export the data into the Hadoop Gold. Once it is available in Hadoop Gold in the CSV format, you can then pull it into your local device, which is given to you by American Express. The similar operation we will be doing while importing the data. First, you can push the file which you would like to import into the Hadoop Gold cluster in the file system. Then using CLI, you can execute the command to push the data into the BigQuery.

00:15:54 - 00:16:45 Unknown Speaker:

So we are making use of the Hadoop Gold environment to facilitate our import and export. So moving ahead, we will be discussing the import first. Well, focus on the import, how we can import the file. So if you see here, BigQuery has the out of the box feature to upload the data from your local system to the GCP. So we have to... use this add button. We need to click on this local file, we need to select the option as upload. We need to describe, we need to provide the details like data set and table, and we can upload the data. We'll see the demonstration of it, how we can do it. And in the next slide. We have explained how we can import the data into the BigQuery from our local system.

00:16:45 - 00:24:47 Unknown Speaker:

So for that we need to have the data. Yeah, bitch. We would like to import in the CSV format uploaded onto the Hadoop Gold cluster, the Hadoop Gold cluster which you have access to. So you see here, we have placed a source on the Hadoop Gold and then we execute the import command. This import command we are executing using this particular shell script, cfcli.sh. And once we perform this activity, Data from the CSV file will be available in our personal data set in the BigQuery project. So how the steps will execute is, the first step is you need to have the configuration file to perform this activity which will be placed into. You're homely. Directory So, as you can see at ID and Home and the personal directory, the config file is stored and what all details it contains is, it has described the screenshot.

00:24:47 - 00:25:38 Unknown Speaker:

So, user email address you need to send, set it as per your ID, then Wire Safe token it will be shared with you. Once you have access to the Lumi, then the queue name, the queue name of the Cold server. You have to mention it over here, and you need to mention the GCP BQ Project ID. You need to mention the project in which you would like to import the data, or export the data. So you have the user email, YSF token, queue, and project name. These four parameters, or these four attributes you have to declare in the configuration file, and this configuration file will be common to both import and the export operation. So, first step, as it is mentioned, we need to create the configuration file, then we need to create the source file.

00:25:38 - 00:26:31 Unknown Speaker:

In the Hadoop Gold Cluster so you can see here. We have appended a screenshot which indicates how we can contain the file, so here we are creating the CSV file. So we have we need to have the Cornerstone folder and the CSV file, which is to be imported, available into the Hadoop Gold Cluster. Then we will be running the CLI command in the third step. So as you can see, first and second step, independently we can perform. Once all of this, both these operations are done, we can come to step number three, wherein we are executing the command on the command line interface. What command we're executing, it is given at the top. This is just a sample command, so you need to click it. For example, here we are... Executing this command, save CLI.sh.

00:26:31 - 00:27:22 Unknown Speaker:

Action attribute, we are setting it to import. Hyphen conf, here we are setting our configuration file. Like here, whatever path of the configuration file, we are mentioning it over here. Hyphen data underscore location. So here we are specifying the location where we have put our CSV file. So LumiIMP is the location where we have. Created our file, so we have mentioned it over here. Hyphen Table underscore name What is the table name we would want? Uh, for when? We for the uh? Importing the data, like whatever the data we have in this file. If we would like to import in the big query, in what table we want to import it. That we have to specify and in just method, we have to specify it as new.

00:27:23 - 00:28:20 Unknown Speaker:

So once we perform this step, once we execute this command, we will get the notification that the job is in progress and the import process is initiated. Once basically the command execution will end over here and after some time you will receive the email that it is completed. And once you receive this email, you can go to your BigQuery console and find that within your personal dataset of the project which you have mentioned in the configuration file, the table is created. The name of the table will be same as the name of the table which is mentioned by you in the command. Like for example here, hyphen table underscore name, we have mentioned import as one. And similarly, if we go to the BigQuery after all the operations are done, the table will be created with import S1.

00:28:21 - 00:29:26 Unknown Speaker:

So let's come out of the slideshow and see how it can be done. So here, folks, if you see here, I have let, and similarly, if we go to the BigQuery, after all the operations are done, the table will be created with import S1. So let's come out of the, slideshow and see how it can be done. So here, folks, if you see here, I have connected to my personal dataset and you can see that the LumiConfig is the file which I have created. If I open it, you will see my configuration file and yeah, you can see my email address and also I have specified my wire safe token and other details in that file. As I was explaining to you, I have query queue name and project name as well.

00:29:27 - 00:30:21 Unknown Speaker:

Please note that wire safe token will be different for the different users. So do not try to use the same one which I'm showing over here. So once I have this, let me show you how I have placed the file on the FileZilla. If you see here, I have connected to my gold server, Hadoop gold server. And within this location, you know, AXBPU analytics CSED on this path. I have created Lumi import folder within my personal folder. And in that folder, I have added this file from my local system because I want to import this data. What data does it contain? Let me quickly show you. So if we just open it. This is the CSP file. It contains the three columns. So first column is a date and first column is date.

00:30:21 - 00:31:13 Unknown Speaker:

Second column is a training title. What all training we are planning to have on that day and duration of the training. This is just a sample data which I have created for testing purpose or the demonstration purpose. So first, okay, so this CLI setup is present and command also I have prepared if you see here to execute it. And have the table available in the BigQuery. So I'm calling this save CLI command. Action is import. Then configuration file, I have put the path of configuration file, which I have created. Data location, where I have kept the data, I am pasting it over here. Table name, what is table name going to be? This is import underscore CLI that I have mentioned over here. And ingest method is going to be new. So we are creating a new table.

00:31:13 - 00:32:10 Unknown Speaker:

So we have indicated that using this. So we can execute it now on the terminal. But before that, let's try to see how we can use the out-of-box functionality import. So for example, see here, As we were showing in the pictures or the screenshots, here we have this add button available, we can click on Add, we can click on Local file. And you know, we have. We get multiple options, so we need to select upload, we need to browse the file. Okay, so test file of Import I'm going to open it, format is CSV data set I am going to create with my own data set, I will name it as Import UI. So this is going to be my table name, so schema of the table, like the columns and data types of it.

00:32:10 - 00:32:56 Unknown Speaker:

Either you can hit Auto Detect Check Auto Detect button, or you can manually type in the field name, data type, the mode, and other details. So I have a simple table, so I will click Auto Detect over here and I will hit this Create Table button. And once I create this, it will create a load job to load the data from my local file into the table. And if you see the data is now available, the table is now available. Within my data set. And even in the preview option, I am able to see the details which are being imported to my BigQuery. So now let's see how we can accomplish the same thing using the command line interface. Again.

00:32:56 - 00:33:47 Unknown Speaker:

I would like to remind you that for import, we have the data size limitation of 10 MB or 100 MB for the import and 10 MB for the export. We have these limitations. So because I'm using the small data, I can use it to directly upload it into the BigQuery. But let's say you are dealing with the data more than 100 MB, then you cannot do it directly. You need to make use of this stopgap solution, which is developed by the team. So let's copy the data and see if it gets imported into the data set or no. So just verify if all of the details are correct. Like path, where we have this command which we are planning to execute, then we have this action, we have. Import action is basically import or export section is import for us now.

00:33:47 - 00:34:52 Unknown Speaker:

So I have specified it configuration file path data location where our data is located, that is that path table name of the imported imported table for imported data data which we would like to import. And in this method, so everything is new, then we can just copy it. You can go to terminal. And we can can just paste the command which we have prepared. So I can see it is getting pasted. So we'll wait until we get, yeah, the final new section. And if I hit enter, so it will ask the question whether the input files have header with the column name. So answer is Yes. I have the header, so I will type in Yes. I hit enter. My input data contains personally identifiable information, so we do not have any personal identifiable information.

00:34:52 - 00:35:46 Unknown Speaker:

So I am hitting it, no. Please note that if you hit the yes, then this solution will again have some other limitations. So as of now, we are not supporting the PII data import because that needs to be encrypted. So as of now, we'll just say no, and anyways, we do not have any data. So we'll hit this button. And it is saying that we have to wait until the import job is in progress. And also we have we are getting the Slack channel, which we can use to post our query in case we need some SSD. One more thing to note here is that guys, once you open the terminal or putty, you need to SSH into your gold Hadoop server. So like I said, I have SSH into the gold server and then I performed this operation.

00:35:47 - 00:36:53 Unknown Speaker:

So now this job is in progress. So as you can see folks now that I have got the notification that import process is initiated and I will receive the email from Lumi data transfer alert at the exp.com when the data is available. So I will wait for that email to come and then I will check the availability of the table in my personal data set. So team as, uh, we have got the notification, uh, the query import process is initiated and we were waiting for the email. So now I have received that email. If you see here. I have received this email for my request ID, which was which I could see it over here like 989. And it is saying that request to import the data is completed and.

00:36:54 - 00:37:50 Unknown Speaker:

Now if I go to the console and if I refresh this, touch the content, I can see that import CLI is created over here. And if I see the data in it, it is, yeah, so we have got the data. So it has appended some additional details, but I think because it was a date, so we need to be a little bit careful about it when we have the date. But now the data is available and imported in the BigQuery. So we discussed how we can import the data and in a similar way, we can export the data as well. But this method we discussed specifically for the data which does not have PII. Suppose we have a data with the PII and we would like to import it, then what we can do?

00:37:51 - 00:38:45 Unknown Speaker:

So that is explained in this document, which will be uploaded on the navigator. You can go through it and reach out to us in case of the assistance required. Or in the Slack channel, you can mention your query. We have also included some import command node like the wire safe token. What it is? So, if you have access to the analytical project, and as we discussed, because import and export functionality is utilized by analytical users, we are enabling this for analytical projects. And once you have access to the project, you can work with, you can get this wire safe token. And while using the Home directory, you need to mention path like ads. Home files to be imported should be a CSV file, first, record should be the header, and other details are mentioned over here.

00:38:45 - 00:50:00 Unknown Speaker:

So even for the ingest method, there are options available, like we are using new, but if when, we can make use of the other option based on our requirement. We are importing the data in the already existing table. And if you want to overwrite the data, we can make use of the overwrite option. Or if you want to append, then we can make use of the append option. So this is something which we need to understand and follow. And suppose you have the data with PII and you would like to upload it, then you will have the additional steps. The six steps are common. Two steps which are independent from each other there. Thank you. Another step wherein you need to create the Excel file and you need to mention all the PII columns and the PII type.

00:50:00 - 00:51:44 Unknown Speaker:

So SDE tag of the corresponding SDE, you have to mention it like this. And you need to also mention the Excel path in the parameter. So if I bring it in the presentation mode, you can see that ingest method would be new, like either new append or overwrite VKI. Saying it as a new and iPhone PII underscore columns Underscore Excel this attribute. We need to fill it using the Excel file which we have created with the PII columns. We need to define it, including the path. Then we can say that yes. Our data contains first the identifiable information, and once it is completed, we will get the email. Similar to the process which we saw while importing the content to the BigQuery. So folks, that's it for the import using the CLI.

00:51:44 - 00:54:51 Unknown Speaker:

We will prepare the mini video for exporting the data out of the BigQuery and you will get the entire idea of how the import and export process works. Thank you so much. Hello everyone, welcome to data transfer using BigQuery mini video session. In this mini video session, we will demonstrate the export operation from BigQuery to Hadoop server. So in the previous mini video session, we discussed about the import operation, how we can import the content from Hadoop server to BigQuery. And today we will, in this session, we will discuss about how we can export the content which is there in BigQuery on to Hadoop. So let me share my screen and this is the deck so we'll cover we'll see some content on the deck and then we will perform the operations on the BigQuery console about how we can export.

00:55:11 - 00:57:39 Unknown Speaker:

Yes. The content which is there in BigQuery on to Hadoop Core Server. So let me share my screen. And this is the deck. So we'll cover, we'll see some content on the deck and then we'll perform the operations on the BigQuery console. So this execution as well, we will need the command line interface and the Hadoop. So when you... I have to export the data out of BigQuery. BigQuery provides the out-of-the-box capability to export the data out of BigQuery in this case. And whatever result of your query execution. I'll give you 4.45 for two. 3.30 is enough. No, 3.30. Eat it, it's good. 3.30 is enough for me. Is, you can export that result into this Save Results option and choose the CSV file. So how? It was very simple.

00:57:39 - 00:58:32 Unknown Speaker:

So if we go to the big query, okay, so I'm showing the big Query console here. So there is this table import Underscore UI, so if I query this table, okay, and if I select it, if I execute it, it has a sample data. This is a sample table which I imported, uh, from my local system. From the CSP file into the BigQuery, and you can see it has three rows and two columns, so now suppose I want to export this to BigQuery? Provides the out of the box functionality. So I can just, uh, save the data onto my local system in the CSV format. And the data limit, you can see there is a data limit of the 10 MB. So even if I save it, you know, because it has very less data.

00:58:32 - 00:59:30 Unknown Speaker:

So you see, the file is saved over here in my local system and I can open it. Yeah, and I can find that there are three rows and two columns are present. So this way we can export in our local system using the out-of-box options available. But what if your data is more than 10 MB? Let's say you have some 100 MBs of data or you may have some GBs of the data, then how can you export it into your local system? So this solution is designed for the analytical users who export the data. From the BigQuery to their local system or they need to export the data to their local system. So you can use this functionality or this solution to export the data from BigQuery into Hadoop Gold Server.

00:59:31 - 01:00:23 Unknown Speaker:

And from Hadoop Gold Server, you can transfer it to your local system using FileZilla or WinSCP. So if you see here, I'm opening my FileZilla. So on the left-hand side, I have my local system, local file system present. And on the right hand side, I am connected to Hadoop COVID server and I have created my Lumi destination folder in my personal folder here. So whatever export operation we are going to perform from the BigQuery, the data will land in this particular folder. So you need to create this folder into your personal folder wherein the data from the BigQuery will land to perform this operation. Now also, You need to have a configuration file like, I'll just quickly show you. So what I have done is on my terminal window, or, you can say, the putty window.

01:00:23 - 01:04:08 Unknown Speaker:

I have performed SSH operation SSH command into my server, and also you can see that in my personal folder, Ads, home and my personal ID. I have this Lumi Config file present and this Lumi config file, it contains what it contains, the email, it contains. Wireship token, which I have received when I got onboarded to this analytical project, which project, that project? Also you need to mention, and also the Uh, on the stone queue name that you have to mention over here. Let's go over the steps in the deck and then we'll perform those in the BigQuery console. So if, let me enter it into the presentation mode, because, yeah, so. We will be facilitating the export from BIG from this query into Hadoop server using command line interface.

01:04:08 - 01:06:28 Unknown Speaker:

So, we will be executing this command, this script, cfcli.sh and action parameter will be set to export. So, how the queue or how the flow will look like. So, first three are the prerequisites. So, first step here, it indicates the configuration file which I just shown. It contains email, token, on the stone queue and the project from which we are going to import then the one time destination. So, in my case, I have greater than Gumi underscore destination, which I have shown you in my personal folder. And then the GCP export file. This is nothing but the table which we have in our data set, the personal data set. And as I have emphasized in the previous import video session as well, guys, that this utility works with the personal data set.

01:06:30 - 01:07:26 Unknown Speaker:

You need to have the table which you have to export in your personal dataset created in the beginning. Once this prerequisites are satisfied, you can execute the command on the command line interface by setting the action to export. Once it is executed, you will get the message in PuTTY saying that it is running. After some time, you will receive the e-mail which gives you the command to decrypt the data which is available after export. Once you execute that command, the data will be decrypted, execute the command, like this is the seventh step. Once you execute the command, the data will be decrypted and will be available to you on the Hadoop whole server in the CSV format. So this is how the flow looks like. So let's... okay, and this show and we'll execute it.

01:07:26 - 01:08:21 Unknown Speaker:

So, uh, let me show you the table which I would like to import, so see, this is my table import Underscore CLI. This table I have created by. This is how the flow looks like. So let's, uh, okay, and this show and we'll execute it. So, uh, let me quickly show you the table which I would like to input. So see, this is. My table, import underscore CLI. This table I have created by executing the import operation. You have attended the import mini video. I had imported the data from the test file on my local system into the BigQuery. So, it has again these two columns and if you see it has the three record only like the import UI table. So, this table we are going to export to the Hadoop OS server.

01:08:23 - 01:09:13 Unknown Speaker:

For that, I have created the command command I have created, uh, keeping in mind all the parameters which I have to choose. So, for example, here I am executing this particular script, I am setting action to export. Okay, so first parameter is action, I am setting it to export. Hyphen Conf. So here I am mentioning my configuration file, so I have shown you that this is my configuration file. So ID and Home Personal Directory and Lumi Config this can be anything like Lumi Underscore Config or whatever you have created, you need to mention it for configuration. Data Location What should be the data? Where should be the data located after the export? So that we are mentioning over here? In this parameter, data underscore location final parameter is a table name.

01:09:13 - 01:10:04 Unknown Speaker:

What is a table which you would like to export? So import underscore CLI is the table. So that I have mentioned and now I can copy it and I can execute it on the terminal. If I come here, if I paste it, so it is getting pasted. Once it is fully pasted, we can hit enter. And this data set does not contain any PI data, so I am mentioning here no. If you want, if you have PII data, then you need to follow the other process wherein you need to prepare the Excel, and that we will explain in the next video session. But the export command, which currently we are demonstrating, is without PII data.

01:10:04 - 01:11:04 Unknown Speaker:

So as you see, I got the notification that export job is in progress and the request ID is available, and we will receive the e-mail from numedatatransferalerts at the rateaexp.com, and this we will receive once the data is available for us to consume in Rube Gold Server. So, let's wait for the e-mail, and then we will check it again. So team, if you see, we were waiting for email from the LumiData transfer, and we can see that we have received that email. And if we go back to the deck and see the flow that we have received the email with the decrypt command. So if you see here, like, again, I will show you that here on the step number six. So email decrypt command we have received.

01:11:06 - 01:12:14 Unknown Speaker:

What it is saying is email is saying that we can log into Hadoop board cluster and following command to decrypt the file. So let's copy this particular command, go back to terminal, paste the command over here in the terminal and once it is completely pasted, we can execute it and we get the notification that decrypt job is in progress. So once it is done, we can see the exported data in Hadoop Gold Server. As we can see that after around two minutes, the job is completed to decrypt the data onto Hadoop Gold Server. And we have got the notification that BigQuery export process completed and data is now available at the location. So the location which we have mentioned by executing the command within that folder, there is this folder available.

01:12:15 - 01:13:24 Unknown Speaker:

So let's go to the FileZilla and you can see that there is no folder available. But if we refresh it, if I open it again, we can see the decrypted data folder is available. If I double click on it, then I can see that one CSV file is available. This CSV file, I can import it onto my local system. And yeah, it is now available. We can see the confirm it with the date today. And we can go to the folder. And we can see that data is available. And we can, like, file is available, we can open it using Excel and we can see the data is exported because it was very small data actually. We can perform this operation using the out of box BigQuery export feature, but when you are dealing with the data in 200 MBs.

01:13:24 - 01:15:08 Unknown Speaker:

You can use this export functionality to export the data of a table into a file and export the data into file and analyze it onto your local system. Thank you so much folks and I will see you in the next video session on same topic in which we discuss how we can work with the PII data on this data transfer utility. Thank you. Another OTP is coming. Another OTP is coming. It is coming now. It has come twice. It has come twice.2026-01-26 BigQuery Web Console Learning Session

Creation Time: 2026/1/26


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-26 12:29 (Duration: 1892 seconds)
  â€¢ Location: Lumi â€” BigQuery web console learning session (virtual)
  â€¢ Attendee: [Not specified â€” presenter and audience from AMEX / Lumi training group]


ðŸ“’Meeting Outline


BigQuery introduction and purpose

  â€¢ What is BigQuery / web console BigQuery web console is an intuitive web interface within the Lumi/GCP environment used to explore, analyze, and generate insights from large datasets. It enables execution and management of SQL queries, monitoring of query execution, and provides a centralized environment for team collaboration.
  â€¢ Primary benefits Scalability to handle very large datasets; dynamic compute resource management that allocates slots automatically based on query needs; and a collaborative model where project-level datasets are shared among users, facilitating cooperative analysis across AMEX teams.


Key terminology and UI components

  â€¢ GCP Console The top-level Google Cloud web interface through which BigQuery and other GCP services are accessed.
  â€¢ SQL Workspace The web-based SQL editor within BigQuery used to write, execute, and manage queries and scripts.
  â€¢ Projects Workspaces (GCP projects) used to onboard users and organize access to BigQuery resources and other services.
  â€¢ Explorer The navigation pane in the BigQuery console for discovering projects, datasets, tables, and views; useful for browsing schema and data organization.
  â€¢ Datasets Top-level containers within BigQuery used to organize tables and views and to control access at a grouping level.


Features and capabilities of BigQuery

  â€¢ Concurrent queries Supports executing multiple queries at the same time so users can run several analyses without waiting for prior queries to finish.
  â€¢ Time Travel Ability to query table snapshots as they existed at a past point in time for historical analysis and recovery scenarios.
  â€¢ Sessions Represents a sequence of interactions between a user and BigQuery; allows maintaining stateful work flows across queries within a session.
  â€¢ Scripts Support for multi-statement SQL scripts to run multiple SQL statements as a single job.
  â€¢ Job priority: interactive vs. batch Interactive priority for low-latency, time-sensitive queries; batch priority for larger, non-urgent queries that can run when resources are available.
  â€¢ Collation Controls string sorting and comparison rules â€” important for correct behavior across different languages and character sets.
  â€¢ Result caching BigQuery caches query results to speed up repeated queries and reduce compute cost when appropriate.
  â€¢ Clustering Organizes table data into clustered segments to optimize query performance on frequently filtered columns.
  â€¢ Query Scheduler Enables scheduling recurring queries for automated, periodic execution.
  â€¢ SQL translator (HiveQL to BigQuery Standard SQL) Tooling to convert HiveQL queries into BigQuery Standard SQL to ease migration of existing analytics workloads.


Guidance and next steps

  â€¢ Practice encouragement Attendees were encouraged to explore the BigQuery console hands-on and to use Lumi resources (e.g., Lumi Academy) for further learning.
  â€¢ Upcoming content Next session will cover access entitlements and data management in BigQuery (getting started with BigQuery â€” permissions, dataset/table management, and governance).


ðŸ“‹Overview

  â€¢ BigQuery web console provides an accessible, collaborative interface for large-scale data analysis.
  â€¢ The platform scales compute dynamically and supports concurrent work across teams.
  â€¢ Key UI components: GCP Console, SQL Workspace, Explorer, Projects, and Datasets.
  â€¢ Core features that improve productivity: concurrent queries, time travel, sessions, scripts, job priority, collation, caching, clustering, query scheduler, and SQL translation tools.
  â€¢ Attendees are advised to practice in the console and refer to Lumi Academy for deeper learning.
  â€¢ A follow-up session will detail access controls and data management best practices.


ðŸŽ¯Todo List

  â€¢ Presenter / Training lead:
    
    â€¢ Schedule next video/session on â€œGetting started with BigQuery: access entitlements and data managementâ€ â€” target date: TBD
    â€¢ Share Lumi Academy resources and links referenced during the session â€” deliverable: resource list, due: 1 week

  â€¢ Attendees / Learners:
    
    â€¢ Hands-on practice in BigQuery console: run example queries, explore Explorer and SQL Workspace â€” suggested timeframe: within 2 weeks
    â€¢ Review Time Travel and clustering examples in Lumi Academy and test on a sample dataset â€” suggested timeframe: within 2 weeks

  â€¢ Data Governance / Admin:
    
    â€¢ Confirm project-level dataset sharing and access model for AMEX teams (roles/permissions) â€” deliverable: access plan, due: prior to next session

(If owners or exact deadlines are known, please provide names/dates to update the Todo List.)


Transcription

00:00:02 - 00:02:22 Unknown Speaker:

Loomy BigQuery Part 3. Let's dive into Lumi's BigQuery web console. Let's dive into Lumi's BigQuery web console, the platform that will make large-scale data analysis accessible for everyone in Amex. During this learning session, we will guide you through an introduction to the BigQuery web console, the key terms associated with BigQuery, and its features and capabilities. Let's get started. What is BigQuery? Imagine having an intuitive web interface where you can explore, analyze, and generate insights from your data. That's the BigQuery web console, a key component of the Lumiac. There you can execute SQL queries, manage, and monitor their execution. BigQuery offers users several advantages. One of its primary benefits is its scalability, which allows it to handle vast amounts of data with ease, making it a perfect fit for AMEX.

00:02:22 - 00:03:37 Unknown Speaker:

Its dynamic management of compute resources ensures efficient handling of extensive datasets by automatically allocating slots based on query size. Moreover, BigQuery fosters collaboration by providing a common dataset for all users within a project, enhancing cooperative data analysis. BigQuery Key Terms What is BigQuery? Imagine having an intuitive web interface where you can explore, analyze, and generate insights from your data. That's the BigQuery web console, a key component of the Lumi ecosystem. There you can execute SQL queries, manage, and monitor their execution. BigQuery offers users several advantages. One of its primary benefits is its scalability, which allows it to handle vast amounts of data with ease. Making it a perfect fit for Amex. Its dynamic management of compute resources ensures efficient handling of extensive datasets by automatically allocating slots based on query size.

00:03:38 - 00:06:05 Unknown Speaker:

Moreover, BigQuery fosters collaboration by providing a common dataset for all users within a project, enhancing cooperative data analysis. BigQuery Key Terms In the BigQuery Web Comp... What is BigQuery? Imagine having an intuitive web interface where you can explore, analyze, and generate insights from your data. That's the BigQuery web console, a key component of the Lumi ecosystem. There you can execute SQL queries, manage, and monitor their execution. BigQuery offers users several advantages. One of its primary benefits is its scalability. Which allows it to handle vast amounts of data with ease, making it a perfect fit for AMEX. Its dynamic management. Compute resources ensures efficient handling of extensive data sets by automatically allocating slots based on query size. Moreover, BigQuery fosters collaboration by. Providing a common data set. For all users within a project. Enhancing cooperative data analysis.

00:06:07 - 00:10:33 Unknown Speaker:

BigQuery key terms. In the BigQuery web console. So, there are several terms that you should be familiar with to navigate the platform more effectively in the future. First, there's the GCP Console. This is a web interface that allows users to access different services by Google Cloud Platform. Then, we have the SQL Workspace. This is essentially a web editor used for SQL execution and management. Another you will often come across is Projects. Which refers to the workspace used to onboard users and access BigQuery, among other services for performing analytics. Additionally, there's the Explorer, a feature that provides a pane for users to discover and navigate across data schemas and data sets. Lastly, we have data sets. These are top-level containers used within the BigQuery web console. To organize and control access to your tables and views.

00:10:34 - 00:13:37 Unknown Speaker:

So let's review how these terms will look once accessing this amazing data tool. This is BigQuery, one of the services GCP Console and Lumi provides to you. This is your SQL workspace. Now this is the Explorer feature. In here you will be able to open your current projects and datasets. That's it for the terms you should be familiarized with. So let's keep going. BigQuery features and capabilities. Progress, you'll come across an array of powerful features and capabilities designed to make your data exploration efficient. One such feature is the ability to execute concurrent queries. This means you can run several analyses simultaneously without waiting for one to finish before starting another. Next up is Time Travel. This allows you to query a table snapshot as it appeared at a specific time in the past. I'm sorry.

00:13:39 - 00:18:57 Unknown Speaker:

I'm sorry. This lets you go back and explore your data history. Then we have sessions, a series of interactions between you and BigQuery. Think of it as having a continuous dialogue of your data where you can. Ask questions with queries and receive answers as results. BigQuery also supports scripts, allowing you to execute. Hit multiple SQL statements in a single go. And speaking of scripts, you get to decide the job priority. Choosing between interactive or batch. Interactive is for those. Queries that need quick responses. Wall badge is perfect for larger queries that can run in the background as resources become available. Collation is another interesting feature. It's all about sorting and comparing strings in your data. Crucial when you're dealing with different languages and character sets.

00:18:58 - 00:24:15 Unknown Speaker:

BigQuery also cleverly caches the results of your queries to ensure faster response times, just like having a fast forward button. Speeding up your data analysis, clustering is another feature that organizes your data for more efficient querying. Imagine having your data sorted into neat piles. Making it easier and quicker to find the information you need. For those of you who like to keep things organized and timely. Query Scheduler lets you set up recurring query executions. Translator is a handy tool that allows you to convert SQL queries written in HiveQL into BigQuery. BigQuery standard SQL. This translator makes sure your high queries can speak the BigQuery language without any hitches. You are all packed with BigQuery features and capabilities. Go ahead, dive into the platform, practice and explore. There's so much to uncover.

00:24:19 - 00:25:11 Unknown Speaker:

Remember, Lumi and BigQuery are opportunities to deal with big data like never before. This platform is a powerful tool designed to simplify and enhance your data analysis tasks. So let's revisit what you've learned. What are the features that facilitate efficiency and insight in your data exploration? You should be thinking about features like concurrent queries, time travel, sessions, and many more have been integrated for this purpose. This is just the beginning, and we're excited to see how you'll leverage these tools to unlock the full potential of big data. Wrapping up, our exploration of BigQuery web console has just begun. With its feature-rich environment and key components like the GCP console, SQL workspace, Explorer, projects and datasets, we are set to unleash the full potential of big data.

00:25:13 - 00:29:19 Unknown Speaker:

By the end of this video, you are now able to understand the purpose of the BigQuery web console. And familiarize yourself with the key features and terms of the BigQuery web console. You are doing great! Don't miss our next video getting started with BigQuery where we'll delve deeper into its access entitlements and data management. For further assistance, check out our resources like Lumi Academy. See you soon! Thank you. You're welcome. Thank you.2026-01-26 Enhancing Rule Execution and Bulk Processing Capabilities

Creation Time: 2026/1/26


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-26 09:04 (Duration: 1688 seconds)
  â€¢ Location: [insert location]
  â€¢ Attendee: [insert names]


ðŸ“’Meeting Outline


Rule Execution & Rule Assist

  â€¢ Rule execution visibility The presenter demonstrated how to view which rules executed for a given deployment ID, including execution coverage percentage and frequency of executions. Example: searched by a discard ID, selected a specific rule and viewed rule execution coverage (100%) and its multiple execution instances.
  â€¢ Inspecting inputs and outputs For a selected rule execution instance, attendees saw the exact input passed into the rule and the resulting output. Example observed: rule validated input and set "send email" to true. This allows verification that rule behavior matches expected business rules.
  â€¢ Rule artifacts and formats Rules engines can use different artifacts: compiled DLLs or decision tables (Excel), confirming decision tables are supported as input formats.


Bulk Processing (ACE)

  â€¢ Overview and use cases Bulk processing is an input channel for ACE to start a process per row in a file. It supports both ad-hoc UI uploads and scheduled/automated ingestion from file locations (e.g., secure file transfer), enabling manual and fully automated bulk starts.
  â€¢ Upload and mapping configuration The Control Center UI shows configuration: choose whether to start a process or case; file type (XLS/CSV); file retention duration (temporary storage, default e.g., 30 days configurable longer); header validation, header-line skip count; and column-to-process-input mapping (map Excel columns to ACE input data).
  â€¢ Execution policy and concurrency Execution policy lets you schedule immediate or delayed starts, control frequency and concurrency of subprocess execution for each row, and configure output generation (e.g., status report fields, source of those fields).
  â€¢ Job details and monitoring Job Details page displays uploaded file metadata, job state, and the subprocess instances started for each row; users can drill into subprocesses to see inputs, outputs, and completion state.
  â€¢ Demo A demo file (five data rows, address fields) was uploaded via the Control Center; the processes completed successfully and attendees reviewed process instance details confirming expected behavior.
  â€¢ Appropriate scenarios Bulk processing is appropriate for ad-hoc large runs (thousands of records) and also for scheduled regular jobs. It supports use when normal daily volumes spike (e.g., thousands rather than hundreds) and can be automated so no manual upload is required.


Searching, Process Intelligence & Variant Paths

  â€¢ Finding cases by branch/path For processes with multiple branches (variants), locating instances that traversed a specific branch (e.g., those that created a click case) can be done several ways:
    â€¢ Design-time: write a value into a correlation key or context data (e.g., set correlationKey4 = clickCaseID). Then search Control Center by that correlation key to find affected processes.
    â€¢ Process Intelligence: use variant path analysis to view which variants (paths) occurred and their frequency; drill into variants to list instances. This will be demonstrated in a future session (Process Intelligence module).
    â€¢ Time-series/observability: emitted ACE events are available in OpenSearch/Elastic cluster or Cornerstone for searches and visualizations.
  â€¢ Design implications Consider including searchable markers (correlation keys) where specific downstream behaviors are important to find later. Process Intelligence provides richer analytics at scale.


Capacity, Scaling & Deployments

  â€¢ Infrastructure baseline & handling spikes Current infrastructure has substantial capacity; examples discussed: default deployments include multiple application pods (typically three) across availability zones for failover. Handling a spike from hundreds to tens of thousands of bulk records is feasible on the existing platform; millions of simultaneous operations would require separate planning.
  â€¢ RPA considerations RPA tasks are handled on separate infrastructure; core automation runs in main infrastructure and triggers asynchronous RPA calls to RPA-hosted resources. Scaling RPA may require additional infrastructure adjustments.
  â€¢ Performance testing Customers must still run performance tests for their expected volumes and SLA targets to benchmark processing time, file sizes, connection loads, and output generation. If tests show capacity needs, the platform can be scaled in the AmEx on-prem cloud with relative ease.
  â€¢ Multiple deployments & reporting Multiple deployment IDs represent separate scalable service groups (each typically backed by multiple servers). Onboarding conversations determine whether a use case uses an existing deployment or requires a new one. Deployment ID is used in reporting to organize and track use cases; moving a use case between deployments would be planned and communicated, not done abruptly.


Administrative Notes & Q&A

  â€¢ Open Q&A and next steps Time reserved for general questions about content from the four-day series. Participants encouraged to raise remaining questions in the Slack channel set up for this training for private/follow-up queries.
  â€¢ Future coverage Remaining topics (process intelligence, functional items) will be covered in Day 5 and specifically process intelligence demo scheduled for next session (Tuesday).


ðŸ“‹Overview

  â€¢ Rule execution can be inspected end-to-end: which rules executed, coverage percentage, per-instance input/output and frequency.
  â€¢ Rules artifacts may be DLLs or decision tables (Excel).
  â€¢ Bulk processing supports UI upload and scheduled automated ingestion; file mapping, header validation, retention, execution policy, concurrency and output generation are configurable.
  â€¢ Job Details view exposes uploaded file state and subprocess instances for diagnosis and tracking.
  â€¢ For locating cases that followed particular branches, use correlation keys in design, Process Intelligence variant analysis, or search emitted events in OpenSearch/Cornerstone.
  â€¢ Infrastructure has default redundancy and capacity sufficient for typical spike scenarios described; performance testing is still required to validate SLA targets.
  â€¢ RPA scaling is handled separately and asynchronously from core automation.
  â€¢ Deployment IDs group use cases into independently scalable deployments and are important for reporting and capacity planning.
  â€¢ Questions can be posted in the course Slack channel for follow-up.


ðŸŽ¯Todo List

  â€¢ owner 1:
    
    â€¢ Add correlation key to process design to mark when click cases are created; document correlation key used â€” due: [insert date], deliverable: updated process design and documentation.
    â€¢ Run performance tests for expected volume scenarios (including potential spike scenarios) and report metrics (processing time, concurrency, output file sizes) â€” due: [insert date], deliverable: performance test report.

  â€¢ owner 2:
    
    â€¢ Configure bulk processing job in Control Center for scheduled automated ingestion from secure file location (if desired) â€” due: [insert date], deliverable: scheduled job configuration and verification run.
    â€¢ If performance tests indicate capacity shortfall, request scaling change with platform team (provide performance report and estimated increased capacity) â€” due: [insert date], deliverable: scaling request ticket and platform team response.

(Replace owner names and due dates with the appropriate assignees and deadlines.)


Transcription

00:00:03 - 00:01:39 Unknown Speaker:

Day 4 is... what rules executed for your deployment ID and you can also check what percentage of rules got executed, which rules got executed getting executed frequently and you can even go and check what was the input out of your rules and what was your output. For example, I actually searched for the discard ID and looking at all the rules that actually got executed. So this is one of the rules and I'm going to click on that. And if you look at it, it says rule execution coverage 100. This is the rule that got executed. These are the different instances of the rule execution and I'm going to actually click on one. So you can look at it on what was the input to the rule and what was the output to the rule.

00:01:40 - 00:02:42 Unknown Speaker:

So input was this and essentially based on the rule data, it validated it and then it has actually set, if you look at it, send email, it is updating it to true. So you can see what went into the rule, what was the data that went into the rule and what was the output of the rule. You know if the execution was proper, and that's the expected execution that you're looking for, okay, out of the rule that you had configured. So this gives you a good insight as to how your rules are doing. And so that's something which will help you as you fulfill your business rules within years. Any questions about rules. Yeah, Barney, just a question. These rules engines, are they typically like Excel files or it's based on rules in my understanding, right?

00:02:43 - 00:03:43 Unknown Speaker:

That's correct, yes, yeah. They can be. Okay, they can be rules on DLL files, okay, or it can be decision tables, which is an Excel, yes. Right, thanks. Moving forward, we want to cover bulk processing. So bulk processing, as we discussed, is one of the input channels for A's, like if you have a file and if you want to actually start a process for every row of the file, then yeah, this is where you would come. And there are multiple ways to start bulk processing too. You can actually do a UI upload. And also you can actually have it as a regular deployment process that you're getting the file from some location and then you're processing it, right? So where it is not a manual operation.

00:03:44 - 00:04:38 Unknown Speaker:

So you have a serial job which pulls their file from, say, some secure file transfer location, and then it actually starts the whole process. So you can come up with that thing. So the flavor that I'm going to show you today is how to upload a file directly from... control center. Before I go ahead and start the processing, I'll show you what it actually is. So this is where you would come to actually say, okay, we discussed a little bit about this during our DHC session, and basically you would say, okay, how do I actually map what is in the file to actually what the process needs. This is how it would look. So essentially what you're going to say is, okay, what is my definition time? Okay, it's a process or a case.

00:04:38 - 00:05:21 Unknown Speaker:

You can start both process or case from here. Okay, and then what is my file time? Okay, it's an XLS or CSV. How long do I want to store my file? Okay, so we actually put the file into a temporary storage and it's free. So do you want to store it for 30 days? That's what this means. If you want to configure it more than that, you can do that. And then the header validation is something that we discussed a little bit, which is basically, uh, is my data in the same order as I want? And if the header names exactly match what I want, okay, so that the validation it would do. And you can configure how many header lines to skip. Okay, so if the file has multiple color lines, it will skip it.

00:05:22 - 00:06:13 Unknown Speaker:

So here it says, okay, my column one of my Excel is first name, column two is middle name, and column three is last name. So, and then you would map it to the ACE data. Okay. So the input data that you would pass. So that is how you would actually map your file to the process input that you are looking for. And there are other tabs. Okay. So I'll just briefly go over it. Okay. We don't have to get into. Too detailed here. This is to execution policy is to, uh, say, how do I want to execute the process? Right? Say, here is where you would say I want to execute it immediately, or they have to execute it. Um, basically after 10 minutes or after a day, things like that.

00:06:13 - 00:07:12 Unknown Speaker:

You would be able to configure it, and you also would say how frequently or how concurrently you would execute the process. Okay, and output generation if your process requires a specific output generation. So you are actually scheduling a file to be processed and you are triggering different sub-processes for each row of the file. And after all the processes are executed, If you want to generate a status report out of all your process distributions, then you can configure your output generation policy, which will say what are the data fields that I need in my output and where do I get it from? So that configuration you can actually configure here, and it would generate an output file for you. So Job Details gives you insight of the file that you just uploaded.

00:07:12 - 00:09:44 Unknown Speaker:

Okay, and then it would actually show you what state is it in? And also your sub-processes, the process that you trigger, all those details, it will actually bring it up. So that's where the job details page comes into question. Thank you very much. So let's see the demo file being uploaded. So this is a demo file. Okay, so let me share all the files. So if you look at it, this is my file. Okay. I have the same last name, building, address, address one, city, state, zip. And so it's kind of a demo data. So and it has five rows of data and one row of address. Okay. So that's what you're looking for. And let me upload this file. And I'm going to select process file. So the file has been created for process file.

00:09:45 - 00:11:15 Unknown Speaker:

Okay so if you look at it this is the file that we uploaded and it's actually it's cross signed okay. If you look at it, all the processes are completed and you can actually go into any of the processes and then see what happened with regard to the process. So you can say all the data that we just took out are there and the process actually went into a completed state. This is the process that we ran. It's a very simple process which actually takes the input and then outputs the data. So that's how you would actually do bulk processing in ACE. I'll take a pause. Any questions on? Is it safe to say that this kind of bulk processing would be ideal for ad hoc processing wherein we have multiple thousands of accounts that we need to process?

00:11:17 - 00:12:09 Unknown Speaker:

Also, yeah, you would, uh, so this is, uh, from the UI. It's, um, yeah, you would be able to do, uh, add processing, and uh, if it's not just for add up processing. You can have a regular process to verify, right? And those, you can actually have it as a schedule process, and you can actually run that. Instead of trying to come here and then have some person upload a file, right? So you can design a process where you can just retrieve the file from location and then you can start the whole process. Okay but if we ever get like some additional cases out of the ordinary like usually if a process let's say just runs 100 cases a day but just in case there is some event that we get 15,000, 20,000 cases then this can be used for that right?

00:12:09 - 00:13:24 Unknown Speaker:

Yeah. That is all I have for today. Okay, so I wanted to cover rule assist and bulk processing. We covered most of it, uh, yesterday, and, uh, the rest of the stuff is like, functional, and process intelligence is something that, uh, which would cover as part of D5. Um, so any other questions. Yeah, we wanted to keep this the rest of the time. I think we knew we only had a short amount of material remaining to cover. But we wanted to keep this open for general questions about any of the things that we've covered over the last four days. So if anybody has questions about anything, whether it's stuff that Marlee showed today or any of the other days, we're happy to take those questions now. I just wanted to remind, we have a Slack channel set up for this.

00:13:25 - 00:14:36 Unknown Speaker:

So if you're not comfortable asking questions in the public forum or you think of something later that you want to know or any of those things, you can feel free to post them on that Slack channel. And we'll answer those there. Murali, I do have a question regarding searching for process related data inside the control center. So let's say if a process is designed in such a way that it has multiple branches to it and only a few set of cases would go to a particular branch, let's say that particular branch creates a click case. So, if I have to find all the cases, all the click cases created from that particular process, is there a way to do a selective search on the total cases which followed a particular branch? Do you want to take the question?

00:14:36 - 00:15:35 Unknown Speaker:

I think process intelligence has a very important part. So let's say this is a real time scenario actually. So there is a process which has two branches in it. One would not create click cases. One does create click cases. So it was difficult for me to find out the cases for which click IP was in it. So is there a way I can do it easily and filter it out based on the path process? Yeah, there's actually a few different techniques for doing that. Some of those techniques might go to how you do your designs in the first place. So when you think about variant paths, you guys saw we could show what we call the SVG, the flow that happened. But then you'd have to know which ones you're looking for.

00:15:36 - 00:16:23 Unknown Speaker:

And I think in your scenario, you had mentioned thousands of entries for bulk. So you're trying to figure that out. So one of the things that you could do is you could add the fact that it created a click case in one of your available correlation keys as part of your design. That would make it easily searchable. So what that looks like is after the click case is created, you put the click case ID in correlation key, and let's just say correlation key 4 just for... a lack of better description, but you add it to your context data, you put it in correlation key four, now whenever you're searching for processes, you can search on correlation key four, does it have a value? And that would show you which process, and then you'd get a list of the processes that created the quick case.

00:16:24 - 00:17:19 Unknown Speaker:

So that's one way to do it. Another way to do it, as Merle was mentioning, is we have process intelligence, and in process intelligence, we have variant path analysis. So variance is what you're describing, right? There's multiple ways we can traverse a process flow or an automation flow. And each different path is a variant. So we can show it there at scale. And if you want to see which variants are happening and how often they're happening, you can come to Process Intelligence. And I'll show you how to see that tomorrow. I'm sorry, Tuesday, when we go back through Process Intelligence. It's a pretty rich feature there that's able to display which variants occurred and how often they occurred. And then also we have some click-through ability that on the specific variants, how often they occurred, you can see which instances they occurred for.

00:17:20 - 00:18:05 Unknown Speaker:

And those are all time-related searches. It'll make a lot more sense when I show it to you. OK, Mitch, go ahead. But I would say if you do need to see that, you know, you can either add it to correlation keys and you can search for it in the control center as part of your design, or you can also pull it up in process intelligence, just your basic reporting. It doesn't have to necessarily be the variant path analysis. As ACE is executing, anything that happens inside ACE gets emitted as time series events. And all that data is available both in our Elastic Cluster, and you have your ability to search or visualize that information through our Open Search toolkit. Or you will also have it in Cornerstone so you can see it there.

00:18:20 - 00:19:03 Unknown Speaker:

Future state, creation of click cases, that's something we want to definitely talk about, how that fits into our designs. A lot of our automations in the current state either create click cases at the end or are meant to work click cases that have already been created. So using click in this way is what we do now, but I'm not sure that that's how we're going to market future state. Because the toolkit here, you can see it's all combined into one thing. You know, looking to create cases for things other than exceptions to be managed is an interesting thing, right? Do we need to do it or do we not need to do it? So we'll look at those on a case-by-case basis. Sorry to use the word case there.

00:19:03 - 00:20:06 Unknown Speaker:

On a scenario-by-scenario basis and determine whether or not it still makes sense. We see click being used in all kinds of interesting ways. A lot of times it's used just because we, you know, it has some reporting associated with it and there's not really any case work to do at all. But we create the case because it's a pass-through to cornerstone. So that's one of the places we see it. I don't know that we're going to continue down that path. I have a question. So yesterday we discussed about processing. Capacity around it, right? Suppose during performance test, we have something that we would require, say, two or three parts to handle the volume. But if there is a certain spike in volume, and that's something absolutely unplanned and ad hoc, how do we tackle that during production?

00:20:09 - 00:20:58 Unknown Speaker:

Well, the volumes that you guys are talking about is still very, very small when it comes to the infrastructure we have allocated through the work. So when you say spike of volume, you're talking about maybe like, let's talk in terms of bulk. In bulk, let's say your process normally has 100 records, but today it has 10,000, right? Handling 10,000 things in bulk is still small for the infrastructure we have set up for these things. If you got to the point where you're doing millions and millions and millions of unexpected things, then that would be where we would have to have that conversation. You know, in case management and automation, if we're doing millions of things all at the same time, then something else has gone wrong in the enterprise. I think, absolutely all right.

00:20:58 - 00:22:00 Unknown Speaker:

So basically, you know, whatever infrastructure we currently have is sufficient to manage a significant volume handling, correct? So we have, like, Uh, by default, we have. I think it's we put three pods, three application containers in each environment that you're running the application. So we have two availability zones that get deployed right away for failover, but three is an awful lot for the kinds of things we're doing here. When we have RPA, that's where we have to scale the infrastructure for the RPAs because those take time, but those become asynchronous activities. We trigger the automation. All the core automation capability happens in your main infrastructure. And then we send a remote call to independent infrastructure to handle the RPA activities. And so based on that and our conversations about what needs to happen there, that's where scaling is a consideration.

00:22:00 - 00:22:52 Unknown Speaker:

And we spend time on making sure that those are done to scale. I see. Sorry, did you have something to add? Yeah, the thing is, because what we said, because we have enough capacity to run your process, it doesn't mean that you don't have to run your performance test. We still expect you to run your performance test for the volumes that you're looking for. The reason being, okay, a couple of things that you want to benchmark, okay, what is the ability to take a look at, okay, these 10,000 records I'm going to process in five hours, 10 hours, whatever. So depending on your business, you want to make sure that you have contributed in such a way that it processes within an appropriate amount of time. And also there are other parameters.

00:22:52 - 00:24:04 Unknown Speaker:

So how many blocks, how many data bit connections, what is your load, and how big of a file you are generating for output, things like that. So you would still have to do a performance test. Have that test part of your product lifecycle. Absolutely. That's insane. Thank you. And if we do find as a result of this test that we do need more infrastructure to support it, that's not a difficult task. Because we're running in the American Express on-premise cloud environment, we have the ability to scale the environment just with a few clicks of some buttons. So if we do find that we need more capacity to handle whatever it is you're doing, it's not a highly complex activity for us to take on. All right. Thanks, Benjamin. So the other thing I want to point out, you can see...

00:24:05 - 00:25:01 Unknown Speaker:

As long as we're on the subject, we have this ACE inter-asset. So today for automations, I think we definitely have more than one deployment. And when we say deployment, those are representative of individually scalable reference columns. We call them services on the tech side, but what a service is, it's its own application servers that are sitting in our cloud. So anytime you see a new deployment ID, you can be confident that we have at least, usually at least three servers sitting behind the scenes supporting this one deployment ID. So what we see for automation, I think GCS has its own, I think TLS has its own, I know GSG has more than one. So we have a number of different deployments that are supporting these different aggregations of use cases that need to run.

00:25:02 - 00:25:54 Unknown Speaker:

And what we do is when we have capacity considerations or regulatory considerations or other such things, right, and this is done by Saurabh and his team, we determine whether or not we need a new deployment and whether the use case should go into an existing one. So all those conversations happen kind of at onboarding time. When you come with a new use case, we'll say, okay, do you know what's your expected volume? Do you know what are the different things that it interacts with? And then so I will say, okay, it just makes sense to go into this deployment or we need a new deployment. Once something's in a deployment, it generally doesn't change. And our commitment to JP and those folks is that if for some reason, and again, we haven't seen it ever, but if for some reason a use case needs to move from one deployment to another, we'll be very transparent.

00:25:54 - 00:27:03 Unknown Speaker:

We'll make sure there's plenty of time to adapt things like reporting and other things that we use to represent that. So things really don't change in that regard. But if they had to, it would be an active conversation that just wouldn't happen suddenly one day. We're in some use cases in a new deployment. And why it matters is because deployment ID is one of the reporting criteria. So when we run our reports, we look for use cases within deployment IDs. It helps to kind of keep things organized and see how things are going. We're on top of the hour, so if there are no more questions, then we can proceed on Tuesday with our day five. Yeah, it looks like a number of the questions from today are about stuff. We're going to cover that.

00:27:03 - 00:27:22 Unknown Speaker:

That's all good. Yeah, and again, guys, remember, we do have these other ways you can ask questions if something comes up, so we appreciate your participation. you know when you ask questions it makes us know that we're hitting some of the right subjects and also that we're not just talking to you know empty space so thank you guys for your participation2026-01-26 Introduction to Lumi: Amex's Next-Gen Data Ecosystem

Creation Time: 2026/1/26


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-26 11:12 (Duration: 1800 seconds / 30 minutes)
  â€¢ Location: No location specified
  â€¢ Attendee: No attendee names provided


ðŸ“’Meeting Outline


Lumi overview and purpose

  â€¢ Lumi introduction
    Lumi is presented as American Expressâ€™s next-generation big data ecosystem hosted on Google Public Cloud, developed and managed by Amex Lumi. The name (Albanian for "river") symbolizes change and renewal. The platform is positioned as more than a data store â€” a foundation for innovation, growth, and next-gen data capabilities.

  â€¢ High-level benefits (for users and Amex)
    For users: smooth transition from legacy systems (Cornerstone), accelerated outcomes, efficient data analysis for decision-making, comprehensive training, built-in BI tools, and automated/self-serve features.
    For Amex: increased scalability, governance, operational efficiency, faster implementation, and a significant advancement in data management.

  â€¢ Google cloud advantages
    Adoption enables leveraging native Google services to accelerate offerings and implementation. Scalability and performance are emphasized as core benefits.


Platform capabilities and design

  â€¢ Core capabilities
    Real-time data access, predictive analytics, data security, unified data experience (single space for data to ensure consistency), reduced latency, and improved performance for analytics workloads.

  â€¢ Simplicity & user experience
    Emphasis on simplicity through automated self-serve services, personalized user experience, and easy onboarding of use cases. The platform architecture summarized as "C" â€” Capture, Cure, Consume â€” reflecting the core data flow.

  â€¢ Support for advanced tech
    The platform simplifies implementing AI and machine learning, making predictive analytics accessible for use cases such as targeted customer acquisition.


Roles, responsibilities, and onboarding

  â€¢ Personas and roles
    Multiple personas introduced; attendees may identify with one or more:
    
    â€¢ Project space developer: builds applications, dashboards, ML models; steers business solutions.
    â€¢ Data ingester: responsible for data ingestion pipelines and data quality.
    â€¢ Product operations: enables data ingestion, infrastructure provisioning, and platform operations.
    â€¢ Lumi governor: ensures data governance, compliance, pipeline and use-case integrity.
    â€¢ Analytical users: write optimized queries, analyze large datasets, and distill insights.
    â€¢ Platform support: provides training, business/operations/technical support to keep Lumi running.

  â€¢ Role expectations
    Each persona has unique contributions toward making Lumi successful; collaboration across personas is essential.


Learning journey and next steps

  â€¢ Training and enablement
    Lumi Academy resources, demos, knowledge assessments, and success stories will reinforce learning. Attendees are encouraged to participate actively, keep up with training, explore the platform, and ask questions.

  â€¢ Outcomes of the overview
    By the end of the session, participants should be able to: identify Lumi benefits, understand how Lumi enhances daily work, and recognize their role(s) in contributing to Lumiâ€™s success.

  â€¢ Follow-up content
    Next resource/video recommended: "Lumi capabilities" for deeper technical and functional details.


ðŸ“‹Overview

  â€¢ Lumi is Amexâ€™s next-generation big data ecosystem on Google Public Cloud (symbolized by the river â€” change and renewal).
  â€¢ The platform delivers scalability, governance, security, real-time access, predictive analytics, and reduced latency.
  â€¢ Architecture mantra: Capture, Cure, Consume.
  â€¢ Simplicity via automated self-serve services and personalized onboarding is a design priority.
  â€¢ Lumi makes AI/ML and predictive analytics more accessible for business use cases (e.g., targeted customer acquisition).
  â€¢ Multiple user personas were defined (developers, ingesters, product ops, governors, analysts, support) â€” each with clear responsibilities.
  â€¢ Learning will be supported by Lumi Academy materials, demos, assessments, and success stories.
  â€¢ Attendees should be prepared to integrate Lumi into daily work and continue with subsequent training (next video: Lumi capabilities).


ðŸŽ¯Todo List

  â€¢ (No owners specified; assign as needed)

  â€¢ unassigned:
    
    â€¢ Identify and publish attendee list and meeting location (ASAP)
    â€¢ Share link/resources to Lumi Academy training materials and next video "Lumi capabilities" (Within 1 week)
    â€¢ Provide role-specific onboarding guides for each persona (Project space developer, Data ingester, Product operations, Lumi governor, Analytical users, Platform support) (Within 2 weeks)
    â€¢ Schedule follow-up session covering "Lumi capabilities" and deeper technical walkthrough (Within 2 weeks)
    â€¢ Create and distribute a short checklist for transitioning from Cornerstone to Lumi for impacted teams (Within 3 weeks)

If you want, I can assign owners and firm deadlines based on your team roster â€” provide names and priorities and Iâ€™ll update the Todo List accordingly.


Transcription

00:00:00 - 00:01:31 Unknown Speaker:

Lumi part one. That's why Lumi represents our step into the next generation of data at American Express. This platform streamlines data management, simplifies predictive analytics, provides real-time data access and data security. But it's more than just a data platform. It's a platform for innovation and growth. Let's talk about what... In this video, we will discuss how Lumi as a next gen... In this video, we will discuss how Lumi as a next-generation data platform enhances your work and outline its benefits. You'll gain an understanding of your role within Lumi and how to be prepared for your learning journey. Now, let's get started. Welcome to the Lumi overview video. As an MX colleague, Lumi will become an integral part of your daily tasks, enhancing your work with Lumi. Why Lumi? It's simple.

00:01:31 - 00:04:18 Unknown Speaker:

LumI is AmeX's next generation big data ecosystem on public cloud. It is hosted on Google Public Cloud and managed and developed by Amex Lumi. The Albanian word for river signifies change and renewal. That's why Lumi represents our step into the next generation of data at American Express. This platform streamlines data management, simplifies predictive analytics, provides real-time data access, and data security. But it's more than just a data platform. It's a platform for innovation and growth. Let's talk about what Lumi brings to us. For you, Lumi ensures a smooth transition from legacy system cornerstone, accelerates outcomes, supports efficient data analysis for decision-making, and provides comprehensive training. It's a complete platform, equipped with bi-tools and automated features. For Amex, Lumi offers scalability, governance, efficiency, and faster implementation. The platform represents a significant advancement in data management.

00:04:32 - 00:05:33 Unknown Speaker:

One of the biggest advantages of adopting Lumi is that we will have the ability to leverage some of Google... In this video, we will discuss how Lumi as a... Welcome to the Lumi... By the end of this video, you are now able to... Identify the... Welcome to the Lumi overview video. As an Amex colleague, Lumi will become an integral part of your daily tasks, enhancing your work with Lumi. Why Lumi? It's simple. Lumi is Amex's next-generation big data ecosystem on public cloud. It is hosted on Google Public Cloud and managed and developed by Amex. Lumi, the Albanian word for river. Signifies change and renewal. That's why Lumi represents our step into the next generation of data at American Express. This platform streamlines data management, simplifies predictive analytics, provides real-time data access, and data security.

00:05:33 - 00:07:42 Unknown Speaker:

But it's more than just a data platform, it's a platform for innovation and growth. Let's talk about what Lumi brings to us. For you, Lumi ensures a smooth transition from legacy system cornerstone, accelerates outcomes, supports efficient data analysis for decision making, and provides comprehensive training. It's a complete platform, equipped with bi-tools and automated features. For Amex, Lumi offers scalability, governance, efficiency, and faster implementation. The platform represents... That's a significant advancement in data management. One of the biggest advantages of adopting Lumi is that we will have the ability to leverage some of Google's native services to accelerate our offerings quickly and efficiently. But what does all this mean for you? Well, the benefits are substantial. Scalability is a big one. Lumi scales the data to meet your needs without compromising performance or usability.

00:07:44 - 00:08:39 Unknown Speaker:

Simplicity is at the heart of Lumi. With our automated self-serve services, data ingestion becomes easy. Personalized user experience and easy onboarding of use cases are another aspect of Lumi's enhanced servicing. This platform is designed with you in mind, making it as user-friendly as possible. Lumi's architecture is as simple as the letter C. Capture, cure it, and consume. That's the mantra we follow. The platform simplifies the implementation of advanced technologies such as AI and machine learning. Making them accessible and easy to use, Lumi provides predictive analytics for targeted customer acquisition, helping you reach customers more effectively. With real-time data availability, the performance of the data processing and analytics tasks is improved. The unified data experience offers various benefits for you. All data resides in a single space, ensures data consistency and provides a comprehensive view of data.

00:08:39 - 00:11:11 Unknown Speaker:

And, finally, reduce latency. With Lumi, you can quickly and efficiently... access your data with minimal delays that's efficiency redefined understanding your role everyone will play a role in helping amex realize those benefits. Whether you're an analytical user that is writing optimized queries or a product operations user, do you already know which persona you identify with? During your Lumi onboarding process, you will be introduced to various roles that can benefit greatly from using Lumi. If you have not identified one yet, we will explain what each one entails. It is important to note that you may identify with one or multiple personas. Remember, each persona plays a unique and crucial role in making the most of what Lumi has to offer. With everyone working together, we can achieve great things.

00:11:13 - 00:23:11 Unknown Speaker:

As a project space developer, you're the driving force behind creating innovative applications, dashboards, and machine learning models. Google Charger. Your role is crucial in steering our business in the right direction. As our data ingester, you lay the foundation for data-driven decision-making by setting up quality data. Thank you. Your work ensures our data is accurate, reliable, and timely. Our product operations team are the gatekeepers who enable data ingestion, infrastructure provisioning, and more. Your dedication ensures that Lumi functions smoothly and efficiently. As a Lumi governor, you uphold the integrity and compliance of our data, ingestion pipelines, and use cases. Your efforts ensure we maintain... The highest standards of quality and accountability. Our analytical users, your ability to turn large amounts of data into plain insights is nothing short of amazing. You help. Everyone understand the story our data is telling.

00:23:12 - 00:25:54 Unknown Speaker:

Last but not least, our platform support team. Your role might be behind the scenes, but it's no less important. You provide the crucial training, business, operation, and technical support that... Keeps Lumi running smoothly. How you'll be prepared, learning journey. Your learning process will be reinforced with a range of materials, including knowledge assessments, demos, success stories, and resources from the Lumi Academy. Prepare yourself for an engaging learning experience where you'll actively participate, learn new skills, and be ready to apply those skills in your daily tasks. Remember, the goal is to integrate Lumi into your everyday tasks and we're here to help you achieve that. Keep up with the training, explore the platform, and ask questions. By the end of this video, you are now able to identify the benefits that Lumi offers to you, understand how Lumi enhances your work, Recognize your role for contributing to Lumi's success.

00:25:54 - 00:26:00 Unknown Speaker:

You are just beginning to dive into the exciting world of Lumi. Don't miss our next video, Lumi capabilities.2026-01-26 Lumi Notebook Workbench Setup and Best Practices

Creation Time: 2026/1/26


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-26 15:43 (Duration: 3357 seconds)
  â€¢ Location: Lumi Notebook / Jupyter workbench session (virtual)
  â€¢ Attendee: (names not explicitly listed in transcript) â€” Presenters/Speakers: Santosh (presenter), Ganesh Venkataraman (POC referenced), James, Alok, others who asked questions


ðŸ“’Meeting Outline


Lumi Notebook workbench setup & first-time authentication / restart behavior

  â€¢ Authentication and ADC / Quota project The presenter explained that after authenticating in the Jupyter/Lumi notebook, credentials are saved and the chosen project is added to Application Default Credentials (ADC) so client libraries use it for billing/quota. The authentication step requires a notebook restart to fully activate the workbench instance (click â€œRestartâ€ then confirm â€œWorkbench notebook activatedâ€) before continuing work.

  â€¢ First-time vs returning user behavior First-time users must perform the authentication and restart steps. Returning users launching a notebook will typically see the notebook workbench page with project ID and resource ID and can click "Open notebook" to start directly.


File persistence, storage mounting, and workspace best practices

  â€¢ Dedicated Cloud Storage (bucket) and mounting Each analytical user has a dedicated GCS bucket (e.g., AXP-US-SS1010 or similar naming with ADS ID). Files edited in the bucket are persisted even if a workbench instance is deleted. Users should mount their bucket to the notebook (via the file browser: drag, then select â€œmount storageâ€, enter bucket name including ADS ID) to ensure file persistence.

  â€¢ Recommended workflow Best practice: create and work directly on notebooks/files stored in the dedicated bucket so work is not lost if an instance is corrupted or recreated. The bucket is the persistent location for code and notebooks.

  â€¢ Temporary export area (Temp data) There is a temporary bucket/folder (Temp data) intended for exporting large files (CSV, etc.). Files in Temp data are ephemeral and will be removed automatically after three days; users must download/export to local storage if they need longer retention.

  â€¢ GitHub integration Users may clone GitHub repositories into their mounted bucket; cloned code will be governed by the ADS (bucket governance) and persisted in the bucket.

  â€¢ Support for mounting and issues Mounting is a one-time activity per user/instance. If mounting fails or files are missing, use the Lumi workbench Slack channel for instance issues. The presenter also referenced Ganesh Venkataraman as POC for feature requests or follow-ups.


Notebook UI, kernels, and environment management

  â€¢ Jupyter-like interface Workbench UI matches Jupyter behavior: new launcher (+) and Python 3 kernel to create .ipynb files. Notebooks created in the UI can be created directly in the mounted bucket.

  â€¢ Folders & file operations Users can create folders within the mounted bucket via the file browser. The bucket should be used to store code permanently; Temp data is for temporary exports only.

  â€¢ Package installation Users can pip-install Python packages directly from within the notebook environment. The environment looks to Amex Artifactory first; if a library is missing there, installation may fail. Use pip install and verify availability with pip list | grep .

  â€¢ Limitations of the analytical workbench The workbench instance is a lightweight analytical environment (e.g., M2D standard). It is intended for analytical use-cases (exploration, queries, transforms). For heavy modeling or large distributed compute requirements, users should request clusters from the IDA team (separate service) which provide larger compute for model training.


Connecting to BigQuery and running queries from the notebook

  â€¢ BigQuery client from Python (pandas-based) Demonstrated steps:
    
    â€¢ Import pandas and google.cloud.bigquery client.
    â€¢ Create a BigQuery client object (uses ADC / authenticated project).
    â€¢ Run SQL via client.query(sql) and convert result to pandas DataFrame (result.to_dataframe()).
    â€¢ Use df.head() to preview results.

  â€¢ Writing pandas DataFrame back to BigQuery Demonstrated steps:
    
    â€¢ Create table schema (SchemaField entries).
    â€¢ Define table_id (project.dataset.table).
    â€¢ Use bigquery.LoadJobConfig(schema=...) and client.load_table_from_dataframe(df, table_id, job_config=...).
    â€¢ Call job.result() to ensure completion; then confirm in BigQuery console (refresh dataset).

  â€¢ Loading CSV from GCS to BigQuery via Python Demonstrated steps:
    
    â€¢ Use client.load_table_from_uri(gs://path/to/file.csv, destination_table, job_config=...).
    â€¢ Provide schema and destination table name; the table will be created if it does not exist.

  â€¢ BigQuery magic commands & SQL editor Notebook supports magic commands (%%bigquery) to run SQL directly in cells. Users can also open the SQL editor or use the BigQuery web console when appropriate. The notebook workbench is optimized for Python workflows; for SQL-only tasks, BigQuery console may be preferable.


Working with larger-than-memory datasets: PySpark vs BigFrames (BigQuery-backed DataFrames)

  â€¢ Pandas limitations Pandas is suitable for small datasets but not for large-scale data. For larger datasets consider PySpark or BigQuery-backed dataframes.

  â€¢ PySpark setup and behavior PySpark and Java are available in the workbench environment. To enable BigQuery connector dependencies, users must download or copy the required jar (spark-bigquery-with-dependencies.jar) into the environment (command shown). The jar must be accessible from the notebook instance (attention to current working directory when executing copy commands). Create SparkSession with the jar added to spark config, then read tables with spark.read.format("bigquery").option("table", "project.dataset.table").load() or via SQL using spark.read.

  â€¢ PySpark caveats PySpark runs on distributed compute and cannot directly write its distributed DataFrame to BigQuery in the same way pandas does. Typical approach:
    
    â€¢ Convert PySpark DataFrame into a BigQuery DataFrame (BigFrames) or collect and then write (but collecting is not feasible for large datasets).
    â€¢ Because of limitations, best practice is to migrate PySpark logic into BigQuery (via BigFrames / SQL) when possible.

  â€¢ BigFrames (BigQuery-backed DataFrame) Introduced bigframes.pandas as bpd (BigFrames library):
    
    â€¢ bpd.read_gbq(table_or_query) returns a BigQuery-backed DataFrame (bigframe) that provides a pandas-like API but executes on BigQuery.
    â€¢ BigFrames scale to large data and leverage BigQueryâ€™s petabyte-scale processing.
    â€¢ Methods are similar to pandas and allow easier conversion and write-back to BigQuery.
    â€¢ Encouraged migration best practices and documentation links were shared for converting pandas/PySpark workloads to BigFrames.


Hive-to-BigQuery conversion helper (magics / accelerator)

  â€¢ myMagics.py and Hive-to-BigQuery conversion The Lumi analytics team built a magic command (%%hive_to_bigquery or similar) contained in mymagics.py that converts Hive/Cornerstone SQL (e.g., HiveQL) into BigQuery-compatible SQL. Steps:
    
    â€¢ Place mymagics.py in the environment and load the magics by executing the cell that loads it.
    â€¢ Use the magic command with either a stored query variable or reference an input file; the magic returns a converted BigQuery SQL string.
    â€¢ The tool handles many Hive constructs and maps them to BigQuery syntax (the in-house team built in parsing logic). Users were encouraged to try it and provide feedback for improvements.

  â€¢ Limitations Magic only converts SQL syntax. It does not create connectivity to Cornerstone/Hive tables directly from Lumi. If data resides in Cornerstone/Hive, it must first be brought into Lumi/GCS/BigQuery before running converted queries.


Q&A highlights / support & next steps

  â€¢ Switching projects within a notebook Project is selected when the workbench instance is created; you cannot switch the project for that instance post-creation. Choose the intended project at creation time.

  â€¢ Mount persistence Mounting the bucket is usually a one-time activity; if an instance shows missing mounts later, contact the Lumi workbench support Slack channel.

  â€¢ GitHub governance Cloned repositories stored in ADS bucket follow bucket governance/policies.

  â€¢ Package installation pip install is available in notebook; installation may fail if the package is not in Amex Artifactory.

  â€¢ PySpark jar issues If users get file-not-found when copying JARs, ensure terminal working directory is correct (instance base path vs bucket), and copy the jar when terminal is at the correct path or use gsutil from the instance base. The presenter offered follow-up help for that step.

  â€¢ Magic conversion accuracy The magic works for many constructs but may require validation (for example, date handling or nuances). Users should verify converted queries and provide feedback for edge cases.


ðŸ“‹Overview

  â€¢ Authentication saved credentials to a file and sets ADC quota project; a restart is required to fully activate the notebook workbench.
  â€¢ Always mount and work from your dedicated ADS bucket (AXP-US-SS1010 or similar) to preserve notebooks and code; Temp data is ephemeral (3 days retention).
  â€¢ You can clone GitHub repos into your bucket; cloned code will be under bucket governance.
  â€¢ The notebook UI behaves like Jupyter: create Python 3 notebooks, install packages with pip, and use the file browser to manage folders/files.
  â€¢ Use google.cloud.bigquery client to read/write DataFrames (pandas) to/from BigQuery for small data.
  â€¢ Use load_table_from_uri to import CSVs from GCS into BigQuery via Python.
  â€¢ PySpark is available but requires adding the BigQuery connector jar; distributed Spark DataFrames are not straightforward to write back to BigQueryâ€”recommended to use BigFrames or convert logic to BigQuery SQL.
  â€¢ BigFrames (bigframes.pandas) provide a scalable, pandas-like API that executes on BigQuery and should be used for large datasets.
  â€¢ Lumi magics (mymagics.py) offer Hive-to-BigQuery SQL conversion; useful accelerator but verify converted SQL and report edge cases.
  â€¢ For workbench issues or feature requests, contact Lumi workbench Slack channel or Ganesh Venkataraman (POC); presenter also available to escalate.


ðŸŽ¯Todo List

  â€¢ Santosh / Presenter:
    
    â€¢ Share the mymagics.py file and sample usage cells with the attendees (no deadline specified).
    â€¢ Follow up on the JAR copy issue experienced in the terminal and provide a step-by-step note for copying spark-bigquery-with-dependencies.jar into the workbench instance (ASAP).

  â€¢ Ganesh Venkataraman (POC):
    
    â€¢ Review and respond to feature requests or follow-ups raised by users about additional connectors and enhancements to the workbench (ongoing).

  â€¢ Attendees / All users:
    
    â€¢ Mount your ADS bucket (AXP-US-SS1010 or appropriate ADS ID) to the notebook workbench and store notebooks/code there (one-time).
    â€¢ If you plan to use PySpark with BigQuery, test the spark-bigquery-with-dependencies.jar steps in your instance and report any issues to Lumi Slack channel (ASAP).
    â€¢ Try the Hive-to-BigQuery magic tool on representative queries, verify results (especially date logic), and provide feedback via Lumi Academy Slack or to Santosh (within 1 week suggested).
    â€¢ For large exports placed in Temp data, download or move to persistent storage within 3 days to avoid automatic deletion (ongoing practice).

If you want, I can convert this into a shareable email or add explicit assignees and deadlines for each action item â€” tell me who should own each item and any target dates.


Transcription

00:00:06 - 00:01:18 Unknown Speaker:

Python Notebook continue. Yeah, so it says credentials has been saved to this file. So, quota project This particular project was added to the ADC right, which can be used by our client libraries for billing in quota. Right now, whatever the query is that we are going to run right up here with respect to this Lumi Notebook, which will be directly tagged to this particular project. So we have well and good. So all we have to do is now close this Jupyter notebook. Whatever steps we did, come back to our Lumi portal. I'll scroll down and you see this right after this authentication, so create second step is authentication. After authentication, you have to restart to activate, like whatever the steps that you executed in notebook terminal. If it should actually reflect, you should click on Restart right.

00:01:18 - 00:02:27 Unknown Speaker:

Only then. So you should get this notification which says Workbench notebook activated, right, so I'll click on continue. Now again, you can go back open your notebook. Start working, write a Python query, which will connect to BigQuery table. You can query the data, perform some data transformation. Again, load the data back to BigQuery as a new table. So these are all the things that you can perform with the Lumi notebook workbench instance, which we will be looking in a while. So let the notebook load. At the other things that you can perform, perform so that you executed in notebook terminal. If it should actually reflect, you should click on Restart right. Only then so you should get this notification which says Workbench notebook activated, right? So I'll click on continue.

00:02:27 - 00:03:29 Unknown Speaker:

Now again, you can go back, open your notebook, start working, write a Python query, which will connect to BigQuery table. You can query the data, perform some data transformation. Again, load the data back to BigQuery as a new table. So these are all the things that you can perform with the Lumi notebook workbench instance, which we will be looking in a while. So let the notebook load. But yeah, so this is how the process looks like. And this you have to do it only for the first time if you are a new user. If you're already an existing user, when you launch a notebook, by default, this is the page that will show up to you. So notebook workbench is there, project ID, resource ID, everything, it will show up.

00:03:30 - 00:04:27 Unknown Speaker:

All you have to do is click on open notebook and you can go ahead and start using it. And one more important thing that I'll show you, like so many people keep reaching out to us asking, right? But sometimes by mistake, the instance got dominated and whatever the files that I was working was, is not available to me right now, right? What do I do in those cases? See, just for this particular training purpose, I deleted my notebook workbench instance. But even though whatever the files that I was earlier working, I still do have it with me. Like how is the question that you might be asking? But all the files have been backed up into my bucket, right? So you can easily mount your bucket to your notebook workbench.

00:04:28 - 00:05:24 Unknown Speaker:

All you have to do is just drag this towards your light. And you see this three vertical, so three horizontal lines, which says mount storage. All you need to do is click on this. Type your bucket name, so here you are seeing AXP-US-SH1010 instead of SH1010. You have to type your ads ID name so that will be your bucket. So I request each and every one to take a copy, right? Or you can directly work here itself, right, so you can keep all your files, codes in this particular place AXP-USSS1010. You can directly start working here itself, so I'll click on this workbench guide so you can directly start editing here itself. Save it so that even if your workbench instance is gone right, you can create it from the scratch.

00:05:24 - 00:06:29 Unknown Speaker:

And all you need to do is just mount this bucket, right? So that's what you have to do. And you have everything in your bucket. So yes, come on here. A quick question. If we wanted to connect to, let's say, ETL results or EMR map results, would we just connect to a bucket the same way? Just mount the bucket where the data results are? Or is there another way to connect to... different data sources besides the S3 buckets. In the notebook workbench instance you're asking me? Yes. Currently the features that they have enabled is like you can directly connect to the bucket or the one thing is if you have your source code in GitHub, You can just directly clone the repository whatever you have it, right.

00:06:29 - 00:07:27 Unknown Speaker:

You can provide that clone link over here and you can clone this here and you can start working it. Currently, only those are the two options that is available. But yeah, if in case you can, if you have any other further queries, right? Or if you feel if this thing is there that would be helpful, then. Ganesh Venkataraman, with the POC available for this. So we're aggressively working on all of this latest releases, all those things. So you can reach out to him. And yeah, if you do not get any response from him, you can also reach out to me so that I can get in touch with him and I can get back to you. Sounds good. Thank you, Satosh. Yeah. Great.

00:07:27 - 00:08:20 Unknown Speaker:

So yeah, first let me, before we go through this particular guide, whatever we have, right, I'll just quickly take you through this notebook workbench. I know each and everyone will be aware how this particular interface works. It's exactly equivalent to a Jupyter notebook if you've already worked with. So this plus icon, which is a new launcher, where our Python 3 kernel is there. So this is what we should be making use of, either, otherwise we can make use of this dominant as of now, right? You can just click on Python 3 kernel where it is going to open a new iPYNB file and where you can get started, right? See whatever the new iPYNB file that I opened, it was directly created in my bucket, so it is not getting created in my instance.

00:08:20 - 00:09:15 Unknown Speaker:

So that is one thing again. I wanted to highlight. So the best practice is always you start working directly on the bucket so that you never lose any of the work that you're doing it. In this plus icon, you can create a new folder within this particular area itself because this is directly integrated into your Jupyter notebook. You can create a new folder here, you can log. And this is only for your, this bucket is only for your codes, right? Because this particular bucket is a permanent bucket and it has some size limitation, right? If in case. If you want to export any large volumes of data, right? So you see an extension of my SS1010, which says Temp data. So this is where we can export all of our CSV files, right?

00:09:15 - 00:10:05 Unknown Speaker:

But anything that you export here, right, can be any large file that you would like to export. So you can do it, but you have a limitation of three days after three days, automatically, whatever the data that is present over here, it would be closed, right? All you have to do is you have to download this to your local system. That's all from your TIM data bucket, right? So because, like in the previous session, whatever I have exported right now, I do not have it here, right? So the name itself says it is a Temp data, so that's what you need to keep in mind. So all of my code, I have it in SS1010 and anything that I want to export. I keep it in my large volumes of data, I keep it in my Temp data bucket.

00:10:05 - 00:11:10 Unknown Speaker:

But I do store it locally because as it is a temporary space. And so yeah. These are all options that is available in this particular file browser, and you can directly connect to BigQuery as well from your notebook. See here, my PRG-P-UMI Academy is available over here. I click on this dropdown, common data set, my personal data set, accessible, and I have a few tables that is created, right? You can also search for tables and data set. You can also click on add a project here, and you can add AXP-Lumi, right? And you can pin this here, and you can add a few tables that is created, right? You can also search for tables and data set. You can also click on add a project here, and you can add AXP-Lumi.

00:11:14 - 00:12:16 Unknown Speaker:

Right, and you can spin this, and you'll see with the AXP-Ruby, you have the DW data set, with the DW data set, you have all of your E3 tables, right? So this is how we can directly create a BigQuery from here, right? So that's pretty much I just wanted to cover here with respect to this Jupyter Notebook interface. We'll just quickly jump on to the workbench guide. Yes, Alok, any questions? Question on this. Yes, so can we switch on the project space? I mean, do we have to use just one project space when we are using this Jupyter file, or can we switch to the other one while working? Like in BigQuery, I think it is... No, even this, I think by default, whatever project and notebook gets created. So that's what is available here, I guess.

00:12:16 - 00:13:09 Unknown Speaker:

You just have to choose that while your workbench instance is getting created. So after which we cannot switch it. So it gets created by default, the project you choose. All right. Thank you. Yeah, I think there were other questions from... Yeah, this is James. I had just a couple questions follow up. On the left, there where you have your ads ID, so this will save all the files directly, right? If you, if you go to the main folder, you'll lose the notebook. Is that what you're saying that I need? Yeah, yeah, this particular notebook, right? So what happens? Let's say there is some issue in the GCP notebook itself, right? Let's say whatever instance that you got created, it got corrupted due to some reason. XYz. And the instance that you were working for these many days is not available.

00:13:10 - 00:14:00 Unknown Speaker:

So it's not getting refreshed. You're unable to open it. So the only option that you have is you have to create it from the scratch. So when you do it, all the files that you were working for these many days, so for example, this notebook template, this will not be available when you create a new workbench instance. So it is like you are starting from the scratch. So no matter if you start from the scratch, all you need to do is again, whenever you create a storage, sorry, whenever you create a workbench instance, you can mount your bucket, which is available for you always. This is the cloud storage that we have. So within the cloud storage, a dedicated bucket is created for you because you are a analytical user. So that bucket you directly integrate here.

00:14:00 - 00:15:04 Unknown Speaker:

So this data is available permanently for you. Always. Okay, second follow-up to that. When I connect to GitHub here and clone, it's within my ads ID. So that has the governance then, right? It's the let's save accordingly, correct? Yep, yeah. Okay. And then I'm not sure if this is the right time, but I had two other questions just up there about packages and then keys. So I didn't know if this is the right time or later. Yeah. Thanks. Yeah. So I see others have raised your hand. Please ask any questions that you might have. So I want to know, do you have to know the button every time you scan the cluster? Or is there something permanently? Can you please repeat your question? Your wire is a bit low. Can you please? Can you hear me now?

00:15:04 - 00:16:00 Unknown Speaker:

Yeah, yeah, great. Now, I'm saying that whenever there's a new cluster with the Lumi analytics, we need to mount the GCS update to this workbench, or is there anything permanently that we don't have to repeat this process? No, this is a one-time activity again, but in case if you're facing any issues, So I think then it should be a issue with your... notebook walkbench. Because I remember that I just did this one time. Like, mount a bucket. And this is always available to me whenever I log in. And that should be the case for others as well. But if you are facing any issues, there is a dedicated Slack channel called as Lumi walkbench instance issues. So they are available only to resolve any issues that you are facing with the walkbench instance.

00:16:01 - 00:17:07 Unknown Speaker:

So they should be able to help you out with that. Now quickly getting on to the topic like using Python or can we get connected to BigQuery. So all those things is what we are going to look now. So these authentication steps, we have already done it, so maybe we can get started directly from this particular place, which says, verify if the bucket is mounted right. So this query will give you some output only if in case. If your bucket is directly mounted to your Jupyter notebook, so only then it has access to go and read your bucket, right? So if I run this, so this. Run button allows me to run within the cell itself. You see this. So it says run the selected cell. Or otherwise, you can make use of this run button at the top.

00:17:07 - 00:18:09 Unknown Speaker:

Or otherwise, you can also make use of a shortcut, shift enter, which is going to execute the cell. So we have three options which we can make use of. So this particular cell was able to get into my bucket and it was able to read, which means. We have mounted our bucket and the notebook workbench instance. So that is one thing I just wanted to show here. And while you're working with your Jupyter notebook, right, this analytical workbench instance, and one more thing that I would like to highlight here is this is specifically meant for analytical users, right? So that AU project, which you see here, PRG-P-AU, where AU indicates you are an analytical user because this is a lightweight instance that is getting created. You see this, right?

00:18:10 - 00:19:10 Unknown Speaker:

M2D standard for, so I think it is a lightweight and this is not a cluster that is getting spinned up where you can leverage the Jupyter notebook for your modeling purposes, right? There might be some users who was going to build a model in order to predict some values, right? Some use cases. For those kinds of people, this is not the right place, right? This workbench instance is not the right place. As you said, there is a IDA team separately where they allow you to spin up a cluster. And on top of that cluster, you can create a Jupyter notebook, and you can work there, you can do modeling stuff. But this is meant only for the analytical users, right? And as an analytical user whenever you're working.

00:19:10 - 00:20:06 Unknown Speaker:

And if you have any need to install a library, right, you can just do this PIP install a library. So if I run this, so that is going to download the library and it is going to install in our environment here. So that that particular library is available for us and we can make use of it, right? This is just a random library that I installed to showcase to you, right? So yeah. And if in case, if you're looking for any library, if that library is available or not, you can do PIP list, GREP and this streamlit, right? So that will tell you if this is available. It will showcase that particular library name and what is its version, even in case if it is not displaying anything so which means that particular library is not available.

00:20:06 - 00:21:11 Unknown Speaker:

So that's the takeaway for us, right? So we can also install so that is doable here. Can you install lotteries in the notebook you're working, or you have to go back to the Central I.Like I didn't get your question. I'm working on a notebook. I don't have a library. I can install it there, right? I don't need to go back to that. Do I need to go back to that? You can directly install in the notebook. Okay. Yeah. Thanks. And one thing that I would like to tell this, right? So let's say it's right looking in indexes. It basically looks into the Amex Artifactory. Rarely there might be some cases where the library that you're looking for might not be in this Amex Artifactory. If that is the case, then the library installation might fail.

00:21:11 - 00:22:07 Unknown Speaker:

But for most of the cases, it is available. Just for your information, I'm just putting it. Now coming back, so you're good, like you got an auto-mount your library, sorry, mount your bucket, and how you can save your files, and then if there is a requirement, you can install your own library, and you can clone it from the Git as well, in case if your source code is in the Git. So these are all some of the features that are available. Now let's actually get into the topic as in how we can run the queries, right? So before we get into the Python, you can directly run your BigQuery query as well in the notebook cell by using something called as magic command. Like how you were doing it in common stone, right? Percentage, percentage high.

00:22:08 - 00:23:04 Unknown Speaker:

Similarly, you can make use of percentage, percentage BigQuery. And you can directly run your SQL queries here. And you can also do that using the SQL editor that is available here as well. You can click on this button. You can click on open SQL editor and you can do it. But if that is the need, then you can directly go to BigQuery web console and you can do it. So this is completely developed for the Python user site where they want to write a Python code and connect to BigQuery query the data. So it is exclusively meant for that. But we don't have those options which we are showcasing yet. So try avoiding this and try to leverage the BigQuery console that you have, right? And please do leverage this when you have Python need, right?

00:23:05 - 00:24:07 Unknown Speaker:

Let's go to the next cell where we are loading data from BigQuery, right? And we are creating a data frame basically. That's what out of the BigQuery table. First, I'm showcasing it, how we are going to do it with the pandas, the native library that is available within the Python, right? And in order to do this, the BigQuery library plays a very, very important role, right? Because that is the one which allows us to create a client object, which is going to establish a connection between the Python and the BigQuery, post which we can write a query, run it, create the data frame, everything, right? So that is the step. You import pandas as PD, you import the BigQuery library inside which you have this client class using which you create this client object.

00:24:07 - 00:25:03 Unknown Speaker:

Now, once you have created the client object, it's all the SQL query. You write a select SQL where I'm selecting from my personal data set, I have this particular table and I've just limited the data to only 10 rows, first 10 rows, right? And what I'm doing, so this particular client object, whatever I have created, it has a method called a query method to which I can pass my SQL. It is going to execute and it is going to provide me the result. Again, using that result, I'm directly converting it to a data frame. So once the query is executed, I'm directly converting it to data frame using 200 score data frame, right? And once I have the data frame available, I am displaying it using the head method. So this head method is available within the Pandas data frame.

00:25:03 - 00:26:08 Unknown Speaker:

Let me run this, which is going to display you the data as a data frame, right? So my table just has only one column, which happens to be a country ID, and those country IDs is being displayed over here. So, you can work with pandas when you're dealing with small amounts of data, right? But you know, Pandas is not compatible with large volumes of data, right? So it will fail. That is where PI Spark came into picture. So that is what most of the users were making use of it in your hive days. But moving on to Lumi, we have a way to run PySpark, right? But the recommendations or the best practices is to convert your PySpark queries into big frames, right? Like a Pandas data frame, we have something called as.

00:26:09 - 00:27:19 Unknown Speaker:

BigQuery data frame, which in short is called as big frames. Because what big frames does is ultimately the background, it leverages the processing power of BigQuery, which is a petabyte scale data warehouse, which can scan petabytes of data in minutes, right? So yeah, we will see one by one now. So first use all to read a BigQuery table. And create a data frame. Now in this, we are going to see how to load a Pandas data frame back to BigQuery, right? So that is what we are going to look at. So we are reading it. We are creating the data frame here, and so from that particular data frame, we are selecting only these two columns, which is short name and effective date. Now, I am storing back this particular two column, this data frame as a table into BigQuery.

00:27:20 - 00:28:14 Unknown Speaker:

So for that, what I'm doing here, I'm creating a schema first. So schema field short name, this is a string, schema field effective date, this happens to be a date. I'm also creating a table name here, right? So here I have it as df1. Let me change it to df2 so that if it is existing, so it doesn't throw any error to me. So table idea I have given, I have given the entire path. So this is my project. This is my personal data set. Create a table of this name, right? And now all I have to do is BigQuery. So this BigQuery library as a function, LoadJobConfig to which I'm going to pass my schema and then my client object as a method called as load table from the data frame.

00:28:15 - 00:29:11 Unknown Speaker:

So it's just simple as your English which says load the table from the data frame that you have. So what all you need to pass? Pass the data frame and where the table has to be created. So that is available in this variable table ID. And job config. Simple as that. So this is going to create and then finally job.result to see if that table has got created or not. So now I'm going to run this particular query. So which is going to create a table for us in my personal data set. So you see at the bottom, it says my kernel was busy. So it means it was executing so far. It finished the execution. It says loaded 10 rows into this particular table. Now I can go back to my BigQuery.

00:29:11 - 00:30:17 Unknown Speaker:

I can refresh here. I'll scroll this. SS1010 and I have DF2. I'll click on this. And I want you to look at the details. 10 rows. It was created on 12th November, right? It was just created right now. So that's what I just wanted to highlight over here. I can also click on preview to look at the data, right? So this is how you can load back your data into BigQuery, right? So that is a data frame. Like what if I have a CSV file, how do I read my CSV file? And then I would like to write it back to a BigQuery table, right? So again, it's very simple. The process is going to be same. Identify the column names, create a schema for it. So same, provide the table ID.

00:30:18 - 00:31:21 Unknown Speaker:

So after table ID, this time the CSV file is present in this particular path, right? So, this is my data set AXP-US-SS1010. Within this, I have this lumi-trainings.csv, right? You see here, I have my csv, lumi-trainings.csv. So, this is the path and same, you load the job config. This time, I'm not reading it from a data frame. Instead, I'm reading it from a URI. So, load table from URI. Provide your URI where the CSV file is present. Provide the path where the table has to get created. And the job config. That's it. So this is going to run. It is going to create the table for you. Maybe I will call the table as CSV2. Let me run this. Yes, I see I've raised your hand. Please do ask.

00:31:24 - 00:32:26 Unknown Speaker:

So is it possible to just load the data from the URI itself, like write them into the table with this function? Are you asking from this function? That's what we're doing here, right? So can you be more clear of the question that you're asking? But you're writing to the table. Okay. No, no, this table is not available at all, right? So I just avoided the table name. Like when I say table ID, this is the path. So within this path, so this name gets created. Maybe I can just show it to you here. Currently, if you see, only CSV1 is there. So the moment I refresh, so a table that gets created with this name. So you're just providing the path, along with that, you're providing the table name as well.

00:32:26 - 00:33:31 Unknown Speaker:

So it's not that the table should be existing. So we are creating a new table, and this table gets created just right now to us. Okay, yeah, oh, actually, I'm asking, like, if you want to simply reload the CSV file from this Google Cloud, Um, storage, uh, should. Could you use the same function here, or just use the like PD already CSV and provide the GS location? If you're using a Python way, this is the option, right? Like, you have to make use of the. You should provide the table ID path URI, and this is the function that you need to make use of, right? So, that's what we have to do. But, if you're directly working with Ruby, BigQuery, Web Console, so you have a different way there where you can import or export the data.

00:33:31 - 00:34:40 Unknown Speaker:

So, that is different altogether. But, if you're doing a Pythonic way, this is the approach. Okay. Yeah. So, till now, whatever we did, Looking at all this particular process was related to Python. When I say Python, the native library, so Pandas, it was working directly with Pandas, right? But if you're working with large volumes of data, right, so that is not going to work as expected. So it is going to throw in the dirt. So that is where I said 10 cornerstone days. Use of PySpark, right? So we will just show you as well as in how we can run your PySpark code. But the only problem that we face whenever we are running our PySpark codes, right? You cannot save the data back directly from your PySpark into a BigQuery table because you know, right?

00:34:40 - 00:35:32 Unknown Speaker:

PySpark is going to work on a distributed system. So that's what the usage of Spark itself right now, whatever the task that has completed on that particular distributed system, so here, in particular in the GCP. So it is not able to save that data by collecting all together all of the data and save it as a big query table. So that is the only place we face the issues where we will have to convert the PI Spark data frame, whatever we have into a big frames. And then we'll have to save it, right? So these are the challenges we face. That is why we suggest users, instead of using PySpark, you can directly convert your code to big frames where big frame is nothing but BigQuery data frame that gets created.

00:35:33 - 00:38:02 Unknown Speaker:

You can do all of the stuff, everything. Again, save the data back to BigQuery table easy, right? But yeah, so we will also show you how to work with PySpark over here. So by default, Java is available to you. PySpark is also available to you. The only thing that you have to do is copy this command. Go to terminal. I want to open terminal now. Go to terminal. Paste this command. This jar file is must, right? Let's see. Okay. Maybe I'll just click on New Notebook Workbench Guide. I think I've copied the entire command. The command just happens to be gs colon. So that's the command. Let me paste the command now. Let me hit on enter. Okay, it says no such file or directory. Try to run this in the notebook.

00:38:11 - 00:41:24 Unknown Speaker:

Okay, it doesn't work over here. Just check one last time. Jar using terminal, so that's what it says. As-salamu alaykum. Okay, maybe I can just get back to you on this as in why we are getting this particular issue. But maybe let's just quickly go back to the workbench guide. But this step is muscle. This is needed. So we have to copy this particular jar file without which we will not be able to work with. I spark in the notebook what makes instance So that step is pretty much needed, but they are just let's see how it behaves in my house cleaning method so it gave me the java version everything. I'm trying to see if this chart file is present. Okay, so Spark is available, Spark version 3.0.0.

00:41:24 - 00:43:05 Unknown Speaker:

But it says it cannot access the Spark BigQuery with dependencies.jar file. So that is not available here, but that should be the command which will copy, or it should be the entire command. Not sure why it is not working? Tell you why it was not working earlier. So you cannot copy the JAR file to your bucket, right? So, earlier I was into the bucket and I was trying to open the terminal. So my terminal by default got opened in this particular path. So since I was executing that particular command, so it was unable to execute it. But right now, you see Terminal 2 is by default opened in our base. So this gets opened at my notebook workbench instance, and where upon executing this, so it had this jar file over here, and now we can easily work with our PySpark code.

00:43:17 - 00:44:15 Unknown Speaker:

If you see, so, this is the PySPark code that I just wanted to highlight over here, so you can import your PySpark. Create the park special. And where? You can create your Spark session by create by providing your app name Config file, right? So this is very, very important because you provide that JAR file in the Config. And then you do this settings right, where you just create your Spark session. Basically, right? And once you have your Spark session created within your Spark, you can directly write the query, right? So you can write an inline SQL query where you want to load this particular table, right? So select this particular, select everything from this particular table, right? This is going to show only 10 rows because I've written only 10.

00:44:16 - 00:45:08 Unknown Speaker:

It is basically going to create a PySpark data frame to you, but it is going to return the data. You can directly write the query or you can separately write the query, store it within a variable, and you can pass that particular SQL query as well. It was easily able to read the table that I have created within my personal dataset within the BigQuery. Similarly, you can also connect to your . AXP-gumi data warehouse as well, right? So you can do this. So you can, this time I've written the SQL, I've stored it within a variable, and I have passed it, passed the variable instead of passing the query. So this is also doable. The steps remain same. You have to create a Spark session, right? That's all.

00:45:08 - 00:46:04 Unknown Speaker:

Once you have a Spark session, you can read the table that is present within the BigQuery, right? And now, if you want to read a table directly, right, you can mention the table name which you want to read. So there is one nested repeated fields, like a table that I have created using this particular feature that is available in the BigQuery. This time, I'm not writing any query. I'm directly reading the entire table, right? So that is what I am doing here. So load the table. Right. So it is capable of loading it. And I just wanted to see this is my parent fully because it is a nested repeated fields. The repeated fields will be available as a nested array. That is what we call it as a nested repeated fields in BigQuery. Right.

00:46:06 - 00:47:15 Unknown Speaker:

So that is there. And using PySpark, if you want, you can also create session tables as well. You do it using this create or replace temp view, which is going to create a session tables and whatever the session tables that you create, you can also create it to a Pandas data frame. All of the stuffs are available over here, right? And yeah, so one final thing that I would like to showcase here as well as we were. Insisting right to you. So start using big frames, right? Big frames is an alternate solution to Pandas, PySpark, providing better scalability and performance, right? While unlocking additional features. So we do have provided some documentation here which you can go through, right? So this is the library. Import bigframes.pandas. As BPD, right? So this is the table that we have provided here.

00:47:16 - 00:49:03 Unknown Speaker:

And from this library BPD, you can read the table, right? Using the read GBQ, you can read this particular table. This is going to create a big frames for you, big query data frames, and which is what we're going to display here. So I want you to take a... Okay. CRT load DF is not there. I'll give DF one. So the query job is getting executed. So zero bytes processed. So 137 bytes processed. Yeah, you see? So, it's showing the data for us, the short name and effective date, right? And df.info, I'd just like to show you that this is of type big frames. When I say big frame, it is nothing but BigQuery data frames, right? So, whenever you're working with large volumes of data, start working with this directly.

00:49:05 - 00:50:10 Unknown Speaker:

This is something similar to your pandas, whatever methodologies you add over their methods, the eight years that you were performing. A similar kind of methods are available within the BigQuery data frame as well, so you can easily work with it. And this is going to leverage your bigQuery data warehouse processing, so your work will be done easier. And again, saving the data back to a bigQuery table is also easy with the big frames. Yeah. And with this, there are certain migration best practices that is listed out here for those people who are getting migration to Lumi. And there are few magics, like the magic commands that is available, which was built by our Lumi analytics team. This is like an accelerator for you. If you're working with certain I queries and if you want to convert those I queries into BigQuery format SQL, right?

00:50:10 - 00:51:23 Unknown Speaker:

For that you have to do two things. One is I would be sharing this particular .Txt file with you which says mymagics.py, right? So this mymagics.py is what has the logic. Which will convert your high query into your sequel way, right? Wherever you are working so place this my magics dot py Load your magics by executing this particular cell That is very very important, which is going to load your magic command So this is my I am query that I have and now All I will do is use this magic command percentage hive to BigQuery. I am passing my SQL query, right? I've stored it within this variable. If I run this, like whatever the hive format it goes there, it has converted into BigQuery SQL format and it has returned me the output, right?

00:51:24 - 00:52:20 Unknown Speaker:

So that is how it is going to give it back to me. Currently, I don't have any input file to showcase, but you can also pass an input file. So you can also pass an input file where it is going to provide the output to you. So this is going to be very, very useful for five users. And then I can also directly pass a query over here. You see this. I am doing a select count star from cstone db3 table. So cstone db3 happens to be a cornerstone database. When I run this query, it just correctly gave me select count star from txp-lumi happens to be the database and it correctly mentioned the format .Dw because within the dw dataset is what this particular table will be there.

00:52:21 - 00:53:26 Unknown Speaker:

Instead of using your SQL translator feature, You can directly start using this particular magic commands, right? So, yeah, this is what, uh, I had pretty much to cover for, uh, today's session, uh. Please feel free to ask any queries that you might have. Yeah, here's a look, yeah. So while converting the hive queries to big queries. You know, sometimes you have to really look at things like date or something else, and then I've not really used that a lot. But I'm just checking if this command the magic command, which is saying if this is like, better option. Yeah, just try using this, right? So they have written the background logic over here, right? Like whatever the magic command that are that you are trying to utilize. I think they have tried to address almost all of the things, right?

00:53:27 - 00:54:23 Unknown Speaker:

Let's see, there are some admins, hive parser, right? So there is a lot of work that has done by the in-house team to develop this. So I think instead of the SQL translator, you can directly start making use of this. And please do provide us the feedback as how this works so that we can start improvising it. Maybe we can provide the... feedback to the in-house team and they can start working on it. Just to add one more question here, Santosh, if in case we have, let's say, any ODL we have created as a repository in Cornerstone using Hive, and the data is very large, and now because we are migrating to Lumi, but we still want to really use that repository, but we do not really know how to do it.

00:54:24 - 00:55:20 Unknown Speaker:

So this way, the cstone command, you know, which you mentioned, the magic command, wherein we can directly, like, I think, you know, pull it using, and we can pull entire data into the data frame, and then we can probably insert it, or, you know, that can be done. What I was trying to showcase here is, You have a query in Hive that gets converted to your BigQuery format. We'll be able to connect to a ODL table available in Cornerstone, so that is not possible from here. Okay, okay, we can't really connect to Hive. Yeah, yeah. So we have to bring in the data in the domain, the data should be available, understood, so. Any other question anyone has, if not, I have opened a poll. Please do provide your feedback. And then, yeah.

00:55:20 - 00:55:51 Unknown Speaker:

So in the meanwhile, please do ask any questions, if in case, if you have. And yeah, if there are no questions now, you can always reach out to us via the Lumi Academy Slack channel whenever you have any queries related to the Python notebook that you are working with. Yeah. So thank you very much for joining today's session. Have a great day, everyone. Bye-bye. Take care. Thank you so much. Bye-bye.2026-01-26 Notebook Workbench Access and Provisioning Training

Creation Time: 2026/1/26


ðŸ“…About Meeting

  â€¢ Date & Time: 2026-01-26 15:24 (Duration: 1133 seconds)
  â€¢ Location: Virtual â€” Lumi Portal / Notebook Workbench session
  â€¢ Attendee: (not explicitly listed in transcript) â€” session presenter Santosh S., participants included James, Alok and other Lumi Notebook Workbench users


ðŸ“’Meeting Outline


Notebook Workbench: Access & Provisioning

  â€¢ Creating a Notebook Workbench instance The presenter demonstrated how to create a Notebook Workbench instance from the Lumi Portal: navigate to Insights â†’ Notebook Workbench â†’ Notebook. New users will be prompted to create an instance; existing users can start their pre-existing instance. The presenter provisioned a demo instance during the session to illustrate the steps.

  â€¢ Project requirement for creation A minimum requirement is access to an analytical project in Lumi (project tags like PRJ-P-AU indicate E3 environment + analytical project). When creating, select the analytical project from the dropdown, confirm creation, and wait for the provisioning lifecycle (requesting â†’ creating â†’ running). Once running, the buttons Restart / Stop / Terminate / Open notebook become enabled.

  â€¢ India projects restriction Notebook Workbench is currently not enabled for India-tagged projects (IND tag) due to India data localization rules; users must not attempt to create instances under India projects. The presenter noted this is a documented restriction and the documentation page will be the first place for updates.


Authentication & First-time Setup

  â€¢ Required authentication steps (one-time per instance) After opening the notebook, users must authenticate the notebook to their AXP account and the selected project. The presenter walked through the two gcloud commands from the documentation:
    
    â€¢ gcloud auth login
    â€¢ gcloud auth application-default login These commands must be run in the notebook terminal. Each command opens a browser link where users must sign-in using their Amex (corporate) email, copy the verification code, paste it into the notebook terminal and confirm. This is a one-time activity per newly created instance; it is not required again when stopping/starting the same instance.

  â€¢ Common pitfalls Users sometimes mistakenly use personal emails during the auth flow, which causes errors. Reminder to use the corporate (AXP) email. If an instance is terminated and recreated, authentication must be re-run for the new instance.


Notebook Usage, Features & Limitations

  â€¢ Opening and using the notebook Once the instance is running, the presenter clicked Open notebook to demonstrate the environment and perform authentication via terminal.

  â€¢ Terminal usage & scheduling alternatives Participants asked whether notebooks can be run from a terminal and about long-running / scheduled executions:
    
    â€¢ Running the notebook itself from a terminal (as in direct background execution) is not currently supported in the same manner as traditional server terminals.
    â€¢ SQL queries can be executed in BigQuery; scheduling in Lumi is typically done with BigQuery scheduler or via DAGs in Cloud Composer (Airflow) rather than running notebook processes from a terminal.
    â€¢ The presenter will follow up regarding terminal-based operations for scheduling or background runs if neededâ€”participants were encouraged to raise requests in Slack for follow-up.

  â€¢ Idle timeout / session duration A participant (James) reported apparent 30-minute idle timeouts. The presenter acknowledged prior settings were longer (1â€“2 hours) and will raise the user request with the team to check whether a recent change caused shorter timeouts and follow up.


Documentation & Support

  â€¢ Reference documentation The presenter shared a compliance/documentation page containing step-by-step screenshots and the latest Lumi Portal screenshots (including the create-workbench UI). The recording of the session will be shared with attendees. The presenter pasted the documentation link in chat for attendees to keep.

  â€¢ Questions & clarifications during demo Participants asked clarifying questions about what constitutes an India project, timeout settings, terminal workflows, and whether authentication is needed more than once. The presenter answered live and requested follow-ups via Slack for items needing investigation (terminal scheduling, timeout behavior).


ðŸ“‹Overview

  â€¢ The Notebook Workbench must be created from Lumi Portal: Insights â†’ Notebook Workbench â†’ Notebook.
  â€¢ You need access to an analytical Lumi project to create a Workbench instance; India-tagged projects are currently not supported.
  â€¢ Provisioning goes through requesting â†’ creating â†’ running; once running you can Open notebook and use control actions.
  â€¢ First-time authentication inside the notebook is required (gcloud auth login and gcloud auth application-default login) â€” run in the notebook terminal and use your AXP email.
  â€¢ Authentication is a one-time step per created instance; restarting the same instance does not require re-authentication.
  â€¢ Running notebooks purely from an external terminal is not supported in the same way; scheduling and background jobs should use BigQuery scheduler or Cloud Composer (DAGs).
  â€¢ Presenter will follow up on reported idle timeout reduction (participant-reported 30-minute timeout) and on whether terminal-based/background execution options are possible.


ðŸŽ¯Todo List

  â€¢ Santosh S. (Presenter / Session lead):
    
    â€¢ Share session recording and documentation link with all attendees (ASAP).
    â€¢ Follow up with platform team about idle timeout changes (investigate reported 30-minute timeout) and report back (target: within 3 business days).
    â€¢ Investigate whether notebook operations can be executed from a terminal or whether alternative workflows exist for background scheduling; respond in Slack if requested by participants (target: within 5 business days).

  â€¢ Platform / Infrastructure Team (action via Santosh):
    
    â€¢ Confirm and publish current idle timeout settings for Notebook Workbench instances and, if changed recently, provide rationale or remediation (deadline: by Santoshâ€™s follow-up timeline).
    â€¢ Provide guidance or a recommended pattern for running notebook-based jobs in background / scheduled mode within Lumi (e.g., Cloud Composer DAG examples or other recommended approach).

  â€¢ James (participant raised timeout issue):
    
    â€¢ Provide detailed reproduction steps and timestamps to Santosh via Slack to help troubleshooting the timeout issue (no strict deadline â€” please provide soon).

  â€¢ All users:
    
    â€¢ When creating a Notebook Workbench instance, ensure you select an analytical project (non-IND) and plan to run the two gcloud auth commands in the notebook terminal the first time you open a newly created instance.
    â€¢ If you require follow-up on terminal scheduling or timeout behavior, post details in Slack and tag Santosh for tracking.

If you need, I can extract the documentation link shared in chat, compile the exact gcloud commands used during the demo, or produce a short step-by-step quick-start checklist for new users. Which would you prefer next?


Transcription

00:00:01 - 00:01:01 Unknown Speaker:

Python notebook on BigQuery. Good morning, good afternoon, good evening wherever you are joining from. I welcome everyone to the domain notebook workbench session. So what is that we are going to cover in this particular session if you ask me. The main thing that I would be highlighting in this particular session is the recent update. Where? All of the existing Notebook workbench users and any new user who would like to create a Notebook workbench instance has to get started. Their journey from the Lumi Portal from Lumi Portal Where do I go? If you ask me, you have to click on Insights within the Lumi Portal and from Insights. All you need to do is go to Notebook Workbench, and with the Notebook Workbench, all you need to do is click on Notebook right.

00:01:01 - 00:01:49 Unknown Speaker:

So once you click on Notebook, you will get into that actual Lumi Notebook page within the Lumi Portal. So if you are a new user, right? So this is what it will show, where it will ask you to create a workbench instance. And I think for most of the users who had their existing notebook workbench instance already, right? I think it was available where you just had to start your workbench instance and open the Jupyter notebook. But I have dominated by notebook workbench instance completely just to showcase it for the new users who are going to join this particular session. So for those people who have already done it, it is going to be a repeater. So bear with me.

00:01:50 - 00:02:46 Unknown Speaker:

And in order to create a notebook workbench instance or work with notebook workbench instance in Gumi, The bare minimum criteria is one should have access to a analytical project in Luby. So when I click on create workbench instance, it shows me this drop down select project under which it is giving me two options. Where you see this PRJ-P, this indicates it is a E3 environment. And within E3, AU indicates it is an analytical project. So, The bare minimum criteria is one should have access to an analytical project, so if you have access to an article project, it will show. In the drop down, choose the project of your choice, click on Continue. And then it is asking, Are you sure you want to create a GCP Notebook Workbench instance under this project? Right?

00:02:46 - 00:04:13 Unknown Speaker:

I'll click on Confirm and it might take a few minutes because it is going to provision me the notebook, right? So I'll click on continue here. As you see, currently, it is in the requesting state where the instance is in the requesting state. And and will be enabled to proceed until unless the requesting state is completed. Right? So once this is done, you should be able to see all this button enabled, Restart, Stop, terminate, Open notebook, all these things will get enabled. So in the meanwhile, while this is happening, I would like to share an important compliance page with you, wherein you have all the latest updates, documentation, everything present in that particular place. Let me search for it. That should not be an issue. Let me search for it. That should not be an issue.

00:04:24 - 00:05:28 Unknown Speaker:

Guide So for those of you who have joined right, just a recap I'll do so. The bare minimum criteria If in case, if you want to work with notebook workbench instance, right, you should have access to a analytical project. So that is the bare minimum criteria that one should possess because otherwise will not be able to work with it. See, one more latest update is also available here. It says, note, India project users. So this particular notebook is not enabled for India projects. So please do not try to create a notebook workman's instance for India projects from Lobby portal, right? So any new updates will be updated in this particular page first. So let me copy this and paste it here with you in the chat so that you have it ending with you.

00:05:29 - 00:06:40 Unknown Speaker:

And yes, the recording of the session will be shared with all of the attendees. So why I brought you to this particular page, if you ask me, like whatever steps that I am carrying out is available in the form of screenshots. But I think that screenshots, yeah, they have updated the latest screenshot itself here, the Lumi portal. So select the project, and once you select the project, right. And once it is available, so it will be the running state. And once it is in the running state. Like, Okay, what does India Project means? Okay, that's a good question. I think you might be a new user. Like, you know, those India data localization, right? So because of this, RBI has instructed that all of the India transaction data should not go out of India, right?

00:06:40 - 00:09:53 Unknown Speaker:

Because of this restriction, like AXP-Lumi is a global data warehouse, right? So Lumi data. Similarly, you have your, like... the project for india project itself which contains only the india data now whatever the project that i have used here prj-p-au boomi academy so that. So I will project so far India project you'll have to you'll be able to see this IND within your project tag. James, I didn't get your question, so this area you would increase the timeout. Maybe 30 minutes now. Can you unmute yourself and be? More clear. I didn't get your question completely. Are you okay? Yeah, I'm okay. Sometimes I may be looking at something, and then I go back, and it kicks out, and I have to restart the system, or it's asking me to restart.

00:09:53 - 00:10:49 Unknown Speaker:

So it appears as though it's maybe only like a 30-minute timeout where it times out. So I was curious where to change that setting. I couldn't find it. I think either the notebook, sorry, asking about the refresh or the entire notebook. I think idle time is what it is. Yeah, idle time probably makes more sense. So initially it was set to one to two hours itself. So, but I'm not sure if they made any recent, uh, changes to it or not. But yeah, so obviously I'll put on your request to the team that few users are facing this issue and maybe I can get back to you on it. Thank you. Yeah, so yeah. There are a lot of things available in this particular documentation so you can go through it whenever you have some time post this particular training.

00:10:49 - 00:11:58 Unknown Speaker:

But we will try to cover most of the things that we have in this particular stuff. So it's if you see currently, after the request requesting it has gone to the creating state, your workbench is being created. Uh, please allow a few minutes, so that's what is happening. Let me just click on Refresh State. It's still in creating itself. Uh. Meanwhile, I'll just quickly take a look at the chat. Is there a way to run notebook from terminal? No, so, uh, you, you cannot do that, so notebook, you cannot run it from terminal. So, this is not available currently. I think it should be possible, I guess. So you can do it, the SQL queries, but not the hook one. And so what is the terminal? In that case, it will not be the gold terminal, right?

00:11:58 - 00:12:54 Unknown Speaker:

So I am assuming there will be some different terminals. Like when you say terminal, like what does that mean? So it is for a Windows user, it happens to be a command prompt, right? Or otherwise, you can make use of a putty terminal that is available. Yeah, exactly. So basically I connect to Gold Server and then then run five query on the server, right? So can we? Can we do that in the Lumi environment too? Like, basically, you do that for your scheduling purposes, right? So you make use of that, know how to run your queries in the background and like. Basically, for scheduling purposes is what, as a FIIM operator, you make use of it. But in Lumi, the equivalent of FIIM happens to be BigQuery. I think within BigQuery itself, you have a scheduler feature available.

00:12:54 - 00:13:48 Unknown Speaker:

And in order to schedule, we also make use of something called as DAGs in Lumi, which comes under the Cloud Composer part. So these options are there, but I haven't used Terminal. That is something maybe I can check and get back to you, but you can just drop a note to me in Slack so that I do not miss it. Okay. Perfect. So coming back to the session now, you see it got changed from the creation state to running state now. All I have to do is click on open notebook, which is going to open a notebook for me. And since we have created this notebook for the first time, we will have to do certain steps. Like we have to authenticate this particular notebook with our ADS ID and the project that we are going to use.

00:13:48 - 00:14:52 Unknown Speaker:

So only then, like post the creation, like whatever queries you're writing to connect to BigQuery, right? So those things would be successful. Otherwise, most of the times it doesn't work, right? So that's why, like, the... authentication step is must so, which most of the users miss. So even if in case if your instance gets terminated by mistake, right? So if you're creating a notebook workbench instance again, right? So you just have to do this step, which I'm going to do now. So all you can do is just click on this particular using gcloud commands again. So that is going to take you to the documentation page, which I shared with you, right? So there are a few commands present here. This gcloud auth login and gcloud auth application default login.

00:14:52 - 00:15:54 Unknown Speaker:

So these are the things that you need to perform. Yeah, I'll just close one thing. So once you're here. So the first thing that you need to do is click on terminal because that is where we'll have to execute those two commands, which I mentioned. Copy this command gcloud auth login from here and go back to the notebook terminal, paste it, hit on enter. So it is going to ask you, do you want to continue? Yes. Hit on continue. So this will give you a.Browser link click on it and complete the sign-in prompts. So please do choose your mix account. So I know some people would have by mistake sometimes they might give their personal email ID which will cause an error. So just be cautious that you provide your American Express email ID here.

00:15:57 - 00:16:58 Unknown Speaker:

So click on continue again, you can scroll down. Click on hello. So these are certain authentication steps. And then finally, you need to copy this verification code. Come back to the notebook. Paste this verification code and hit on enter. So now it says you are now logged in as santosh.s at the rate axp.com, right? This is a one-time activity. You need not do it again and again, right? Yes, Alok, yeah. I see your hands raised. One time, Santosh, you mean that, so today, you know, when I'm going to use it, so the first time I'll do it, so again, let's say, you know, if it is logged out, like I have to, you know, restart the workbench. Yeah. So I'll have to authenticate it again? No, not required.

00:16:59 - 00:17:54 Unknown Speaker:

So, only when you are creating a notebook workbench instance for the first time, you just have to execute this two commands post, which you can stop your instance. Whenever you're not working it, you can come back after two days or a week again, start it, and you can start working directly. You need not do this stuff, okay, so that won't be required. So here I see the project ID is mentioned, right? That this is the project ID that's going to be in use. Uh, but and you mentioned that, uh, I can't really use India Project ID on. Currently, that is not available. I think the team might be working on it. Okay, cool, thank you. Yeah. So let's go with the second command, that is also pretty the same steps, you just have to copy the command paste in the terminal, so you have to hit on Enter.

00:17:54 - 00:18:12 Unknown Speaker:

So the same thing we have to carry out. Even this is going to give us a browser link. The same authentication steps we have to carry out using our Amex email ID. So Google Auth library, click on continue.